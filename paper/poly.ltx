\documentclass[preview]{elsarticle}

\usepackage{amssymb}
\usepackage[table]{xcolor}
\usepackage{subfig}
\usepackage{ytableau}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{pgfplots}
\usepackage{mathtools}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\usetikzlibrary{arrows, calc, positioning, patterns, pgfplots.polar}

\usetikzlibrary{snakes,arrows,shapes}

\title{Polynomial evaluation on super scalar architecture, applied to the elementary function $e^x$}

\author[rvt]{T. Ewart }
\ead{timothee.ewart@epfl.ch}

\author[rvt]{S. Yates}
\ead{sam.yates@epfl.ch}

%\author[rvt]{F.  Sch\"urmann }
%\ead{felix.schuermann@epfl.ch}

\author[rvt]{F. Delalondre}
\ead{fabien.delalondre@epfl.ch}

\author[rvt]{F.  Sch\"urmann }
\ead{felix.schuermann@epfl.ch}


\address[rvt]{Blue Brain Project, Campus Biotech, Ch. des Mines 9. CH-1212 Gen\`eve}


\graphicspath{{plot/figures/}{../res_poly/figures/}{graph/}} %do not forget the / at the end

\newcommand{\R}{\mathbb{R}}


\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\begin{document}

\begin{abstract}

%The polynomial evaluation is a main field of computer science, and it has  been massively studied.
%In this paper, we focused on the benchmarking of the polynomial evaluation of the main algorithms
%of literature on the last processor of IBM and Intel.
%
%Modern processors are super scalar and out of order, therefor the performance of the polynomial evaluation 
%will depend of the degree of parallelism of the algorithm, to maximise
%the pipelines and floating point unit usage. The performance will be achieved, 
%if the algorithm has massive degree of parallelism.
%
%
%The art of polynomial evaluation is well know and well documented in the literature. The performance of a polynomial evaluation
%depends on two factor, the efficiency of the algorithm 


The polynomial evaluation of small degree polynomials is critical for the computation of elementary functions. 
It has been massively studied, and is well documented. In this paper, we have evaluated the existing methods for polynomial evaluation
on super scalar architecture.  And, we have completed this work by factorisation evaluation, which is surprisingly neglected in the literature.  
This work has been focused on the most recent processors of  Intel, amongst others, several computational units are available.
Moreover, we have applied our work on the elementary $e^x$ that  requires, in the current implementation,
an evaluation  of a polynomial of degree 10 for a satisfying precision. The results obtained for the  factorization scheme benchmark are faster than the famous
and efficient  Estrin's method.


\end{abstract}


\maketitle

\section{Superscalar processor}
It exists several definition of superscalar processor, but two seem more interesting:  "Dispatch multiple instructions every cycle" from
\cite{thomas1987high} or "Execute two or more scalar operations in parallel" from \cite{Lam1990}.
Whatever the definitions, they are two main ideas: execute instructions concurrently and independently in separate execution pipelines.
The throughput of the execution into the concurrent pipelines may be really improved by allowing out-of-order execution.
Nevertheless, if the dispatcher is ineffective at keeping all of these units fed with instructions, the performance of the system will be no better than that of a simpler, cheaper in-order design.

Modern processors have more than one pipelines. On the last release of Intel this number reaches 8 pipelines for the Broadwell processor, and more interesting processors can have several Floating Point Unit (FPU - \textit{e.g.} two on the Broadwell) and Arithmetic Logic Unit (APU). Complementary to the execution pipeline, the execution of a single instructions is also pipeline in several step \cite{Power7FPU}. It allows to restart the same operation (if no dependency chain) without waiting the completeness of the previous one.

This paper focuses on the evaluation on polynomial of degree 10 on super-scalar processor, and \textit{in fine} it's application on the elementary function
$e^x$. State of the art of polynomial is first presented and the elementary function $e^x$. Then, more technically we describe the core unit of the last
processor of Intel (SandyBridge and Broadwell) 
%and IBM (Power7 and Power8)
, and the different method we will use for coding and measure the veracity of the results.
We present results for the polynomial evaluation and exponential for scalar and vectorial version, and then we compare to vendor exponential library.

\section{State of the art}
In evaluating the polynomial of degree $n$,
 \begin{eqnarray}
   P(x) = a_0 + a_1 x  + \dots + a_{n} x^{n}, \,\, (a_i  \in \R) \label{P0}
 \end{eqnarray}
 a large number of methods can be applied. In this study, we selected the most significant, extracted from the seminal work of Knuth \cite{Knuth:1997:ACP:270146}. %the art of programming [cite vol 2].
 The followings methods have been selected: the Brute Force method, the Horner method from order 1 to $n-1$, the Estrin method. Moreover, factorisation tachniques have been applied to all the  methods previously enumerated. To improve performance, the main challenge consists in maximizing the parallelism and minimizing the pipelining data hazards, in order to avoid bubbles that have the effect of deteriorating performance.
%\section{Algorithms}
\subsection{Evaluation of Powers \label{powern}}
The basic step in performing a polynomial evaluation is the computation of the power function.
To compute $x^n$, the naive approach would be to multiply $x$ with itself $n$ times in sequence, which has complexity of o($n$).
Following the advice of Knuth, the Right-to-Left binary method for exponentiation is an excellent alternative for our small degree polynomial \cite{Knuth:1997:ACP:270146}. 
The method has a complexity o($\lfloor log(n) \rfloor + \upsilon(n)$) multiplications where  $\upsilon(n)$ is the number of ones in the binary representation of $n$.
\subsection{Brute force}
The brute force method simply consists in evaluating the polynomial~\eqref{P0}, term after term, computing the power function as described above.
The complexity of the algorithm is $n$ additions and  $2n-1$ multiplications.
\subsection{Horner method: classical - $1^{st}$-order}
The homonym Horner's method at order one is the most classical. Given an input value $x_0$, it evaluates recursively the polynomial~\eqref{P0} as
 \begin{eqnarray}
   P(x_0) = a_0 +  x_0 ( a_1  +  x_0 ( a_2 + x_0(  \dots a_{n}  ) )).
  \end{eqnarray}
\textit{``Horner's method''} has a complexity of $n$ multiplications and $n$ additions, or $n$ Fused Multiply-Add (FMA). The method is efficient,
but inefficient from the point of view of the processor, because it can introduce pipelining data hazards due to the dependency chain in the recursive process.

\subsection{Horner method: $k^{th}$-order}
\textit{``Horner's method''} can be generalised to an arbitrary order by building new polynomials over $x^k$ instead of $x$ \cite{Dorn:1962:GHR:1661979.1661987}.
The main advantage is to introduce parallelism that removes the data hazards, especially for high orders of $k$.
Given a degree of parallelism $k$, Horner's $k^{th}$-order method reads:
 \begin{eqnarray}
   P(x_0) &=&  Q_0(x_0^k) + Q_1(x_0^k) + \dots + Q_{k-1}(x_0^k) \\
   Q_{k-1}(x_0^k) &=&  x_0^{k-1}(a_{k-1} + x_0^{k}(a_{2k-1} + x_0^{k}(a_{3k-1} + x_0^{k}(\dots a_{m \le n}))))
     \end{eqnarray}
This method hasa complexity of $n+k-1$ multiplications and $n$ additions. The classical Horner method corresponds to $k=1$.

\subsection{Horner method: Estrin}
An alternative to the generalization of Horner's rule is the Estrin method \cite{10.1109/AFIPS.1960.28}, described as follows:
\begin{equation}\label{estrin}
\begin{split}
c_i^{(0)} &= a_i + x_0  a_{i+1}, \\
c_i^n     &= c_i^{(n-1)} + x_0^{2^n} c_{i+2^n}^{n-1}, \\
P(x_0)    &= c_0^n.
\end{split}
\end{equation}
Developing equation~\eqref{estrin} leads to: 
\begin{eqnarray}
P(x_0) &=&  a_0 + a_1 x_0   \nonumber \\
               &+&  x_0^2(a_2  + a_3 x_0) \nonumber \\
               &+&  x_0^4(a_4  + a_5 x_0 + x_0^2 (a_6 + a_7x_0)) \nonumber \\
               &+&  x_0^8(a_8  + a_9 x_0 + x_0^2 (a_{10} + a_{11}x_0) + \nonumber \\ 
               & &  x_0^4 ((a_{12}+a_{13}x_0 +x_0^2(a_{14}+a_{15}x_0)))))  \nonumber \\
               & &\dots
\end{eqnarray}
This method has $n$ multiplications and $n$ additions, just like Horner's
method, but it has a remarkable pattern design for the following reason: every
$c_i^j$ can be associated to an independent FMA, and by virtue of this
independence we expect a good efficiency on super scalar machines (i.e. no
bubbles in the pipeline).

\subsection{Factorization}
Finally we consider the factorization method.
This method is based on the fundamental algebraic property of polynomials with real coefficients, which states that any polynomial of degree $n$ can be decomposed into a product of $n$ factors
\begin{equation}\label{factor}
\prod\limits_{i=1}^n (x - z_i),
\end{equation}
where either $z_i \in \mathbb{R}$ or there is a couple of complex conjugate numbers $z_i, z_j \in \mathbb{C}$.
In this work we only consider polynomials with fixed coefficients known a priori, thus we can compute the root of the polynomials offline using an external program.
We recall the property that, given two complex conjugate numbers $z_i, z_j$, the product $(x - z_i)(x - z_j)$ factorizes to $x^2 + \alpha x + \beta$ where $\alpha, \beta \in \mathbb{R}$.
We use this property to rewrite~\eqref{factor} such that it only contains real coefficients by explicitly computing all the products between complex conjugate roots:
\begin{equation}\label{real-factor}
\prod\limits_{i=1}^{n-l/2} (x - z_i)\prod\limits_{j=1}^{l/2} (x^2 + \alpha_j x + \beta_j),
\end{equation}
where $l$ is the number of complex conjugate roots of the polynomial~\eqref{P0}, with $l<n$ and $l$ always even.
Instead of only evaluating all the simple factors appearing in~\eqref{real-factor} one-by-one, we consider all their possible combinations into polynomials of higher degree, to test whether a particular sequence of higher degree polynomials yields better results.
Each higher degree polynomial can be evaluated by one
of the previously described methods.
The factorization technique can introduce a large degree of parallelism and may outperform others for superscalar architectures.

\subsection{complexity}
To conclude this section on algorithms, the complexity of the different methods is summarised in Table \ref{complexity}.
\begin{table}[h!]
\begin{center}
\begin{tabular}{l c c  }
	\hline
		             & addition & multiplication \\
Brute force            &  $n$       & $2n-1$     \\
Horner $k^{th}$-order  & $n+k-1$ & $n$   \\
Estrin                    & $n$        & $n$             \\
Factorization        &    combination            &   combination   \\    
	\hline
\end{tabular}
\end{center}
\caption{Number of operations for different algorithms used for the evaluation of a polynomial of degree $n$. \label{complexity}}
\end{table}%

\section{Elementary function: $e^x$}
Small polynomials are extensively used in the approximation of elementary functions.
The present study focuses on $e^x$.
A possible approach to compute $e^x$ is:

\begin{equation}\label{begin_exp}
\begin{split}
\forall x \in  \mathbb{R}, \exists  y \in [0,ln 2] & \textrm{  with } k \in \mathbb{N} \textrm{ such that:}\\
    x  & =  y + k ln 2   \\
e^x  &=  2^k e^y
%       &=&  2^k P(y) \label{exponential_formula} 
\end{split}
\end{equation}

The restriction on the range $[0,ln 2]$ simplifies the evaluation of $e^y$. A truncated Taylor series could be a good idea to evaluate $e^y$.
Nevertheless, it can be a source of large error due to its locality properties, as we are going to demonstrate.

\subsection{Truncated series}
 If we consider the Taylor series:

\begin{eqnarray}
e^y = \underbrace{\overbrace{1 + y + \frac{y^2}{2!} + \dots + \frac{y^n}{n!}}^{\text{Approximation}} + \overbrace{\frac{y^{n+1}}{n+1!} + \dots}^{\text{Truncation Error}}}_{\text{Exact mathematical formulation}}
\end{eqnarray}

Here the "Truncation Error" corresponds to the remainder $R_n$ from Taylor's Theorem. It can be expressed under an integral or the Lagrange form by:
\begin{eqnarray}
R_n & = & \int_{a}^{y} \frac{(y-t)^n}{n!}f^{(n+1)}(t)dt \,\,\, \text{(the integral form)} \\
	& = & \frac{f^{(n+1)}(c)}{(n+1)!}(y-1)^{n+1}, \,\,\, c \in [a,y] \,\,\, \text{(the Lagrange form)}
\end{eqnarray}
For $f(y)=e^y$ and $a=0$, we have $f^{(n+1)}(y)=e^y$, thus
\begin{eqnarray}
R_n &=& \frac{e^c}{(n+1)!}y^{n+1}, \,\,\, c \in [0,y] \\
       &\leq& \left| \frac{e^y}{(n+1)!} y^{n+1} \right|
\end{eqnarray}

When $y=ln2$ and $n=10$, the remainder term has magnitude $R_{10} \leq \left| \frac{e^{ln2}}{11!}ln(2)^{11}\right| \sim 8.89 \times 10^{-10}$, which can be considered a large error compared to floating point accuracy. According to this formula, to guarantee an error lower than the double precision of the machine, $n=16$ is needed. An alternative would be to develop the Taylor 
series around $ln2/2$ but once again the error would be local.

\subsection{Polynomial approximation}

As demonstrated truncated Taylor series can not be selected due to locality issues. An excellent alternative is to approximate $e^x$ with a polynomial using a minimax method.
The Remez algorithm is an excellent solution. The method certify than $f(x)$  ($e^x$ in our case) can be approximate by a unique minimax solution $R(x)$ minimising 
an  error function \textit{e.g.} $E(x)=f(x)-R(x)$) in the concerning interval. Moreover the minimax solution verifies (Chebyshev proof).

\begin{itemize}
\item if $R(x)$ is a polynomial of degree $n$ with $n+1$ $c_i$ coefficients, there are $n+2$ unknowns: the $n+1$ coefficients of the polynomial, and the maximal value of the error function ($E(x) = f(x) - R(x)$)
\item The error function has $n+1$ roots and $n+2$ extrema 
\item The extrema alternate in sign, and all have the same magnitude 
\end{itemize}

That means that if we know the location of the extrema of the error function then we can write N+2 simultaneous equations:

\begin{eqnarray}
R(x_i) + (-1)^iE = f(x_i) \label{system}
\end{eqnarray}

where $E$ is the maximal error term, and $x_i$ are the abscissa values (or control point) and should be the $n+2$ extrema of the error function. The resolution of this system of equation is easy 
nevertheless it does not  provide the position of the extrema. In practice the  Remez algorithm necessitates an initialisation and  two iterative steps. First the polynomial approximation comes from a 
Chebyshev approximation, the extrema are determinate, they become the control points $x_i$ of the error functions. Then the two iterative steps are performed:

\begin{enumerate}
\item Compute the $n+1$ coefficients $c_i$ and the error $E$ by solving the  equation \ref{system}. A solution is obtained with the same error on the control point but it is not necessarily the minimax 
solution, because the extrema are not located on the control points.
\item Find the extrema of the new solution using the properties that the extrema is between the $n+1$ root of the polynomial and the control points. This step is called exchange step. 
And iterate until the error function is lower than the wanted precision. 
\end{enumerate}

The Remez method is powerfull, a full detail and rigorous analysis can be found in \cite{Fraser:1965:SMC:321281.321282}. In this paper, the \texttt{boost::math::tools} package was selected.  A polynomial of degree 10 gives a satisfactory trade-off between accuracy and performance. This polynomial will be the starting point for the method previously describe. 

\subsection{Notations}
In this section, the following notations are introduced. Any polynomial $P$ is characterised by two parameters, the degree and the method of evaluation. 
The degree is represented by the superscript $n$, whereas the subscript is the method of evaluation. The different methods of evaluation are
the brute force method ($b$), the Horner method of order $k$ ($h^k$) and the Estrin method ($e$). For example, $P^{6}_{e}$ means a polynomial of degree 6
evaluated with the method of Estrin, whereas $P^8_{h^2}$ means a polynomial of degree 8 evaluated with the method of Horner with order 2.

\subsection{Combinatory}

The factorization method and the different polynomial evaluation techniques described above give rise to a large number of possible combinations of polynomial factors.
We now present a method for computing exactly how many different combinations we need to consider.

In this study, we focus on the evaluation of polynomials of degree 10, where the coefficients are known in advance. We concentrate specifically on polynomials of degree 10 
because they are very useful in the approximation of the exponential function $e^x$ with $x \in [0, ln2]$.
The polynomials we consider are strictly positive in the range $[0, ln2]$, and therefore they do not have any real roots, but five pairs of complex conjugate
roots. From the five pairs, five quadratic polynomials can be built following the factorization process described above. 
The product of these five quadratic factors is the factorisation scheme.
We note that there exist other alternative schemes.

To find the total number of combinations of polynomial factors, we start by considering all possible partitions of the number 10 as sum of even numbers.
We only consider even numbers because, as stated above, all the polynomial factors we consider are of degree two, arising from the product of two factors associated to complex conjugate roots.
Partitioning the number ten in even summands s equivalent to partitioning the number five into arbitrary summands, then multiplying by two.
There are seven partitions of the number five, which are $(5),(4,1),(3,2),(3,1^2),(2^2,1),(2,1^3),(1^5)$.
Consequently the partitions of ten into even summands are

\begin{eqnarray}
 (10),\,(8,2),\,(6,4),\,(6,2^2),\,(4^2,2),\,(4,2^3) \textrm{ and } (2^5) \label{evaluation}
\end{eqnarray}

To evaluate the number of combinations we make three considerations:

\begin{enumerate}
\item each polynomial $P^m$ has $m+1$ methods of evaluation: $P^m_b$, $P^m_e$ and $P^m_{h^{k}}$ where $k \in [1,m-1]$ because $P^m_{h^1} = P^m_{h^n}$;
\item for each polynomial product $P^l P^m$ with $l\neq m$, there are $(l+1) \times (m+1)$ possible combinations (we do not consider the alternative $P^m P^l$  because multiplication is commutative);
\item for each product of $k$ polynomials having the same degree $m$ there are ${m+k \choose k} $  and not ${m+1 \choose k}$ possible combinations because repetitions are not counted.
\end{enumerate}

Applying this scheme to~\eqref{evaluation} determines the total number of combinations $S$:

\begin{eqnarray}
S  &=&  \textrm{card}((10) + (8,2) + (6,4) + (6,2^2) + (4^2,2) + (4,2^3) + (2^5)) \nonumber \\
    &=& 11 + 9\times3 + 7\times5 + 7\times{4 \choose 2} + 3 \times {6 \choose 2} + 5 \times {5 \choose 3} + {7 \choose 5}  \nonumber \\
    &=& 231
\end{eqnarray}

The total number of combinations to evaluate $P^{10}$ is therefore 231.

\subsection{ From x to k and y}
The polynomial is solved consequently, going back to the $e^x$ computation. The equation~\eqref{begin_exp} can be solved  although it has two unknowns.\\
$\forall x \in  \mathbb{R}, \exists  y \in [0,ln 2], k \in \mathbb{N} \nonumber $ such that:
\begin{eqnarray}\label{xyk}
x & = & y + k ln 2.
\end{eqnarray}
To determine $k$, we apply the floor function to both sides of~\eqref{xyk}
\begin{eqnarray}\label{floork}
\floor*{\frac{x}{ln2}} & = &  \underbrace{ \floor*{\frac{y}{ln 2}}}_{=0} + \underbrace{\floor*{k}}_{=k} \nonumber \\
			      & = & k.
\end{eqnarray} 
Now that $k$ has been determined, we can compute $y$ by solving~\eqref{xyk}.

The previous equations can be integrated in the following algorithm:

\begin{algorithm}[H]
 \KwData{$x$ floating point number}
 \KwResult{$k$ signed integer and $y$ floating point number}
$ k =  \floor*{\frac{x}{ln2}} $ \;
\tcc{For rounding issue $-k \times ln2$ correction: in two steps}
$y = x - k\times6.93145751953125 \times  10^{-1}$\;
$y = x - k\times1.42860682030941723212  \times 10^{-6}$\;
 \textbf{return} $k$ and $y$\;
 \caption{$2^k$ evaluation algorithm. Note the operation  interpret\_as\_double interprets $k$ as a double it is not a cast operation.}
\end{algorithm}

\subsection{$2^k$ fast evaluation}
Computing $2^k$ may be done with a left bit shift but it can only work if $ (k << 63) \leq 2^{64}$ and it requires a cast from integer to double, which can be a compute intensive instruction.
An alternative solution utilises the floating point representation. Rational numbers can be represented approximately under the following form:

\begin{eqnarray}
x = -1^s \times (1+F) \times 2^{e+bias}
\end{eqnarray}


\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.4]{618px-IEEE_754_Double_Floating_Point_Format} 
\caption{double precision floating point number using IEEE-754 norm \label{doubleIEEE}}
\end{center}
\end{figure}

For double precision numbers (Figure \ref{doubleIEEE}): $s$ sign bit, $e + bias $ is the exponent (11 bits), it is bias (engineering sense of the word).
It it biased because an alternative  based on the two complementary method makes things harder specially for comparison. 
The bias is equal to 1023 and $e$ belongs [-1022,1023]. $F$ is the mantissa (53 bits) $\sum_{n=1}^{p-1} bit_n \times 
2^{-n}$. The method consists in taking the wanted $k$, adding the bias and moving it into the exponent part of the floating point representation, and letting the machine interpret it like a double.
The method is efficient, fast and does not introduce any errors nor does it require any conversion operations. This method can be described by the following algorithm:

\begin{algorithm}[H]
 \KwData{$k$ signed integer}
 \KwResult{$2^k$ as double}
 $ k = (1023 + k) << 52 $\;
 \textbf{return} interpret\_as\_double($k$)\;
 \caption{$2^k$ evaluation algorithm. Note the operation  interpret\_as\_double interprets $k$ as a double it is not a cast operation.}
\end{algorithm}

\subsection{Boundary limits}

The last step consists in fixing the boundary limits. This step is mandatory and it introduces additional workflows, although it is sometimes neglected 
in previous exponential study \cite{Costas}. The  boundary limits to respect are summarised in the Table~\ref{boundary}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{l c c c c c c }
$x$                         & $-\infty$  &$+\infty$ & 0 & NaN & $> max$  & $< max$  \\
		          \hline
$e^x$                   &  0 & $+\infty$  & 1 & NaN & $+\infty$   & 0 \\
		             \hline
\end{tabular}
\caption{Boundary of the exponential \label{boundary}}
\end{center}
\end{table}%

Branching is impossible on SIMD architectures, nevertheless we describe an alternative using bit manipulation, as described in the Algorithm~\ref{manipulatiomanipulation}.

\begin{algorithm}[H]
 \KwData{$x$ floating point number}
 \KwResult{$e^x$ with correct boundary condition (table \label{boundary})}
 \tcc{  inequality operator return false (0) or  true (-1)}
 $mask0 = \neg(|x| > max)$\;   
 $mask1 = x < max$\;
 $mask2 =  +\infty$\;
 \textbf{Compute}: $y = e^x$\;
 $y \,\, \&= mask0$\;
 $mask3  \,\, \&= \neg mask1$\;
 $y \,\, |= mask2$\;
 \textbf{return} $y$\;
 \caption{Boundary condition algorithm\label{manipulation}}
\end{algorithm}

This algorithm has been applied for the scalar and vectorial versions.

\section{Processors \& methods}
Performing a full review of processors is a huge work and it is outside the scope of this paper. We focus on the general execution of the processors, but still consider enough details
to explain our results. Whatever the architecture of the chip, the execution of the binary code follows the same logic. Fetch/decode units read a stream of instructions
from the L1 instructions cache memory and decode them into a series of micro operations that are executed by the processor's parallel executions units.

\subsection{Intel SandyBridge and Broadwell architecture}
The SandyBridge and Broadwell architecture derived from the core2duo architecture (based on a Pentium M). Both architectures support the new set of 256 bit vectorial instructions 
named AVX and AVX2 (Broadwell only).  For both architectures, the instructions fetch unit bandwidth 
is limited to 16 bytes per clock cycle. The instructions decoding execution is complicated, it is split
between a predecoder and a decoder. The predecoder job is to detect where the instructions start, a hard task because the size of an instruction can vary between 
1 to 15 bytes. The predecoder also identifies the additional field (if available) of the instructions (prefix, ModR/M, SIB, displacement and immediate). Then 4 decoders decompose
the incoming instructions into 1 or more $\mu$ops. Only the first decoder can generate more than one $\mu$op. For SandyBridge, decoders will generate 3 to 4  
$\mu$ops per cycle. On Broadwell, the generation rate is fixed to 4 $\mu$ops per cycle. The 16 bytes per clock cycle is a serious limitation, therefore both architectures have a cache memory
to avoid the limitations arising from the 16 byte per cycle bandwidth. The effect of the cache memory can double the total size of  $\mu$ops generated: 32 bytes per cycle. 
%The cache is organised around 32 sets $\times$ 8 ways $\times$ 6 $\mu$ops.
%$3 caches lines of 6 $\mu$ops. can be allocated simultaneously for a maximum of 32 bytes of code per cycle. 
Both architecture have the possibility to perform a fusion of instructions.
At this point, $\mu$ops are still in-order. They are now  stored in the re-order buffer (168 entries for SandyBridge and  192 entries for Broadwell) for the renaming of the register,  
the re-ordering $e.g.$ following a Tomasulo Algorithm, and \emph{in fine} the reservation station of 54 entries on Sandy Bridge and 60 on Broadwell respectively. At this last point the $\mu$ops will be executed on the execution units in out-of-order mode.
%The out of order execution is now performed on 6 ports on the SandyBridge and 8 port on the Broadwell, however the number of port dedicated for the vectorial addition, multiplications and FMA (Broadwell only) are
%equal to two, the port 0 and 1 respectively. 

The Sandy Bridge has six execution ports (or terminology execution pipelines). There are tree integer ALUs so that the most common integer operations can execute
 with a throughput of four instructions per clock cycle (port 0, 1 and 5). Two ports can handle memory read operations (port 2 and 3)
Port 4 is for memory write. Read/Write can be performed by port 2, 3 and 4 (write only) but the port 4 can not make address calculation.
The maximum throughput is one unfused $\mu$op on each port per clock cycle.
Port 0, 1 and 5 support usual 256 bit vector operations. There are separate ports for multiplication of general purpose registers and vector registers. 
The integer and floating point vector multiplier are on port 0.  These two multipliers can run simultaneously and both are fully pipelined with a throughput of 1 vector operation per clock cycle.
%Integer division uses the floating point division unit on port 0. This is the only unit that is not pipelined.

The Broadwell architecture has 8 execution ports (or terminology execution pipelines).  There are four integer ALUs so that the most common integer operations can execute
 with a throughput of four instructions per clock cycle. There are three ports that can handle integer vector operations (port 0, 1 and 5).
 Two ports can handle branches (port 0 and 6).  Two ports can handle memory read operations (port 2 and 3), and one port can handle memory writes (port 4).
Two ports can handle floating point vector operations (port 0 and 1). The direct consequences  of this larger number of port for floating point operations 
is a better throughput for vectorial addition (0.8) and vectorial multiplication (0.5) compared to SandyBridge.

%\subsection{IBM Power7 and Power8 architecture}
%The Power8 is the last release of the IBM processor derived from the Power7.
%Similarly to the x86 architecture, the Power7/8 chips support vectorial instructions but are limited to 128 bit width. 
%For both architectures the Instruction Fetch Unit bandwidth is equal to 32 bytes per clock. Contrary to the X86 architecture the instructions are enconded in the POWER 32-bit
%fixed-width RISC ISA. Therefore 8 inscriptions are pre-decoded into $\mu$ops by the IFU, and store in I-cache of 32 KB (4-way set associative  L1 cache for the Power7 and
%8-way set associative  L1 cache). This operation is performed in 2 cycles. Similarly to x86, both architectures can perform fusion of instructions, with higher efficiency for the Power8 architecture.
%If the requesting thread is ready, the  $\mu$ops are bypassed directly from the I-cache to the instructions buffer (still in order). The Power7 has a rate of completed instructions per cycle of 6, compared to 8 of the Power8.
%
%The  $\mu$ops are be managed by the Instructions Sequencing Unit (ISU), responsible for doing the renaming in out-of order mode and the dispatching in the various issue queues to
%the executions pipelines. After renaming and the dispatching the $\mu$ops (at least the compute ones) are transferred to the UniQueue (as the reservation station for the x86 architecture). 
%the UniQueue is implemented as 48-entry queue  (power7) or 64-entry queue (power8) and split into two halves.
%
%The Power7 has 12 executions pipeline, but XXX are dedicated to half UniQueue, so  only  3 instructions per cycle can be executed, 
%one fixed-point instruction on the Fixed-Point Unit, one load, store or simple  fixed-point instruction (Addition and logical instructions) on the Load/Store unit (LSU). One load or simple-fixed instruction on the Load unit (LU) and one scalar/vector instruction on the Vector Scalar Unit. 
%
%The Power8 has 16 execution pipelines, but  6 are dedicated to half UniQueue, so  only  4 instructions per cycle can be executed, one fixed-point instructions on the Fixed-Point Unit,
%one load, store or simple  fixed-point instruction (Addition and logical instructions) on the Load/Store unit (LSU)  and one scalar/vector instructions on the Vector Scalar Unit. The 4 other executions pipelines are dedicated to the cryptography, the branch execution, the condition logical and the decimal floating point operations. 


\subsection{Latency, throughput definitions}

The latency of an instruction is the delay that the instruction generates in a dependency chain. 

The throughput is the maximum number of instructions of the same kind that can be executed per clock cycle
when the operands of each instruction are independent of the preceding instructions.

For example, a reciprocal throughput of 2 for FMUL means that a new FMUL instruction can start executing
 clock cycles after a previous FMUL. A reciprocal throughput of 0.33 for ADD means that the execution units can handle 3 integer additions per clock cycle.

The latency and the throughput of the main instructions for all platforms are reported in Table~\ref{latencyvendor}.

\subsection{Precision}

We compute the precision using the Unit of Least Precision (ULP) \cite{muller:inria-00070503}.
The standard definition is: \textit{ulp(x) is the gap between the two floating-point numbers closest to $x$, even if $x$ is one of them.}
The precision for the implementation of the exponential function will depend only on the error arising from the polynomial evaluation, because the evaluation of $2^k$ uses the exact hardware representation of the floating point numbers, and the boundaries conditions 
only introduce bit  manipulation tricks. In this work, we compute the ULP using the boost library, more specially the \texttt{boost::math}
package.

\subsection{measurement tools}

Measuring the throughput/latency is not a trivial task, especially for the latency. A direct measurement is not trivial due 
to the large number of variables which may impact the measurements. 
Therefore, the approach used within this study consists of performing indirect measurements by mapping the instructions associated to their
mnemonics into a for loop whose unroll factor was parameterizable (between 1 and 5). 
To avoid introducing some operation throughput effect which would corrupt the measurement of latency, a dependency between instructions 
throughout the loop iterations has been artificially enforced. 

Within this study, all benchmarks were compiled using GCC and repeated 100 times. 
%The results of this experiment are reported in Figure~\ref{fig.LATENCY_OPS} in function of the used unroll factor.  
The experimental data has been fitted to a linear distribution ($y=ax+b$ where $a$ corresponds to the slope of the curve which is equal to 100 times the actual 
latency of the instruction).  Latency measurements have been carried out for all basic instructions +, $\times$ and FMA and compared to the 
vendor results to validate our method with success. Latency of the previous instructions are reported in Table~\ref{latencyvendor}.

%\begin{figure}[h]
%\begin{center}
%\includegraphics[scale=0.7]{platency.eps} %\vspace{-1cm}
%\caption{Value of latencies using a linear approximation for vectorial double precision instructions on Power8: $+$. 
%The slop $a$ represents the latency of the instructions. The residual $b$ of 60 cycle in 0, corresponds to the the additional workload in cycle introduced 
%by the program/processor. 
%\label{fig.LATENCY_OPS}}
%\end{center}
%\end{figure}

\begin{table}[ht]
\begin{center}
%\begin{tabular}{l c c c c c c }
%			& \multicolumn{2}{c}{SandyBridge} & \multicolumn{2}{c}{Broadwell}  & \multicolumn{2}{c}{Power 7/8}\\ 
%                         & Thr.  & Lat. & Thr. & Lat. & Thr. & Lat.  \\
%		          \hline
%add                   &  1 & 3  &   0.8 & 3 &  1 & 6     \\
%mul                   &  1 & 5  &   0.5 & 5 &  1 & 6   \\
%FMA                 &   -  & -  &   0.5 & 5  &  1 & 6   \\
%		             \hline
%\end{tabular}
\begin{tabular}{l c c c c  }
			& \multicolumn{2}{c}{SandyBridge} & \multicolumn{2}{c}{Broadwell}  \\ 
                         & Thr.  & Lat. & Thr. & Lat. \\
		          \hline
add                   &  1 & 3  &   0.8 & 3    \\
mul                   &  1 & 5  &   0.5 & 5    \\
FMA                 &   -  & -  &   0.5 & 5    \\
		             \hline
\end{tabular}
\end{center}
\caption{Latency/Throughput add/mul/fma on the Broadwell architecture. \label{latencyvendor}}
\end{table}%

A full review  of all x86 instructions can be found in \cite{agner} where fragmented information can be founded in \cite{Sinharoy:2011:IPM:2001058.2001059, Power8}
To conclude, Intel has released in the last years the IACA tool \cite{iaca}. This tool was nice to perform latency and throughput analysis, and
obtain predictions on several Intel processor target. Unfortunately the tool has not been maintained since 2012, date of its last release. 
However, we utilize the tool to generate the Direct Acyclic Graph of the polynomial evaluation on the Broadwell platform, Figure~\ref{DAG}.

\subsection{Programming model}

Programming polynomial evaluation and $e^x$ for high performance computing is not trivial because the code can not be polluted by additional workload, and useless
instructions. A solution would be to write directly in assembly language: the solution is conceivable although it is very time consuming. 
We consider the alternative of using meta-programming associated to a recursive algorithm. It simplifies the generation of all polynomial evaluations
as soon as all possible methods have been programmed. Moreover, as the algorithms are generic, they can be plugged into any DSL to generate 
a SIMD code. The algorithms are successively instantiated with the basic type double and a generic vector class coming from the Cyme library.
To control the quality of our ASM at least on x86 platform we extract the DAG (Figure~\ref{DAG}) using the IACA tool, for example for the evaluation
of the first order Horner method on a polynomial of degree 10. The match is perfect between the theory and the generated assembly.

\begin{figure}[t]
\subfloat[]
{
%\resizebox{.1\textwidth}{.9\textwidth}{%
\begin{tikzpicture}[>=latex',line join=bevel,scale=0.27]
%%
\node (11) at (213.0bp,18.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {11. mov};
  \node (10) at (213.0bp,90.0bp) [draw=pink,fill=pink,circle,scale=0.3] {10. FMA};
  \node (1) at (213.0bp,738.0bp) [draw=pink,fill=pink,circle,scale=0.3] {1. FMA};
  \node (0) at (213.0bp,810.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {0. mov};
  \node (3) at (213.0bp,594.0bp) [draw=pink,fill=pink,circle,scale=0.3] {3. FMA};
  \node (2) at (213.0bp,666.0bp) [draw=pink,fill=pink,circle,scale=0.3] {2. FMA};
  \node (5) at (213.0bp,450.0bp) [draw=pink,fill=pink,circle,scale=0.3] {5. FMA};
  \node (4) at (213.0bp,522.0bp) [draw=pink,fill=pink,circle,scale=0.3] {4. FMA};
  \node (7) at (213.0bp,306.0bp) [draw=pink,fill=pink,circle,scale=0.3] {7. FMA};
  \node (6) at (213.0bp,378.0bp) [draw=pink,fill=pink,circle,scale=0.3] {6. FMA};
  \node (9) at (213.0bp,162.0bp) [draw=pink,fill=pink,circle,scale=0.3] {9. FMA};
  \node (8) at (213.0bp,234.0bp) [draw=pink,fill=pink,circle,scale=0.3] {8. FMA};
  \draw [->] (5) ..controls (213.0bp,423.98bp) and (213.0bp,414.71bp)  .. (6);
  \draw [->] (2) ..controls (213.0bp,639.98bp) and (213.0bp,630.71bp)  .. (3);
  \draw [->] (7) ..controls (213.0bp,279.98bp) and (213.0bp,270.71bp)  .. (8);
  \draw [->] (6) ..controls (213.0bp,351.98bp) and (213.0bp,342.71bp)  .. (7);
  \draw [->] (4) ..controls (213.0bp,495.98bp) and (213.0bp,486.71bp)  .. (5);
  \draw [->] (8) ..controls (213.0bp,207.98bp) and (213.0bp,198.71bp)  .. (9);
  \draw [->] (1) ..controls (213.0bp,711.98bp) and (213.0bp,702.71bp)  .. (2);
  \draw [->] (9) ..controls (213.0bp,135.98bp) and (213.0bp,126.71bp)  .. (10);
  \draw [->] (0) ..controls (213.0bp,783.98bp) and (213.0bp,774.71bp)  .. (1);
  \draw [->] (3) ..controls (213.0bp,567.98bp) and (213.0bp,558.71bp)  .. (4);
  \draw [->] (10) ..controls (213.0bp,63.983bp) and (213.0bp,54.712bp)  .. (11);
%
\end{tikzpicture}
%}
}
\subfloat[]
{
\begin{tikzpicture}[>=latex',line join=bevel,,scale=0.27]
%%
\node (11) at (229.0bp,519.0bp) [draw=pink,fill=pink,circle,scale=0.3] {11. $\times$};
  \node (10) at (372.0bp,283.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {10. mov};
  \node (13) at (130.0bp,639.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {13. mov};
  \node (12) at (227.0bp,401.0bp) [draw=pink,fill=pink,circle,scale=0.3] {12. $\times$};
  \node (15) at (131.0bp,401.0bp) [draw=pink,fill=pink,circle,scale=0.3] {15. FMA};
  \node (14) at (130.0bp,519.0bp) [draw=pink,fill=pink,circle,scale=0.3] {14. FMA};
  \node (17) at (280.0bp,161.0bp) [draw=pink,fill=pink,circle,scale=0.3] {17. FMA};
  \node (16) at (158.0bp,283.0bp) [draw=pink,fill=pink,circle,scale=0.3] {16. FMA};
  \node (18) at (280.0bp,41.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {18. mov};
  \node (1) at (35.0bp,639.0bp) [draw=pink,fill=pink,circle,scale=0.3] {1. FMA};
  \node (0) at (35.0bp,752.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {0. mov};
  \node (3) at (38.0bp,401.0bp) [draw=pink,fill=pink,circle,scale=0.3] {3. FMA};
  \node (2) at (35.0bp,519.0bp) [draw=pink,fill=pink,circle,scale=0.3] {2. FMA};
  \node (5) at (280.0bp,283.0bp) [draw=pink,fill=pink,circle,scale=0.3] {5. $\times$};
  \node (4) at (248.0bp,639.0bp) [draw=pink,fill=pink,circle,scale=0.3] {4. $\times$};
  \node (7) at (372.0bp,639.0bp) [draw=pink,fill=pink,circle,scale=0.3] {7. FMA};
  \node (6) at (372.0bp,752.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {6. mov};
  \node (9) at (372.0bp,401.0bp) [draw=pink,fill=pink,circle,scale=0.3] {9. FMA};
  \node (8) at (372.0bp,519.0bp) [draw=pink,fill=pink,circle,scale=0.3] {8. FMA};
  \draw [->,solid] (17) ..controls (280.0bp,109.5bp) and (280.0bp,100.4bp)  .. (18);
  \draw [->,solid] (4) ..controls (267.3bp,594.26bp) and (273.1bp,577.62bp)  .. (276.0bp,562.0bp) .. controls (291.24bp,479.97bp) and (287.72bp,381.97bp)  .. (5);
  \draw [->,solid] (14) ..controls (130.44bp,467.23bp) and (130.52bp,458.16bp)  .. (15);
  \draw [->,solid] (16) ..controls (204.58bp,236.18bp) and (225.05bp,216.05bp)  .. (17);
  \draw [->,solid] (4) ..controls (240.85bp,593.6bp) and (238.66bp,579.97bp)  .. (11);
  \draw [->,solid] (7) ..controls (372.0bp,588.43bp) and (372.0bp,575.18bp)  .. (8);
  \draw [->,solid] (6) ..controls (372.0bp,707.13bp) and (372.0bp,697.21bp)  .. (7);
  \draw [->,solid] (15) ..controls (141.65bp,354.23bp) and (143.95bp,344.36bp)  .. (16);
  \draw [->,solid] (2) ..controls (36.172bp,472.7bp) and (36.54bp,458.45bp)  .. (3);
  \draw [->,solid] (8) ..controls (372.0bp,472.7bp) and (372.0bp,458.45bp)  .. (9);
  \draw [->,solid] (3) ..controls (78.351bp,360.99bp) and (100.96bp,339.14bp)  .. (16);
  \draw [->,solid] (13) ..controls (130.0bp,590.01bp) and (130.0bp,581.05bp)  .. (14);
  \draw [->,solid] (10) ..controls (336.87bp,236.18bp) and (323.75bp,219.07bp)  .. (17);
  \draw [->,solid] (5) ..controls (280.0bp,238.16bp) and (280.0bp,225.98bp)  .. (17);
  \draw [->,solid] (9) ..controls (372.0bp,356.71bp) and (372.0bp,345.03bp)  .. (10);
  \draw [->,solid] (12) ..controls (200.65bp,355.7bp) and (192.43bp,341.89bp)  .. (16);
  \draw [->,solid] (0) ..controls (35.0bp,705.68bp) and (35.0bp,694.02bp)  .. (1);
  \draw [->,solid] (11) ..controls (228.19bp,470.82bp) and (228.0bp,460.08bp)  .. (12);
  \draw [->,solid] (1) ..controls (35.0bp,592.2bp) and (35.0bp,577.16bp)  .. (2);
%
\end{tikzpicture}
}
\subfloat[]
{
\begin{tikzpicture}[>=latex',line join=bevel,scale=0.27]
%%
\node (11) at (44.0bp,446.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {11. mov};
  \node (10) at (445.0bp,580.0bp) [draw=pink,fill=pink,circle,scale=0.3] {10. FMA};
  \node (13) at (248.0bp,446.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {13. mov};
  \node (12) at (44.0bp,307.0bp) [draw=pink,fill=pink,circle,scale=0.3] {12. FMA};
  \node (15) at (146.0bp,307.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {15. mov};
  \node (14) at (248.0bp,307.0bp) [draw=pink,fill=pink,circle,scale=0.3] {14. FMA};
  \node (17) at (322.0bp,44.0bp) [draw=pink,fill=pink,circle,scale=0.3] {17. FMA};
  \node (16) at (228.0bp,173.0bp) [draw=pink,fill=pink,circle,scale=0.3] {16. FMA};
  \node (19) at (580.0bp,307.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {19. mov};
  \node (18) at (555.0bp,446.0bp) [draw=pink,fill=pink,circle,scale=0.3] {18. $\times$};
  \node (1) at (559.0bp,932.0bp) [draw=pink,fill=pink,circle,scale=0.3] {1. FMA};
  \node (0) at (559.0bp,1044.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {0. mov};
  \node (3) at (289.0bp,816.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {3. mov};
  \node (2) at (525.0bp,816.0bp) [draw=pink,fill=pink,circle,scale=0.3] {2. FMA};
  \node (5) at (397.0bp,307.0bp) [draw=pink,fill=pink,circle,scale=0.3] {5. $\times$};
  \node (4) at (466.0bp,700.0bp) [draw=pink,fill=pink,circle,scale=0.3] {4. FMA};
  \node (7) at (716.0bp,446.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {7. mov};
  \node (6) at (339.0bp,173.0bp) [draw=pink,fill=pink,circle,scale=0.3] {6. $\times$};
  \node (9) at (648.0bp,173.0bp) [draw=pink,fill=pink,circle,scale=0.3] {9. FMA};
  \node (8) at (716.0bp,307.0bp) [draw=pink,fill=pink,circle,scale=0.3] {8. FMA};
  \draw [->,solid] (18) ..controls (492.54bp,390.84bp) and (464.25bp,366.31bp)  .. (5);
  \draw [->,solid] (18) ..controls (566.61bp,381.37bp) and (568.9bp,368.82bp)  .. (19);
  \draw [->,solid] (15) ..controls (176.84bp,256.35bp) and (188.88bp,236.98bp)  .. (16);
  \draw [->,solid] (18) ..controls (570.99bp,576.9bp) and (584.84bp,728.08bp)  .. (573.0bp,856.0bp) .. controls (572.18bp,864.82bp) and (570.84bp,874.14bp)  .. (1);
  \draw [->,solid] (10) ..controls (485.62bp,530.25bp) and (500.71bp,512.15bp)  .. (18);
  \draw [->,solid] (6) ..controls (330.26bp,258.85bp) and (328.92bp,311.31bp)  .. (339.0bp,356.0bp) .. controls (353.78bp,421.53bp) and (390.18bp,490.39bp)  .. (10);
  \draw [->,solid] (1) ..controls (545.05bp,884.22bp) and (541.99bp,873.95bp)  .. (2);
  \draw [->,solid] (11) ..controls (44.0bp,391.77bp) and (44.0bp,376.3bp)  .. (12);
  \draw [->,solid] (0) ..controls (559.0bp,999.87bp) and (559.0bp,990.83bp)  .. (1);
  \draw [->,solid] (8) ..controls (689.62bp,254.8bp) and (679.36bp,234.87bp)  .. (9);
  \draw [->,solid] (14) ..controls (239.76bp,251.62bp) and (237.85bp,239.01bp)  .. (16);
  \draw [->,solid] (5) ..controls (410.95bp,221.12bp) and (413.25bp,167.3bp)  .. (396.0bp,124.0bp) .. controls (389.2bp,106.93bp) and (376.79bp,91.366bp)  .. (17);
  \draw [->,solid] (3) ..controls (347.72bp,777.18bp) and (392.34bp,748.44bp)  .. (4);
  \draw [->,solid] (9) ..controls (586.61bp,205.32bp) and (551.84bp,228.33bp)  .. (531.0bp,258.0bp) .. controls (473.52bp,339.85bp) and (454.45bp,458.04bp)  .. (10);
  \draw [->,solid] (18) ..controls (544.94bp,570.41bp) and (534.64bp,696.76bp)  .. (2);
  \draw [->,solid] (2) ..controls (501.25bp,769.1bp) and (494.62bp,756.29bp)  .. (4);d
  \draw [->,solid] (17) ..controls (377.93bp,83.18bp) and (400.95bp,102.52bp)  .. (417.0bp,124.0bp) .. controls (477.55bp,205.06bp) and (517.78bp,316.76bp)  .. (18);
  \draw [->,solid] (18) ..controls (451.15bp,422.98bp) and (368.62bp,405.8bp)  .. (297.0bp,392.0bp) .. controls (208.31bp,374.91bp) and (178.05bp,395.85bp)  .. (97.0bp,356.0bp) .. controls (91.648bp,353.37bp) and (86.437bp,350.03bp)  .. (12);
  \draw [->,solid] (7) ..controls (716.0bp,393.9bp) and (716.0bp,374.42bp)  .. (8);
  \draw [->,solid] (5) ..controls (334.49bp,262.88bp) and (305.62bp,241.97bp)  .. (281.0bp,222.0bp) .. controls (276.49bp,218.34bp) and (271.86bp,214.42bp)  .. (16);
  \draw [->,solid] (13) ..controls (248.0bp,391.77bp) and (248.0bp,376.3bp)  .. (14);
  \draw [->,solid] (18) ..controls (615.96bp,394.95bp) and (640.38bp,374.59bp)  .. (662.0bp,356.0bp) .. controls (667.62bp,351.16bp) and (673.53bp,346.0bp)  .. (8);
  \draw [->,solid] (5) ..controls (372.75bp,250.8bp) and (367.38bp,238.59bp)  .. (6);
  \draw [->,solid] (3) ..controls (244.81bp,711.63bp) and (167.61bp,534.97bp)  .. (93.0bp,392.0bp) .. controls (86.463bp,379.47bp) and (78.964bp,366.16bp)  .. (12);
  \draw [->,solid] (4) ..controls (457.65bp,652.08bp) and (456.0bp,642.81bp)  .. (10);
  \draw [->,solid] (16) ..controls (265.13bp,121.83bp) and (278.34bp,103.98bp)  .. (17);
  \draw [->,solid] (12) ..controls (87.475bp,227.8bp) and (127.44bp,165.72bp)  .. (175.0bp,124.0bp) .. controls (203.6bp,98.917bp) and (241.53bp,78.783bp)  .. (17);
  \draw [->,solid] (18) ..controls (529.27bp,533.04bp) and (513.76bp,581.81bp)  .. (498.0bp,624.0bp) .. controls (494.32bp,633.85bp) and (490.08bp,644.3bp)  .. (4);
  \draw [->,solid] (18) ..controls (461.37bp,408.97bp) and (395.19bp,382.33bp)  .. (339.0bp,356.0bp) .. controls (324.67bp,349.29bp) and (309.36bp,341.47bp)  .. (14);
  \draw [->,solid] (18) ..controls (606.89bp,393.91bp) and (620.81bp,375.44bp)  .. (629.0bp,356.0bp) .. controls (646.87bp,313.59bp) and (650.6bp,260.89bp)  .. (9);
%
\end{tikzpicture}
}
\caption{Broadwell mnemonic DAG of the polynomial evaluation for three methods of evaluation (latency/throughput), (a) classical Horner (50.0/3.55 [cycle]), (b) Estrin (22.68/2.90 [cycle]), (c)  Estrin$^6 \times$BruteForce$^4$ (28.94/3.11 [cycle])  \label{DAG}}
\end{figure}

\section{Processors and Compiler}

This work focuses on the last release of Intel Xeon processor (Broadwell) and an oldest generation (SandyBridge). We privileged the intel architecture 
because it is well know and  well documented. The compiler is GCC. All informations are summarised in the table \ref{env_compil}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{ l c c c c }
\hline
Architecture   & Model & FMA & Compiler & Options Compilation \\
SandyBridge & E5-2670 & no   & GCC 4.9  & \texttt{-O3 -march=native } \\
                      &                &        &                 & \texttt{-fast-math -fabi-version=6}  \\
BroadWell     & E5-2630 & yes & GCC 4.9.3  &  \texttt{-O3 -march=native -fma} \\
                      &                &        &                 & \texttt{-fast-math -fabi-version=6}  \\
\hline
\end{tabular}
\caption{Description of the processors, the compilers and the compilation options. \label{env_compil}} 
\end{center}
\end{table}

The two options \texttt{-fast-math} and \texttt{-fma} are mandatory to better computation of $x$ to the power of $n$ (as presented in the section \ref{powern}) and the utilization of the FMA for the scalar version. And finally the option \texttt{-fabi-version=6} for a usage of GCC vector Instructions through Built-in Functions.

 
\section{Results}
\subsection{Polynomials, preliminary analysis and validations}

Build an efficient theoretical method is not trivial, and vendor simulator performance are not released on the market. 
Nevertheless a few experimental data for latency measurement with the simplest polynomial evaluation can be verified easily. 
The classical Horner's method  provides a dependency chain between all the operations (As shown by the AMS DAG, Figure \ref{DAG}), therefore any instructions
can be only executed if the previous one is finished. The classical Horner's method gives 10 successives mull/add or 
FMA with dependency,  Therefore the theoretical total latency of the polynomial is: $ 10 [\textrm{mul+add}]_{latency}$ or $ 10 [\textrm{FMA}]_{latency}$
if the machine supports natively the FMA operations. The measurement gives 50.03 [cycle] on the Broadwell architecture and 80.01 [cycle] on the SandyBridge 
architecture which correspond perfectly on the tiny model proposes  previously.

Last but not least the simple factorizations combining the classical horner can be also modeled. Considering   $P^6_{h^1}P^4_{h^1}$, as the operations
mul, add and FMA are fully pipelined, the computation of the polynomial of lower degree should be fully hidden by the polynomial of larger degree.
In such situation the total latency of the produce of the polynomial is $ 6 [\textrm{mul+add}]_{latency} + [\textrm{mul}]_{latency}$ or $ 6 [\textrm{FMA}]_{latency}+ [\textrm{mul}]_{latency}$.
The last multiplication corresponds to the final product between the two polynomials, it is a blocking operations. The measurement gives  36.00 [cycle] on the Broadwell 
architecture 53.04 [cycle] on the SandyBridge, one more time the results are perfects.  This first analysis shows already two informations, the factorization and the programming model is efficient.

\subsection{Polynomials, latency analysis} The totality of the results latency/throughput for the polynomial evaluation on all platforms are presented on the figures \ref{PLOT_LT0}. 
The results are not distributed uniformly. For the scalar version they are distributed around 30 [cycle] (BroadWell) and 40 [cycle] (SandyBriddge). For the vectorial version, the distribution is more regular. In details, some packets of points appear (confirm by the table \ref{LTR_EXP_0} and \ref{LTR_EXP_1}. It should be an compiler artefact, specially for the scalar version. The -fast-math reorganise the computation  of the power of $n$, as all the version are just slightly different (confirm by a fast quick look on the disassemble ,code), considering the out of order execution and the renaming of the register, it is clear that a few version give the same results. Oppositely for the vectorial version these packets are less visible because SIMD intrinsics programming prevent any optimization of the -fast-math, and  every algorithm is more unique.  

For the Latency measurement, as excepted for all platform the longest latency concerns the classical Horner method due to the chain dependency. Considering the Horner method like the reference performance point the spectra of the results are large. The best results can out-perform by a factor lightly larger than 2 for the SandyBridge. In details if we have a look on the 10 best  results in the Table \ref{LTR_EXP_0} and \ref{LTR_EXP_1}, as excepted the factorisation is a good pattern for improve the performance. 

\subsection{Polynomials, throughput analysis}

The throughput analysis is similar to the latency one, but with one more additional information. Factorization pattern increase slightly the throughput in cycle, it can be
easily understand. The  factorisation pattern  improves the parallelism and consequently a better fill up of the pipeline, therefore it takes more cycle to restart a new operations
as the pipelines are full.

\subsection{Polynomials, precision analysis}
The ULP error estimator is presented on the Figure  \ref{PRECISION_ULP}. Errors is dispatched in two group. First group with a small ULP 2/3 or a larger one around 8/9.
This behaviour is explained easily. The first group concerns all the polynomial of type $P^{10}_m$, no factorisation has been applied on it. Contrary to the $2^{nd}$ group 
where the factorisation has been applied. This processus was performed with Matlab software, and it necessitates resolution of quadratic equation  it introduces numerical error.
of the determinant due to the round-off error. Kahan the father of IEEE floating point investigated the phenomena on simple quadratic \cite{Kahan2002}, 
consequently it should be possible to improve the ULP of all factorisation pattern. 

More surprisingly, the vector presents a shift of 1 ULP compare to the scalar version. Although we do not have a clear idea of this shift, for the scalar
version the compiler has more freedom than the vector version where  the SIMD intrinsics associated to the algorithm fix the operations executions.
In the scalar version, the compiler reorganise the operations following is setting in policy, we think the generated is done to maximise the performance and
minimise the error.

\subsection{exponentials}

give results compare to vendors intel svml and mass



\begin{figure}[h]
\mbox{
\hspace{-2cm}
\includegraphics[scale=0.7]{allht_scalar.eps} 
\hspace{-2.5cm}
\includegraphics[scale=0.7]{allht_vector.eps}
}
\caption{Latency/Throughput for  all architectures, scalar version left and vectorial version right \label{PLOT_LT0} }
\end{figure}

\begin{figure}[h]
\mbox{
\vspace{-2.5cm}
\includegraphics[scale=0.7]{hw_histo_vector_l_sb.eps} 
\hspace{-2.5cm}
\includegraphics[scale=0.7]{hw_histo_vector_t_sb.eps} 
}
\mbox{
\vspace{-2.5cm}
\includegraphics[scale=0.7]{hw_histo_vector_l_hw.eps} 
\hspace{-2.5cm}
\includegraphics[scale=0.7]{hw_histo_vector_t_hw.eps} 
}
\caption{Latency and Throughput frequency of the vectorial version. The analysis is done for the original polynomial or the factorization associated. Sampling interval of 1 for the latency and 0.1 for the throughput.}
\end{figure}


\begin{figure}[h]
\mbox{
\hspace{-2cm}
\includegraphics[scale=0.7]{all_precision_scalar.eps} 
\hspace{-2.5cm}
\includegraphics[scale=0.7]{all_precision_vector.eps}
}
\caption{Histogram of the precision of all polynomial evaluation for scalar version (left) and vector version (right). The bar of 
the histogram is the total number os polynomial evaluations for a given ULP \label{PRECISION_ULP}}
\end{figure}



\begin{table}[ht]
\begin{center}
\begin{tabular}{ l c c c  l c c  c }
\hline
		       & \multicolumn{3}{c}{scalar} & &  \multicolumn{3}{c}{vector} \\
		       &   \multicolumn{3}{c}{Throughput Criteria} &   &  \multicolumn{3}{c}{Throughput Criteria} \\
 Algorithm        & Th.  & La.      & ULP  & Algorithm                & Th.   & La.     & ULP \\ 	
$P_{h^4}^{6}P_e^{2}P_{h^1}^{2}$ & 6.08 & 35.10 & 8  & $P_b^{2}P_{h^2}^{4}P_{h^2}^{4}$ & 5.95 & 34.77 & 9 \\
$P_{h^4}^{6}P_{h^1}^{2}P_{h^1}^{2}$ & 6.20 & 36.03 & 8  & $P_b^{2}P_{h^1}^{4}P_{h^1}^{4}$ & 6.13 & 43.70 & 9 \\
$P_{h^4}^{6}P_{h^1}^{4}$ & 6.23 & 37.82 & 8  & $P_b^{2}P_{h^1}^{4}P_{h^2}^{4}$ & 6.24 & 43.06 & 10 \\
$P_{h^4}^{6}P_e^{4}$ & 6.26 & 34.12 & 8  & $P_b^{2}P_e^{4}P_{h^2}^{4}$ & 6.37 & 31.56 & 11 \\
$P_{h^3}^{6}P_e^{2}P_e^{2}$ & 6.27 & 39.17 & 8  & $P_b^{2}P_e^{2}P_e^{2}P_e^{2}P_{h^1}^{2}$ & 6.37 & 40.05 & 9 \\
$P_{h^4}^{6}P_{h^2}^{4}$ & 6.28 & 34.18 & 8  & $P_b^{2}P_e^{4}P_{h^1}^{4}$ & 6.39 & 37.27 & 10 \\
$P_{h^3}^{6}P_b^{2}P_b^{2}$ & 6.30 & 39.18 & 9  & $P_b^{2}P_e^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 6.41 & 39.73 & 9 \\
$P_{h^3}^{6}P_{h^1}^{2}P_{h^1}^{2}$ & 6.31 & 38.93 & 9  & $P_b^{2}P_e^{2}P_e^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 6.44 & 40.05 & 9 \\
$P_{h^5}^{6}P_b^{2}P_{h^1}^{2}$ & 6.32 & 38.81 & 8  & $P_b^{2}P_b^{2}P_b^{2}P_b^{2}P_b^{2}$ & 6.46 & 42.46 & 8 \\
$P_{h^5}^{6}P_b^{2}P_b^{2}$ & 6.33 & 38.87 & 8  & $P_b^{2}P_b^{2}P_e^{2}P_e^{2}P_e^{2}$ & 6.49 & 41.38 & 9 \\
		       & \multicolumn{3}{c}{Latency Criteria} &           &  \multicolumn{3}{c}{Latency Criteria}\\
$P_e^{6}P_b^{4}$ & 7.00 & 29.03 & 9  & $P_e^{10}$ & 7.09 & 29.07 & 4 \\
$P_e^{6}P_e^{4}$ & 6.99 & 29.72 & 8  & $P_e^{6}P_e^{4}$ & 8.22 & 29.15 & 10 \\
$P_{h^1}^{2}P_b^{4}P_e^{4}$ & 7.46 & 30.16 & 9  & $P_e^{6}P_{h^2}^{4}$ & 7.32 & 29.15 & 9 \\
$P_{h^1}^{2}P_e^{4}P_{h^2}^{4}$ & 7.12 & 31.20 & 9  & $P_e^{2}P_e^{4}P_{h^2}^{4}$ & 7.74 & 30.05 & 9 \\
$P_e^{2}P_b^{4}P_e^{4}$ & 7.47 & 31.67 & 9  & $P_{h^1}^{2}P_e^{4}P_{h^2}^{4}$ & 7.68 & 30.07 & 10 \\
$P_b^{4}P_b^{2}P_e^{2}P_{h^1}^{2}$ & 7.33 & 31.70 & 10  & $P_e^{6}P_b^{4}$ & 7.58 & 30.65 & 8 \\
$P_b^{4}P_b^{2}P_b^{2}P_{h^1}^{2}$ & 7.34 & 31.70 & 10  & $P_b^{2}P_e^{4}P_{h^2}^{4}$ & 6.37 & 31.56 & 11 \\
$P_e^{4}P_e^{2}P_e^{2}P_{h^1}^{2}$ & 7.36 & 31.70 & 10  & $P_e^{8}P_e^{2}$ & 7.53 & 32.27 & 8 \\
$P_e^{4}P_b^{2}P_b^{2}P_{h^1}^{2}$ & 7.20 & 31.71 & 9  & $P_e^{8}P_{h^1}^{2}$ & 7.49 & 33.05 & 8 \\
$P_b^{4}P_e^{2}P_e^{2}P_{h^1}^{2}$ & 7.36 & 31.71 & 12  & $P_e^{6}P_e^{2}P_{h^1}^{2}$ & 7.48 & 33.05 & 9 \\
\hline
\end{tabular}
\end{center}
\caption{Best Latency/throughput [cycle] polynomial evaluation on SandyBridge platform. The criteria indicates the sort of the tuple (throughput, latency, ulp), 
on a specific part \label{LTR_EXP_0}}
\end{table}%

\begin{table}[ht]
\begin{center}
\begin{tabular}{ l c c c  l c c  c }
\hline
		       & \multicolumn{3}{c}{scalar}              &           &  \multicolumn{3}{c}{vector}\\
		       & \multicolumn{3}{c}{Throughput Criteria} &           &  \multicolumn{3}{c}{Throughput Criteria}\\
 Algorithm             & Th.  & La.     & ULP                    & Algorithm & Th.   & La.     & ULP\\
$P_{h^1}^{2}P_e^{4}P_{h^1}^{4}$ & 3.54 & 25.06 & 9  & $P_e^{6}P_e^{4}$ & 3.30 & 20.99 & 8 \\
$P_{h^1}^{2}P_b^{4}P_{h^1}^{4}$ & 3.54 & 25.03 & 8  & $P_e^{6}P_{h^2}^{4}$ & 3.40 & 21.03 & 9 \\
$P_e^{2}P_e^{4}P_{h^1}^{4}$ & 3.54 & 25.03 & 8  & $P_e^{6}P_{h^1}^{4}$ & 3.40 & 23.67 & 8 \\
$P_e^{6}P_e^{2}P_e^{2}$ & 3.54 & 25.59 & 9  & $P_e^{10}$ & 3.41 & 22.65 & 4 \\
$P_e^{4}P_b^{2}P_b^{2}P_b^{2}$ & 3.54 & 23.17 & 9  & $P_{h^2}^{6}P_{h^1}^{4}$ & 3.45 & 26.28 & 8 \\
$P_e^{4}P_b^{2}P_e^{2}P_e^{2}$ & 3.54 & 23.17 & 10  & $P_{h^2}^{6}P_{h^2}^{4}$ & 3.46 & 27.27 & 8 \\
$P_e^{4}P_e^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 3.54 & 23.50 & 9  & $P_e^{6}P_e^{2}P_e^{2}$ & 3.46 & 23.80 & 9 \\
$P_e^{4}P_b^{2}P_b^{2}P_e^{2}$ & 3.54 & 23.19 & 10  & $P_e^{2}P_e^{4}P_e^{4}$ & 3.47 & 23.65 & 10 \\
$P_e^{4}P_b^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 3.54 & 23.57 & 9  & $P_e^{8}P_e^{2}$ & 3.47 & 26.01 & 9 \\
$P_e^{6}P_b^{2}P_e^{2}$ & 3.55 & 25.58 & 8  & $P_e^{2}P_{h^1}^{4}P_{h^2}^{4}$ & 3.49 & 26.45 & 9 \\
		       & \multicolumn{3}{c}{Latency Criteria} &           &  \multicolumn{3}{c}{Latency Criteria}\\
$P_b^{2}P_b^{2}P_b^{2}P_b^{2}P_{h^1}^{2}$ & 3.64 & 21.46 & 9  & $P_e^{6}P_e^{4}$ & 3.30 & 20.99 & 8 \\
$P_b^{2}P_e^{2}P_e^{2}P_e^{2}P_{h^1}^{2}$ & 5.03 & 21.46 & 9  & $P_e^{6}P_{h^2}^{4}$ & 3.40 & 21.03 & 9 \\
$P_b^{2}P_b^{2}P_b^{2}P_e^{2}P_{h^1}^{2}$ & 3.63 & 21.46 & 10  & $P_{h^1}^{2}P_e^{4}P_{h^2}^{4}$ & 3.57 & 22.43 & 9 \\
$P_b^{2}P_b^{2}P_e^{2}P_e^{2}P_{h^1}^{2}$ & 3.63 & 21.47 & 8  & $P_e^{2}P_e^{4}P_{h^2}^{4}$ & 3.56 & 22.62 & 10 \\
$P_b^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 3.58 & 21.48 & 7  & $P_e^{10}$ & 3.41 & 22.65 & 4 \\
$P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 3.62 & 21.48 & 8  & $P_{h^1}^{2}P_e^{4}P_e^{4}$ & 3.49 & 23.29 & 9 \\
$P_e^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 3.58 & 21.48 & 8  & $P_b^{2}P_e^{4}P_{h^2}^{4}$ & 3.85 & 23.33 & 10 \\
$P_e^{2}P_e^{2}P_e^{2}P_e^{2}P_{h^1}^{2}$ & 3.63 & 21.51 & 9  & $P_e^{2}P_e^{4}P_e^{4}$ & 3.47 & 23.65 & 10 \\
$P_b^{2}P_b^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 3.60 & 21.64 & 8  & $P_e^{6}P_{h^1}^{4}$ & 3.40 & 23.67 & 8 \\
$P_b^{2}P_e^{2}P_e^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 5.05 & 21.64 & 8  & $P_e^{6}P_e^{2}P_{h^1}^{2}$ & 3.53 & 23.74 & 9 \\

\hline
\end{tabular}
\end{center}
\caption{Best Latency/throughput [cycle] polynomial evaluation on Broadwell platform. The criteria indicates the sort the tuple (throughput, latency, ulp),
on a specific part \label{LTR_EXP_1}}
\end{table}%

\section{Conclusions}

factorisation is good

\bibliographystyle{elsarticle-num}
\bibliography{bib/modeling}

\end{document}
