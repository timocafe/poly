\documentclass[preview]{elsarticle}

\usepackage{amssymb}
\usepackage[table]{xcolor}
\usepackage{subfig}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{pgfplots}
\usepackage{mathtools}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\usetikzlibrary{arrows, calc, positioning, patterns, pgfplots.polar}

\usetikzlibrary{snakes,arrows,shapes}

\title{Polynomial evaluation on super scalar architecture, applied to the elementary function $e^x$}

\author[rvt]{T. Ewart }
\ead{timothee.ewart@epfl.ch}

\author[rvt]{S. Yates}
\ead{sam.yates@epfl.ch}

%\author[rvt]{F.  Sch\"urmann }
%\ead{felix.schuermann@epfl.ch}

\author[rvt]{F. Delalondre}
\ead{fabien.delalondre@epfl.ch}

\author[rvt]{F.  Sch\"urmann }
\ead{felix.schuermann@epfl.ch}


\address[rvt]{Blue Brain Project, Campus Biotech, Ch. des Mines 9. CH-1212 Gen\`eve}


\graphicspath{{plot/figures/}{../res_poly/figures/}{graph/}} %do not forget the / at the end

\newcommand{\R}{\mathbb{R}}


\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\begin{document}

\begin{abstract}

%The polynomial evaluation is a main field of computer science, and it has  been massively studied.
%In this paper, we focused on the benchmarking of the polynomial evaluation of the main algorithms
%of literature on the last processor of IBM and Intel.
%
%Modern processors are super scalar and out of order, therefor the performance of the polynomial evaluation 
%will depend of the degree of parallelism of the algorithm, to maximise
%the pipelines and floating point unit usage. The performance will be achieved, 
%if the algorithm has massive degree of parallelism.
%
%
%The art of polynomial evaluation is well know and well documented in the literature. The performance of a polynomial evaluation
%depends on two factor, the efficiency of the algorithm 


The polynomial evaluation of small degree polynomials is critical for the computation of elementary functions. 
It has been massively studied, and is well documented. In this paper, we have evaluated the existing methods for polynomial evaluation
on super scalar architecture.  And, we have completed this work by factorisation evaluation, which is surprisingly neglected in the literature.  
This work has been focused on the most recent processors of  Intel and IBM, amongst others, several computational units are available.
Moreover, we have applied our work on the elementary $e^x$ that  requires, in the current implementation,
an evaluation  of a polynomial of degree 10 for a satisfying precision.


\end{abstract}


\maketitle

\section{Superscalar processor}

idea Agerwala and Cocke (1987) [Dispatch multiple instructions every cycle]
but prefer definitiion Lam (1990) [Execute multiple operations in parallel]
...two main idea: Execute instructions concurently and independently in separate pipelines .... 
Improve throughput of concurrent pipelines by allowing out-of-order execution ....
modern processor have at least two independent pipelines .....

complementary to the pipelining of the floating point unit where instructions (mnemonic) ... restart the same operation
(if no dependency) without waiting the completeness of the previous one

\section{State of the art}
In evaluating the polynomial of degree $n$,
 \begin{eqnarray}
   P(x) = a_0 + a_1 x  + \dots + a_{n} x^{n}, \,\, (a_i  \in \R) \label{P0}
 \end{eqnarray}
 a large number of method can be applied. In this study, we selected the most significatives, extracted from the great book of Knuth the art of programming [cite vol 2].
 This paper focuses on the Brute Force method, the Horner method from order 1 to $n-1$, the Estrin method and the factorization associated to the methods, we have just enumerates.
 The main is challenge of applying this method is maximize the parallelism and minimise the pipelining data hazard, to avoid bubble and consequently deteriorate the performance.
\section{Algorithms}
\subsection{Evaluation of Powers}
The first step to perform polynomial evaluation is the evaluation of the power. Tee simple method consisting of successive multiplication of $x$ has a  complexity of o($n$) multiplication.
Following the advice of Knuth the Right-to-left binary method for exponentiation is the fastest for our small degree polynomial [cite Knuth chapter]. The method has a complexity 
o($\lfloor log(n) \rfloor + \upsilon(n)$) multiplications where  $\upsilon(n)$ number of ones in the binary representation of $n$.
\subsection{Brute force}
The brute force method simply consists to evaluate the polynomial \ref{P0} term after term sing evaluation of power as described previously.
The complexity of the algorithm is  $n$ additions and  $2n-1$ multiplications.
\subsection{Horner method: classical - $1^{st}$-order}
The Horner method at order one is the most classical. It is also very well know under the name \textit{"Horner's rule"}.
It evaluates recursively the polynomial \ref{P0} at $x = x_0$,
 \begin{eqnarray}
   P(x_0) = a_0 +  x_0 ( a_1  +  x_0 ( a_2 + x_0(  \dots a_{n}  ) ))
  \end{eqnarray}
The \textit{"Horner's rule"} requires a complexity of $n$ multiplications and $n$ additions or $n$ FMA. The method is efficient,
but it is inefficient  for the processor because it introduces  data hazard pipelining due to the dependency chain of the recursive process.

\subsection{Horner method: $k^{th}$-order}
The  \textit{"Horner's rule"} can be generated at any order by building new polynomial over $x^k$ [CITE ....]. It is the generalisation of the previous section,
the main advantage is to introduce parallelism that remove the data hazard specially for high order of $k$
 \begin{eqnarray}
   P(x_0) &=&  Q_0(x_0^k) + Q_1(x_0^k) + \dots + Q_{k-1}(x_0^k) \\
   Q_{k-1}(x_0^k) &=&  x_0^{k-1}(a_{k-1} + x_0^{k}(a_{2k-1} + x_0^{k}(a_{3k-1} + x_0^{k}(\dots a_{m \le n}))))
     \end{eqnarray}
This method has $n+k-1$ multiplications and $n$ additions. $k$ degree of parallelism. The classical Horner method correspond to $k=1$

\subsection{Horner method: Estrin}
An alternative to the  generalization of Horner's rule is the Estrin method [cite estrin], described as follow:
\begin{eqnarray}
c_i^{(0)} = a_i + x_0  a_{i+1} \\
c_i^n = c_i^{(n-1)} + x_0^{2^n} c_{i+2^n}^{n-1} \\
P(x_0) = c_0^n
\end{eqnarray}
A development of the previous equation look likes 
\begin{eqnarray}
P(x_0) &=&  a_0 + a_1 x_0   \nonumber \\
               &+&  x_0^2(a_2  + a_3 x_0) \nonumber \\
               &+&  x_0^4(a_4  + a_5 x_0 + x_0^2 (a_6 + a_7x_0)) \nonumber \\
               &+&  x_0^8(a_8  + a_9 x_0 + x_0^2 (a_{10} + a_{11}x_0) + x_0^4 ((a_{12}+a_{13}x_0 +x_0^2(a_{14}+a_{15}x_0)))))  \nonumber \\
               & &\dots
\end{eqnarray}
This method has $n$ multiplication and $n$ additions, but it is a remarkable pattern design for two reasons.
Every $c_i^j$ can be associated to an independent FMA, as independent it will have a good efficiency on
super scalar machine (no bubble).

\subsection{factorization}
Finally the last method, we will evaluate is the factorization. As we are looking for polynomial with fix coefficients. We compute
the root of the polynomial using an outside program. Then the roots are recombined into produce of linear polynomial, quadratic if the root 
are complex and then combinaison of quadratic or linear polynomial. At the end every "new" polynomial can be evaluated by one 
of the previous described method. This method can introduce a large parallelism and may outperform for superscalar architecture.

\subsection{number of operations}
To conclude this section on algorithm, the complexity of the different method are summarize  in the table \ref{complexity}.
\begin{table}[h!]
\begin{center}
\begin{tabular}{l c c  }
	\hline
		             & addition & multiplication \\
Brute force            &  $n$       & $2n-1$     \\
Horner $k^{th}$-order  & $n+k-1$ & $n$   \\
Estrin                    & $n$        & $n$             \\
Factorization        &    combinaison            &   combinaison   \\    
	\hline
\end{tabular}
\end{center}
\caption{ number of operation of the different algorithms of evaluation of the polynomial of degree $n$. \label{complexity}}
\end{table}%

\subsection{Notations}

$P^{n}_{m}$ polynomial of degree $n$, $m$ indicate the method ( $e = \textrm{Estrin}$, $h^k = \textrm{Horner at the the $k$ order}$, $b = \textrm{Brute Force}$) how we compute



\subsection{Combinatory}

Starting point  Polynomial degree 10 approximation $e^x$ no real root, complex only, 5 pair conjugate, 5 quadratic. How many factorization  scheme? Partition of 10
into even summands  (equivalent decomposition of 5 "multiply by 2"). $P^{10} = P^6P^4 = P^6P^2P^2 = P^4P^4P^2 = P^4P^2P^2P^2 = P^2P^2P^2P^2P^2 $.
Multiplication commutative. Every polynomial degree+1 method of evaluation (bruteforce + estrin + horner kth = degree-1) (horner kth=degree equivalent order one).
if $P^n P^m$ and $n!=m$ so $n \times m$ possibilitties. if $n=m$,  ${n+k-1 \choose k} $ possibilities (and not ${n \choose k}$ because repetitions are allowed ). On the present example 231 possibilities.  Program generates all the possibilities, get all the factorisation scheme.

\subsection{Elementary function: $e^x$}


$\forall x \in  \mathbb{R}, \exists  y \in [0,ln 2] \textrm{  with } k \in \mathbb{N} \nonumber$
\begin{eqnarray}
    x  & = & y + k ln 2  \nonumber \\
e^x  &= & 2^k e^y \nonumber \\
       &=&  2^k P(y) \label{exponential_formula} 
\end{eqnarray}
$P(y)$ is approximation polynomial of $e^y$ in the interval $[0,ln2]$. Plenty of method Remez algorithm [cite], Pade approximat  [cite]. What about the 
the series expensioms ? NO !!!! Taylor goes to infinite so by fixing a given limit becomes truncated Taylor seires, the precision will be limited to  0,
 and the error increase as soon as $x \rightarrow ln 2$.
 
 $P(y)$ determined, we apply our 231 schemes to evaluate it.
 
\subsubsection{ from x to k and y}
From equation \ref{exponential_formula}  one equation two unknows, how to do ? A bit of math, from previsously 
$\forall x \in  \mathbb{R}, \exists  y \in [0,ln 2] \textrm{  with } k \in \mathbb{N} \nonumber $
\begin{eqnarray}
x & = & y + k ln 2  \nonumber \\
\floor*{\frac{x}{ln2}} & = &  \underbrace{ \floor*{\frac{y}{ln 2}}}_{=0} + \underbrace{\floor*{k}}_{=k} \nonumber \\
			      & = & k 
\end{eqnarray} 
$k$  is determinate and consequently $y$.

\begin{algorithm}[H]
 \KwData{$x$ floating point number}
 \KwResult{$k$ signed integer and $y$ floating point number}
$ k =  \floor*{\frac{x}{ln2}} $ \;
\tcc{For rounding issue $-k \times ln2$ correction: in two steps}
$y = x - k\times6.93145751953125 \times  10^{-1}$\;
$y = x - k\times1.42860682030941723212  \times 10^{-6}$\;
 \textbf{return} $k$ and $y$\;
 \caption{$2^k$ evaluation algorithm. Note the operation  interpret\_as\_double interprets $k$ as a double it is not a cast operation.}
\end{algorithm}

\subsubsection{$2^k$ fast evaluation}
compute $2^k$ first idea in mind left bit shift , working until 63 and need expensive integer floating point conversion with error propagation.
Solution using floating point representation, number can be represented exactly under the following form:

\begin{eqnarray}
x = -1^s \times (1+F) \times 2^{e+bias}
\end{eqnarray}

$s$ sign bit, $e + bias $ is the exponent (11 bits), it is bias (engineering sense of the world). It it biased to facilitate comparison, two complementary method make think harder
bias = 1023 and e belongs [-1022,1023]. $F$ is the fraction (53 bits) $\sum_{n=1}^{p-1} bit_n \times 2^{-n}$. So ... compute $2^k$ set up bit to 0, 

\begin{algorithm}[H]
 \KwData{$k$ signed integer}
 \KwResult{$2^k$ as double}
 $ k = (1023 + k) << 52 $\;
 \textbf{return} interpret\_as\_double($k$)\;
 \caption{$2^k$ evaluation algorithm. Note the operation  interpret\_as\_double interprets $k$ as a double it is not a cast operation.}
\end{algorithm}



\subsubsection{boundary}

\begin{table}[ht]
\caption{Boundary of the exponential }
\begin{center}
\begin{tabular}{l c c c c c c }
$x$                         & $-\infty$  &$+\infty$ & 0 & NaN & $> max$  & $< max$  \\
		          \hline
$e^x$                   &  0 & $+\infty$  & 1 & NaN & $+\infty$   & 0 \\
		             \hline
\end{tabular}
\end{center}
\caption{The number 1 as a specific binary representation in binary number  \label{boundary}}
\end{table}%

compute without branching because, branching may be "slow" and it does not exist for SIMD.
following bit trick algorithm:

\begin{algorithm}[H]
 \KwData{$x$ floating point number}
 \KwResult{$e^x$ with correct boundary condition (table \ref{boundary})}
 \tcc{  inequality operator return false (0) or  true (-1)}
 $mask0 = \neg(|x| > max)$\;   
 $mask1 = x < max$\;
 $mask2 =  +\infty$\;
 \textbf{Compute}: $y = e^x$\;
 $y \,\, \&= mask0$\;
 $mask3  \,\, \&= \neg mask1$\;
 $y \,\, |= mask2$\;
 \textbf{return} $y$\;
 \caption{Boundary condition algorithm}
\end{algorithm}



\begin{figure}[h]
\includegraphics[scale=0.4]{618px-IEEE_754_Double_Floating_Point_Format} 
\caption{double precision floating point number using IEEE-754 norm \label{doubleIEEE}}
\end{figure}

\section{Processor \& methods}
Perform a full review of processors is a hugh work and it is outside the topic of this paper. We focus on the general execution of the processors, but still with enough details
to understand our results. Whatever the architecture of the ship the execution of the binary code follows the same logic. Fetch/decode units read a stream of instructions
from the L1 instructions cache memory and decode them into a series of micro operations that are executed by the processor's parallel executions units.

\subsection{Intel SandyBridge and Broadwell architecture}
The SandyBridge and Broadwell architecture derived from the core2duo architecture (based on a Pentium M). Both architecture support the new set of 256 bit vectorial instructions 
named AVX and AVX2 (Broadwell only).  For both architectures, the instructions fetch unit bandwidth 
is limited to 16 bytes per clock cycle. The instructions decoding execution is complicated, it is split
between a predecoder and a decoder. The predecoder job is to detect where the instructions start, a hard task because the size of the instructions vary between 
1 to 15 bytes. The predecoder also identify the additional field (if available) of the instructions (prefix, ModR/M, SIB, displacement and immediate). Then 4 decoders handle
the incoming instructions into 1 or more $\mu$ops. Only the first decoder can generate more than one $\mu$op. For SandyBridge, per cycle the decoders will generate 3 to 4  
$\mu$ops. On Broadwell the generation is stabilised to 4  $\mu$ops. The 16 bytes per clock cycle is a serious limitation, therefore both architecture have a cache memory
for decoded $\mu$ops after the decoders is presented to avoid limitation of the 16 byte bandwidth. The effect of the cache memory can double the totale size of  $\mu$ops generated
 to  bytes of code per cycle. 
%The cache is organised around 32 sets $\times$ 8 ways $\times$ 6 $\mu$ops.
%$3 caches lines of 6 $\mu$ops. can be allocated simultaneously for a maximum of 32 bytes of code per cycle. 
Both architecture have the possibilities to perform the fusion of instructions.
At this point, $\mu$ops are still in-order. They are now  stored in the re-order buffer (168 entries for SandyBridge and  192 entries for Broadwell) for the renaming of the register,  
the re-ordering $e.g.$ following a Tomasulo Algorithm, and in fine the reservation station  of 54 entries on Sandy Bridge and 60 on Broadwell respectively. At this last point the $\mu$ops will be executed on the execution units in out-of-order mode.
%The out of order execution is now performed on 6 ports on the SandyBridge and 8 port on the Broadwell, however the number of port dedicated for the vectorial addition, multiplications and FMA (Broadwell only) are
%equal to two, the port 0 and 1 respectively. 

The Sandy Bridge has six execution ports (or terminology execution pipelines). There are tree integer ALUs so that the most common integer operations can execute
 with a throughput of four instructions per clock cycle (port 0, 1 and 5). Two ports can handle memory read operations (port 2 and 3)
Port 4 is for memory write. Read/Write can be performed by port 2, 3 and 4 (write only) but the port 4 can not make address calculation.
The maximum throughput is one unfused $\mu$op on each port per clock cycle.
Port 0, 1 and 5 support usual 256 bit vector operations. There are separate ports for multiplication in general purpose registers and vector registers. 
The integer and floating point vector multiplier on port 0.  These two multipliers can run simultaneously and both are fully pipelined with a throughput of 1 vector operation per clock cycle.
%Integer division uses the floating point division unit on port 0. This is the only unit that is not pipelined.

The Broadwell architecture has 8 execution ports (or terminology execution pipelines).  There are four integer ALUs so that the most common integer operations can execute
 with a throughput of four instructions per clock cycle. There are three ports that can handle integer vector operations (port 0, 1 and 5).
 Two ports can handle branches (port 0 and 6).  Two ports can handle memory read operations (port 2 and 3), and one port can handle memory writes (port 4).
Two ports can handle floating point vector operations (port 0 and 1). The direct consequences  of this larger number of port for floating point operations 
is a better throughput for vectorial addition (0.8) and vectorial multiplication (0.5) compared to SandyBridge.

\subsection{IBM Power7 and Power8 architecture}
The Power8 is the last release of the IBM processor derived from the Power7.
As x86 architecture Power7/8 support vectorial instructions limited to 128 bit width. 
For both architecture the Instruction Fetch Unit bandwidth is equal to 32 bytes per clock. Contrary to the X86 architecture the instructions are enconded in the POWER 32-bit
fixed-width RISC ISA. Therefore 8 inscriptions are pre-decoded into $\mu$ops by the IFU, and store in I-cache of 32 KB (4-way set associative  L1 cache for the Power7 and
8-way set associative  L1 cache). This operations is performed in 2 cycles. As X86 both architecture can performed fusion of instructions with more facility for the Power8
If the requesting thread is ready, the  $\mu$ops are bypassed directly the I-cache to the instructions buffer
still in order). The Power7 has a instructions completion/cycle of 6 compare to 8 of the Power8.

The  $\mu$ops will be managed by the Instructions Sequencing Unit (ISU) doing the renaming in out-of order mode and the dispatch in the various issue queues to
the executions pipelines. After renaming and the dispatching the $\mu$ops (at least the compute one) are transferred to the UniQueue (as the reservation station fro X86). 
the UniQueue is implemented as 48-entry queue  (power7) or 64-entry queue (power8) and split into two halves.

The Power7 has 12 executions pipelines, but ? are dedicated per half UniQueue, but  only  3 instructions per cycle can be executed, 
one fixed-point instructions on the Fixed-Point Unit, one load, store or simple  fixed-point instruction (Addition and logical instructions) on the Load/Store unit (LSU). One load or simple-fixed instruction on the Load unit (LU) and one scalar/vector instructions to the Vector Scalar Unit. 

The Power8 has 16 execution pipelines, but  6 are dedicated per half UniQueue, but  only  4 instructions per cycle can be executed, one fixed-point instructions on the Fixed-Point Uni,
one load, store or simple  fixed-point instruction (Addition and logical instructions) on the Load/Store unit (LSU)  and one scalar/vector instructions to the Vector Scalar Unit. The 4 other executions pipelines are dedicated to the cryptography, the branch execution, the condition logical and the decimal floating point operations. 


\subsection{Latency, throughput }

The latency of an instruction is the delay that the instruction generates in a dependency chain. 
For example, a reciprocal throughput of 2 for FMUL means that a new FMUL instruction can start executing
2 clock cycles after a previous FMUL. A reciprocal throughput of 0.33 for ADD means that the execution units can handle 3 integer additions per clock cycle.

The throughput is the maximum number of instructions of the same kind that can be executed per clock cycle
when the operands of each instruction are independent of the preceding instructions.

The latency and the throughput of the main instructions for all platforms are reported in the table \label{latencyvendor}.

\subsection{Precision}

The precision computed by the Unite of Least Precision (ULP).
The original definition: \textit{ulp(x) is the gap between the two floating-point number nearest $x$, even is $x$ is one of them.}
As we described previously the precision for the exponential implementation will depend only on the error of the polynomial evaluation.
Because the error on the evaluation on the $2^k$ used the hardware representation of the floating point and the boundaries conditions 
only introduce bit  manipulation trick.  The ULP has been computed using external library, here the boost library more specially the boost::math
package.

\subsection{measurement tool}

Measure the throughput and the latency is not trivial specially for the latency. A direct measurement is not trivial due 
to the large number of variables which may impact the measurements. 
Therefore, the approach used within this study consists of performing indirect measurements by mapping the instructions associated to their
mnemonics into a for loop whose unroll factor was parameterizable (between 1 and 5). 
To avoid introducing some operation throughput effect which would corrupt the measurement of latency, a dependency between instructions 
throughout the loop iterations has been artificially enforced. 

Within this study, all benchmarks were compiled using GCC and repeated 100 times. The results of this experiment are reported in Figure \ref{fig.LATENCY_OPS} in function of the used unroll factor.  
The experimental data has been fit to a linear distribution ($y=ax+b$ where $a$ corresponds to the slope of the curve which is equal to 100 times the actual 
latency of the instruction).  Latency measurements have been carried out for all basic instructions +, $\times$, $/$ and FMA and compare to the 
vendor results to validate our method with success. Latency of the previous instructions are reported in the table \ref{latencyvendor}.

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.7]{platency.eps} %\vspace{-1cm}
\caption{Value of latencies using a linear approximation for vectorial double precision instructions on Power8: $+$. 
The slop $a$ represents the latency of the instructions. The residual $b$ of 60 cycle in 0, corresponds to the the additional workload in cycle introduced 
by the program/processor. 
\label{fig.LATENCY_OPS}}
\end{center}
\end{figure}

\begin{table}[ht]
\caption{Latency/Throughput add/mul/fma on the Haswell architecture. \label{latencyvendor}}
\begin{center}
\begin{tabular}{l c c c c c c }
			& \multicolumn{2}{c}{SandyBridge} & \multicolumn{2}{c}{Broadwell}  & \multicolumn{2}{c}{Power 7/8}\\ 
                         & Thr.  & Lat. & Thr. & Lat. & Thr. & Lat.  \\
		          \hline
add                   &  1 & 3  &   0.8 & 3 &  1 & 6     \\
mul                   &  1 & 5  &   0.5 & 5 &  1 & 6   \\
FMA                 &   -  & -  &   0.5 & 5  &  1 & 6   \\
		             \hline
\end{tabular}
\end{center}
\end{table}%

A full review  of all x86 instructions can be found in [CITE agner] where fragmented information can be founded in [CITE p7 and P8] paper.
To conclude, Intel has released in the last years the IACA tool [CITE]. This tools was nice to perform latency and throughput analysis, and
do prediction on several Intel processor target. Unfortunately the tools is not anymore maintained since 2012 date of the last release. 
However, we utilize the tool to generate Direct Acyclic Graph of the polynomial evaluation on the Broadwell platform, figure \ref{DAG}

\subsection{Programming model}

Program polynomial evaluation and $e^x$ is not trivial because the code can not be polluted by additional workload, and useless
instructions. A solution will be to write directly in assembly language, the solution is conceivable although is time consuming. 
An alternative is meta-programming associated to recursive algorithm. It simplify the generation of all polynomial evaluation
as soon as all method has been coded. Moreover as the algorithms are generics they can be plugged to any DSL to generate 
a SIMD code, it is what we did. The algorithm are successively instantiated with the basic type double and a generic vector class coming from the Cyme library.
To control the quality of our ASM at least on x86 platform we extract the DAG (Figure \ref{DAG}) using the IACA tool. Per example for the evaluation
of the Horner method of the first order on a polynomial of degree 10. The match is perfect between the theory and the generated assembly.

We insiste all algorithm are generated at compiler time.

\begin{figure}
\subfloat[]
{
%\resizebox{.1\textwidth}{.9\textwidth}{%
\begin{tikzpicture}[>=latex',line join=bevel,scale=0.3]
%%
\node (11) at (213.0bp,18.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {11. mov};
  \node (10) at (213.0bp,90.0bp) [draw=pink,fill=pink,circle,scale=0.3] {10. FMA};
  \node (1) at (213.0bp,738.0bp) [draw=pink,fill=pink,circle,scale=0.3] {1. FMA};
  \node (0) at (213.0bp,810.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {0. mov};
  \node (3) at (213.0bp,594.0bp) [draw=pink,fill=pink,circle,scale=0.3] {3. FMA};
  \node (2) at (213.0bp,666.0bp) [draw=pink,fill=pink,circle,scale=0.3] {2. FMA};
  \node (5) at (213.0bp,450.0bp) [draw=pink,fill=pink,circle,scale=0.3] {5. FMA};
  \node (4) at (213.0bp,522.0bp) [draw=pink,fill=pink,circle,scale=0.3] {4. FMA};
  \node (7) at (213.0bp,306.0bp) [draw=pink,fill=pink,circle,scale=0.3] {7. FMA};
  \node (6) at (213.0bp,378.0bp) [draw=pink,fill=pink,circle,scale=0.3] {6. FMA};
  \node (9) at (213.0bp,162.0bp) [draw=pink,fill=pink,circle,scale=0.3] {9. FMA};
  \node (8) at (213.0bp,234.0bp) [draw=pink,fill=pink,circle,scale=0.3] {8. FMA};
  \draw [->] (5) ..controls (213.0bp,423.98bp) and (213.0bp,414.71bp)  .. (6);
  \draw [->] (2) ..controls (213.0bp,639.98bp) and (213.0bp,630.71bp)  .. (3);
  \draw [->] (7) ..controls (213.0bp,279.98bp) and (213.0bp,270.71bp)  .. (8);
  \draw [->] (6) ..controls (213.0bp,351.98bp) and (213.0bp,342.71bp)  .. (7);
  \draw [->] (4) ..controls (213.0bp,495.98bp) and (213.0bp,486.71bp)  .. (5);
  \draw [->] (8) ..controls (213.0bp,207.98bp) and (213.0bp,198.71bp)  .. (9);
  \draw [->] (1) ..controls (213.0bp,711.98bp) and (213.0bp,702.71bp)  .. (2);
  \draw [->] (9) ..controls (213.0bp,135.98bp) and (213.0bp,126.71bp)  .. (10);
  \draw [->] (0) ..controls (213.0bp,783.98bp) and (213.0bp,774.71bp)  .. (1);
  \draw [->] (3) ..controls (213.0bp,567.98bp) and (213.0bp,558.71bp)  .. (4);
  \draw [->] (10) ..controls (213.0bp,63.983bp) and (213.0bp,54.712bp)  .. (11);
%
\end{tikzpicture}
%}
}
\subfloat[]
{
\begin{tikzpicture}[>=latex',line join=bevel,,scale=0.3]
%%
\node (11) at (229.0bp,519.0bp) [draw=pink,fill=pink,circle,scale=0.3] {11. $\times$};
  \node (10) at (372.0bp,283.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {10. mov};
  \node (13) at (130.0bp,639.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {13. mov};
  \node (12) at (227.0bp,401.0bp) [draw=pink,fill=pink,circle,scale=0.3] {12. $\times$};
  \node (15) at (131.0bp,401.0bp) [draw=pink,fill=pink,circle,scale=0.3] {15. FMA};
  \node (14) at (130.0bp,519.0bp) [draw=pink,fill=pink,circle,scale=0.3] {14. FMA};
  \node (17) at (280.0bp,161.0bp) [draw=pink,fill=pink,circle,scale=0.3] {17. FMA};
  \node (16) at (158.0bp,283.0bp) [draw=pink,fill=pink,circle,scale=0.3] {16. FMA};
  \node (18) at (280.0bp,41.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {18. mov};
  \node (1) at (35.0bp,639.0bp) [draw=pink,fill=pink,circle,scale=0.3] {1. FMA};
  \node (0) at (35.0bp,752.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {0. mov};
  \node (3) at (38.0bp,401.0bp) [draw=pink,fill=pink,circle,scale=0.3] {3. FMA};
  \node (2) at (35.0bp,519.0bp) [draw=pink,fill=pink,circle,scale=0.3] {2. FMA};
  \node (5) at (280.0bp,283.0bp) [draw=pink,fill=pink,circle,scale=0.3] {5. $\times$};
  \node (4) at (248.0bp,639.0bp) [draw=pink,fill=pink,circle,scale=0.3] {4. $\times$};
  \node (7) at (372.0bp,639.0bp) [draw=pink,fill=pink,circle,scale=0.3] {7. FMA};
  \node (6) at (372.0bp,752.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {6. mov};
  \node (9) at (372.0bp,401.0bp) [draw=pink,fill=pink,circle,scale=0.3] {9. FMA};
  \node (8) at (372.0bp,519.0bp) [draw=pink,fill=pink,circle,scale=0.3] {8. FMA};
  \draw [->,solid] (17) ..controls (280.0bp,109.5bp) and (280.0bp,100.4bp)  .. (18);
  \draw [->,solid] (4) ..controls (267.3bp,594.26bp) and (273.1bp,577.62bp)  .. (276.0bp,562.0bp) .. controls (291.24bp,479.97bp) and (287.72bp,381.97bp)  .. (5);
  \draw [->,solid] (14) ..controls (130.44bp,467.23bp) and (130.52bp,458.16bp)  .. (15);
  \draw [->,solid] (16) ..controls (204.58bp,236.18bp) and (225.05bp,216.05bp)  .. (17);
  \draw [->,solid] (4) ..controls (240.85bp,593.6bp) and (238.66bp,579.97bp)  .. (11);
  \draw [->,solid] (7) ..controls (372.0bp,588.43bp) and (372.0bp,575.18bp)  .. (8);
  \draw [->,solid] (6) ..controls (372.0bp,707.13bp) and (372.0bp,697.21bp)  .. (7);
  \draw [->,solid] (15) ..controls (141.65bp,354.23bp) and (143.95bp,344.36bp)  .. (16);
  \draw [->,solid] (2) ..controls (36.172bp,472.7bp) and (36.54bp,458.45bp)  .. (3);
  \draw [->,solid] (8) ..controls (372.0bp,472.7bp) and (372.0bp,458.45bp)  .. (9);
  \draw [->,solid] (3) ..controls (78.351bp,360.99bp) and (100.96bp,339.14bp)  .. (16);
  \draw [->,solid] (13) ..controls (130.0bp,590.01bp) and (130.0bp,581.05bp)  .. (14);
  \draw [->,solid] (10) ..controls (336.87bp,236.18bp) and (323.75bp,219.07bp)  .. (17);
  \draw [->,solid] (5) ..controls (280.0bp,238.16bp) and (280.0bp,225.98bp)  .. (17);
  \draw [->,solid] (9) ..controls (372.0bp,356.71bp) and (372.0bp,345.03bp)  .. (10);
  \draw [->,solid] (12) ..controls (200.65bp,355.7bp) and (192.43bp,341.89bp)  .. (16);
  \draw [->,solid] (0) ..controls (35.0bp,705.68bp) and (35.0bp,694.02bp)  .. (1);
  \draw [->,solid] (11) ..controls (228.19bp,470.82bp) and (228.0bp,460.08bp)  .. (12);
  \draw [->,solid] (1) ..controls (35.0bp,592.2bp) and (35.0bp,577.16bp)  .. (2);
%
\end{tikzpicture}
}
\subfloat[]
{
\begin{tikzpicture}[>=latex',line join=bevel,scale=0.3]
%%
\node (11) at (44.0bp,446.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {11. mov};
  \node (10) at (445.0bp,580.0bp) [draw=pink,fill=pink,circle,scale=0.3] {10. FMA};
  \node (13) at (248.0bp,446.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {13. mov};
  \node (12) at (44.0bp,307.0bp) [draw=pink,fill=pink,circle,scale=0.3] {12. FMA};
  \node (15) at (146.0bp,307.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {15. mov};
  \node (14) at (248.0bp,307.0bp) [draw=pink,fill=pink,circle,scale=0.3] {14. FMA};
  \node (17) at (322.0bp,44.0bp) [draw=pink,fill=pink,circle,scale=0.3] {17. FMA};
  \node (16) at (228.0bp,173.0bp) [draw=pink,fill=pink,circle,scale=0.3] {16. FMA};
  \node (19) at (580.0bp,307.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {19. mov};
  \node (18) at (555.0bp,446.0bp) [draw=pink,fill=pink,circle,scale=0.3] {18. $\times$};
  \node (1) at (559.0bp,932.0bp) [draw=pink,fill=pink,circle,scale=0.3] {1. FMA};
  \node (0) at (559.0bp,1044.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {0. mov};
  \node (3) at (289.0bp,816.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {3. mov};
  \node (2) at (525.0bp,816.0bp) [draw=pink,fill=pink,circle,scale=0.3] {2. FMA};
  \node (5) at (397.0bp,307.0bp) [draw=pink,fill=pink,circle,scale=0.3] {5. $\times$};
  \node (4) at (466.0bp,700.0bp) [draw=pink,fill=pink,circle,scale=0.3] {4. FMA};
  \node (7) at (716.0bp,446.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {7. mov};
  \node (6) at (339.0bp,173.0bp) [draw=pink,fill=pink,circle,scale=0.3] {6. $\times$};
  \node (9) at (648.0bp,173.0bp) [draw=pink,fill=pink,circle,scale=0.3] {9. FMA};
  \node (8) at (716.0bp,307.0bp) [draw=pink,fill=pink,circle,scale=0.3] {8. FMA};
  \draw [->,solid] (18) ..controls (492.54bp,390.84bp) and (464.25bp,366.31bp)  .. (5);
  \draw [->,solid] (18) ..controls (566.61bp,381.37bp) and (568.9bp,368.82bp)  .. (19);
  \draw [->,solid] (15) ..controls (176.84bp,256.35bp) and (188.88bp,236.98bp)  .. (16);
  \draw [->,solid] (18) ..controls (570.99bp,576.9bp) and (584.84bp,728.08bp)  .. (573.0bp,856.0bp) .. controls (572.18bp,864.82bp) and (570.84bp,874.14bp)  .. (1);
  \draw [->,solid] (10) ..controls (485.62bp,530.25bp) and (500.71bp,512.15bp)  .. (18);
  \draw [->,solid] (6) ..controls (330.26bp,258.85bp) and (328.92bp,311.31bp)  .. (339.0bp,356.0bp) .. controls (353.78bp,421.53bp) and (390.18bp,490.39bp)  .. (10);
  \draw [->,solid] (1) ..controls (545.05bp,884.22bp) and (541.99bp,873.95bp)  .. (2);
  \draw [->,solid] (11) ..controls (44.0bp,391.77bp) and (44.0bp,376.3bp)  .. (12);
  \draw [->,solid] (0) ..controls (559.0bp,999.87bp) and (559.0bp,990.83bp)  .. (1);
  \draw [->,solid] (8) ..controls (689.62bp,254.8bp) and (679.36bp,234.87bp)  .. (9);
  \draw [->,solid] (14) ..controls (239.76bp,251.62bp) and (237.85bp,239.01bp)  .. (16);
  \draw [->,solid] (5) ..controls (410.95bp,221.12bp) and (413.25bp,167.3bp)  .. (396.0bp,124.0bp) .. controls (389.2bp,106.93bp) and (376.79bp,91.366bp)  .. (17);
  \draw [->,solid] (3) ..controls (347.72bp,777.18bp) and (392.34bp,748.44bp)  .. (4);
  \draw [->,solid] (9) ..controls (586.61bp,205.32bp) and (551.84bp,228.33bp)  .. (531.0bp,258.0bp) .. controls (473.52bp,339.85bp) and (454.45bp,458.04bp)  .. (10);
  \draw [->,solid] (18) ..controls (544.94bp,570.41bp) and (534.64bp,696.76bp)  .. (2);
  \draw [->,solid] (2) ..controls (501.25bp,769.1bp) and (494.62bp,756.29bp)  .. (4);d
  \draw [->,solid] (17) ..controls (377.93bp,83.18bp) and (400.95bp,102.52bp)  .. (417.0bp,124.0bp) .. controls (477.55bp,205.06bp) and (517.78bp,316.76bp)  .. (18);
  \draw [->,solid] (18) ..controls (451.15bp,422.98bp) and (368.62bp,405.8bp)  .. (297.0bp,392.0bp) .. controls (208.31bp,374.91bp) and (178.05bp,395.85bp)  .. (97.0bp,356.0bp) .. controls (91.648bp,353.37bp) and (86.437bp,350.03bp)  .. (12);
  \draw [->,solid] (7) ..controls (716.0bp,393.9bp) and (716.0bp,374.42bp)  .. (8);
  \draw [->,solid] (5) ..controls (334.49bp,262.88bp) and (305.62bp,241.97bp)  .. (281.0bp,222.0bp) .. controls (276.49bp,218.34bp) and (271.86bp,214.42bp)  .. (16);
  \draw [->,solid] (13) ..controls (248.0bp,391.77bp) and (248.0bp,376.3bp)  .. (14);
  \draw [->,solid] (18) ..controls (615.96bp,394.95bp) and (640.38bp,374.59bp)  .. (662.0bp,356.0bp) .. controls (667.62bp,351.16bp) and (673.53bp,346.0bp)  .. (8);
  \draw [->,solid] (5) ..controls (372.75bp,250.8bp) and (367.38bp,238.59bp)  .. (6);
  \draw [->,solid] (3) ..controls (244.81bp,711.63bp) and (167.61bp,534.97bp)  .. (93.0bp,392.0bp) .. controls (86.463bp,379.47bp) and (78.964bp,366.16bp)  .. (12);
  \draw [->,solid] (4) ..controls (457.65bp,652.08bp) and (456.0bp,642.81bp)  .. (10);
  \draw [->,solid] (16) ..controls (265.13bp,121.83bp) and (278.34bp,103.98bp)  .. (17);
  \draw [->,solid] (12) ..controls (87.475bp,227.8bp) and (127.44bp,165.72bp)  .. (175.0bp,124.0bp) .. controls (203.6bp,98.917bp) and (241.53bp,78.783bp)  .. (17);
  \draw [->,solid] (18) ..controls (529.27bp,533.04bp) and (513.76bp,581.81bp)  .. (498.0bp,624.0bp) .. controls (494.32bp,633.85bp) and (490.08bp,644.3bp)  .. (4);
  \draw [->,solid] (18) ..controls (461.37bp,408.97bp) and (395.19bp,382.33bp)  .. (339.0bp,356.0bp) .. controls (324.67bp,349.29bp) and (309.36bp,341.47bp)  .. (14);
  \draw [->,solid] (18) ..controls (606.89bp,393.91bp) and (620.81bp,375.44bp)  .. (629.0bp,356.0bp) .. controls (646.87bp,313.59bp) and (650.6bp,260.89bp)  .. (9);
%
\end{tikzpicture}
}
\caption{Haswell mnemonic DAG of the polynomial evaluation for three methods of evaluation (latency/throughput), (a) classical Horner (50.0/3.55 [cycle]), (b) Estrin (22.68/2.90 [cycle]), (c)  Estrin$^6 \times$BruteForce$^4$ (28.94/3.11 [cycle])  \label{DAG}}
\end{figure}


\section{Results}


\subsection{polynomials}
build th. model difficult, to make a prediction [cite simulator] outside our knowledge.
Simple model easy for latency and the Horner method order 1...

horner order 1 -> 10 mul/add or FMA  with dependency = 10 * latency of mul/add or FMA... it works all platform see table..
produce of two quadratic horner large degree fix the time 6* latency of mul/add or FMA... + the final mul (dependency) it works.

\subsection{exponentials}

give results compare to vendors intel svml and mass



\begin{figure}[h]
\mbox{
\hspace{-2cm}
\includegraphics[scale=0.7]{allht_scalar.eps} 
\hspace{-2.5cm}
\includegraphics[scale=0.7]{allht_vector.eps}
}
\caption{Latency/Throughput for  all architectures, scalar version left and vectorial version right}
\end{figure}

\begin{figure}[h]
\mbox{
\hspace{-2cm}
\includegraphics[scale=0.7]{hw_histo_vector_l.eps} 
\hspace{-2.5cm}
\includegraphics[scale=0.7]{hw_histo_vector_t.eps}
}
\caption{Latency and Throughput frequency of the vectorial version. The analysis is done for the original polynomial or the factorization associated. Sampling interval of 1 for the latency and 0.1 for the throughput.}
\end{figure}


\begin{figure}[h]
\mbox{
\hspace{-2cm}
\includegraphics[scale=0.7]{all_precision_scalar.eps} 
\hspace{-2.5cm}
\includegraphics[scale=0.7]{all_precision_vector.eps}
}
\caption{Histogram of the precision of all polynomial evaluation for scalar version (left) and vector version (right). The bar of 
the histogram is the total number os polynomial evaluations for a given ULP}
\end{figure}



\begin{table}[ht]
\begin{center}
\begin{tabular}{l l c c c  l c c  c }
\hline
		     & & \multicolumn{3}{c}{scalar} & &  \multicolumn{3}{c}{vector} \\
		     &  &   \multicolumn{3}{c}{Throughput Criteria} &   &  \multicolumn{3}{c}{Throughput Criteria} \\
Platform        & Al.                                            & Th.  & La.      & ULP  & Al.                & Th.   & La.     & ULP \\ 	
SandyBridge & $P^{10}_e$                             & 6.73 & 30.20  & 3       & $P^{10}_e$  & 6.89 & 35.81& 3 \\
Haswell         & $P^6_{h^{1}}P^4_{h^{1}}$      & 3.13 & 35.03  & 8           & $P^{10}_e$  & 2.90 & 22.69 & 2 \\
Power7         & $P^{10}_e$                             & 6.60  & 26.02  & 2      & $P^{10}_e$  & 6.70 & 16.94  & 3 \\  
Power8          & $P^{10}_e$                            &  5.31  & 26.11 & 2       &$P^{10}_e$  & 7.58  & 17.03 & 2 \\
		     &  &   \multicolumn{3}{c}{Latency Criteria} &   &  \multicolumn{3}{c}{Latency Criteria} \\
SandyBridge & $P^6_eP^4_e$                      & 7.12 & 28.21  & 8        & $P^6_eP^4_e$  & 7.21 & 29.89 & 8 \\
Haswell         & $P^4_bP^2_bP^2_eP^2_e$  & 3.14 & 26.07  & 9        & $P^{10}_e$  & 2.90 & 22.69 & 3 \\
Power7         & $P^{10}_e$                             & 6.60  & 26.02  & 2      & $P^{10}_e$  & 6.70 & 16.94  & 3 \\  
Power8          & $P^{10}_e$                            &  5.31  & 26.11 & 2       &$P^{10}_e$  & 7.58  & 17.03 & 2 \\
\hline
\end{tabular}
\end{center}
\caption{Best Latency/throughput [cycle] polynomial evaluation on all platforms. The criteria indicates the sort the tuple (throughput, latency, ulp), 
on a specific part \label{LTR_EXP}}
\end{table}%

\section{Conclusions}

factorisation is good

\end{document}
