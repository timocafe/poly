\documentclass[preview]{elsarticle}

\usepackage{amssymb}
\usepackage[table]{xcolor}
\usepackage{subfig}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{pgfplots}
\usepackage{mathtools}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\usetikzlibrary{arrows, calc, positioning, patterns, pgfplots.polar}

\usetikzlibrary{snakes,arrows,shapes}

\title{Polynomial evaluation on super scalar architecture, applied to the elementary function $e^x$}

\author[rvt]{T. Ewart }
\ead{timothee.ewart@epfl.ch}

\author[rvt]{S. Yates}
\ead{sam.yates@epfl.ch}

%\author[rvt]{F.  Sch\"urmann }
%\ead{felix.schuermann@epfl.ch}

\author[rvt]{F. Delalondre}
\ead{fabien.delalondre@epfl.ch}

\author[rvt]{F.  Sch\"urmann }
\ead{felix.schuermann@epfl.ch}


\address[rvt]{Blue Brain Project, Campus Biotech, Ch. des Mines 9. CH-1212 Gen\`eve}


\graphicspath{{plot/figures/}{../res_poly/figures/}{graph/}} %do not forget the / at the end

\newcommand{\R}{\mathbb{R}}


\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\begin{document}

\begin{abstract}

%The polynomial evaluation is a main field of computer science, and it has  been massively studied.
%In this paper, we focused on the benchmarking of the polynomial evaluation of the main algorithms
%of literature on the last processor of IBM and Intel.
%
%Modern processors are super scalar and out of order, therefor the performance of the polynomial evaluation 
%will depend of the degree of parallelism of the algorithm, to maximise
%the pipelines and floating point unit usage. The performance will be achieved, 
%if the algorithm has massive degree of parallelism.
%
%
%The art of polynomial evaluation is well know and well documented in the literature. The performance of a polynomial evaluation
%depends on two factor, the efficiency of the algorithm 


The polynomial evaluation of small degree polynomials is critical for the computation of elementary functions. 
It has been massively studied, and is well documented. In this paper, we have evaluated the existing methods for polynomial evaluation
on super scalar architecture.  And, we have completed this work by factorisation evaluation, which is surprisingly neglected in the literature.  
This work has been focused on the most recent processors of  Intel and IBM, amongst others, several computational units are available.
Moreover, we have applied our work on the elementary $e^x$ that  requires, in the current implementation,
an evaluation  of a polynomial of degree 10 for a satisfying precision.


\end{abstract}


\maketitle

\section{Superscalar processor}

idea Agerwala and Cocke (1987) [Dispatch multiple instructions every cycle]
but prefer definitiion Lam (1990) [Execute multiple operations in parallel]
...two main idea: Execute instructions concurently and independently in separate pipelines .... 
Improve throughput of concurrent pipelines by allowing out-of-order execution ....
modern processor have at least two independent pipelines .....

complementary to the pipelining of the floating point unit where instructions (mnemonic) ... restart the same operation
(if no dependency) without waiting the completeness of the previous one

\section{State of the art}
 In evaluating the polynomial,
 \begin{eqnarray}
   P(x) = a_0 + a_1 x  + \dots + a_{n} x^{n}, \,\, (a_i  \in \R) \label{P0}
 \end{eqnarray}
 ... Brute force  ... Horner method ... $k^{th}$-order Horner ... Estrin ... adaptation coefficient .... factorisation ... challenge: maximum parallelism and minimise data  hazard pipelining to fill up pipeline, avoid bubble, maximise performance

\section{Algorithms}
\subsection{Evaluation of Powers}
... Right-to-left binary method for exponentiation ... not the fastest ... but enough for us ... because did at compile time.  
$ \lfloor log(n) \rfloor + \upsilon(n)$ multiplications where  $\upsilon(n)$ number of ones in the binary representation of $n$
\subsection{Brute force}
... evaluate polynomial \ref{P0}  at $x = x_0$ ... using evaluation of power as described previously  $n$ addition, $2n-1$ multiplication
\subsection{Horner method: classical - $1^{st}$-order}
.... the most classical .... \textit{"Horner's rule"} .... evaluate polynomial \ref{P0} at $x = x_0$

 \begin{eqnarray}
   P(x_0) = a_0 +  x_0 ( a_1  +  x_0 ( a_2 + x_0(  \dots a_{n}  ) ))
  \end{eqnarray}
Horner rules $n$ multiplications and $n$ additions. Good but data  hazard pipelining 

\subsection{Horner method: $k^{th}$-order}
Generalisation of the previous section ... Introduce parallelism ...

 \begin{eqnarray}
   P(x_0) &=&  Q_0(x_0^k) + Q_1(x_0^k) + \dots + Q_{k-1}(x_0^k) \\
   Q_{k-1}(x_0^k) &=&  x_0^{k-1}(a_{k-1} + x_0^{k}(a_{2k-1} + x_0^{k}(a_{3k-1} + x_0^{k}(\dots a_{m \le n}))))
     \end{eqnarray}
$n+k-1$ multiplications and $n$ additions, $k$ degree of parallelism. Classical Horner method $k=1$

\subsection{Horner method: Estrin}
Another generalization of Horner's rule:

\begin{eqnarray}
c_i^{(0)} = a_i + x_0  a_{i+1} \\
c_i^n = c_i^{(n-1)} + x_0^{2^n} c_{i+2^n}^{n-1} \\
P(x_0) = c_0^n
\end{eqnarray}
look like 

\begin{eqnarray}
P(x_0) &=&  a_0 + a_1 x_0   \nonumber \\
               &+&  x_0^2(a_2  + a_3 x_0) \nonumber \\
               &+&  x_0^4(a_4  + a_5 x_0 + x_0^2 (a_6 + a_7x_0)) \nonumber \\
               &+&  x_0^8(a_8  + a_9 x_0 + x_0^2 (a_{10} + a_{11}x_0) + x_0^4 ((a_{12}+a_{13}x_0 +x_0^2(a_{14}+a_{15}x_0)))))  \nonumber \\
               & &\dots
\end{eqnarray}
$n$ multiplication, $n$ additions, remark pattern design for FMA. All $c_i^j$ are independent.

\subsection{factorization}
factorize the polynomial (outside), get produce of linear or quadratic if complex-root. combinaison different method $e.g.$ 
$k$ factor gives $k-1$ factor multiplication, factor depends of the method of multiplications. 

\subsection{number of operations}

\begin{table}[h!]
\begin{center}
\begin{tabular}{l c c  }
	\hline
		             & addition & multiplication \\
Brute force            &  $n$       & $2n-1$     \\
Horner $k^{th}$-order  & $n+k-1$ & $n$   \\
Estrin                    & $n$        & $n$             \\
Factorization        &    combinaison            &   combinaison   \\    
	\hline

\end{tabular}
\end{center}
\caption{ number of operation of the different algorithms of evaluation of the polynomial of degree $n$. \label{default}}
\end{table}%



\subsection{Notations}

$P^{n}_{m}$ polynomial of degree $n$, $m$ indicate the method ( $e = \textrm{Estrin}$, $h^k = \textrm{Horner at the the $k$ order}$, $b = \textrm{Brute Force}$) how we compute



\subsection{Combinatory}

Starting point  Polynomial degree 10 approximation $e^x$ no real root, complex only, 5 pair conjugate, 5 quadratic. How many factorization  scheme? Partition of 10
into even summands  (equivalent decomposition of 5 "multiply by 2"). $P^{10} = P^6P^4 = P^6P^2P^2 = P^4P^4P^2 = P^4P^2P^2P^2 = P^2P^2P^2P^2P^2 $.
Multiplication commutative. Every polynomial degree+1 method of evaluation (bruteforce + estrin + horner kth = degree-1) (horner kth=degree equivalent order one).
if $P^n P^m$ and $n!=m$ so $n \times m$ possibilitties. if $n=m$,  ${n+k-1 \choose k} $ possibilities (and not ${n \choose k}$ because repetitions are allowed ). On the present example 231 possibilities.  Program generates all the possibilities, get all the factorisation scheme.

\subsection{Elementary function: $e^x$}


$\forall x \in  \mathbb{R}, \exists  y \in [0,ln 2] \textrm{  with } k \in \mathbb{N} \nonumber$
\begin{eqnarray}
    x  & = & y + k ln 2  \nonumber \\
e^x  &= & 2^k e^y \nonumber \\
       &=&  2^k P(y) \label{exponential_formula} 
\end{eqnarray}
$P(y)$ is approximation polynomial of $e^y$ in the interval $[0,ln2]$. Plenty of method Remez algorithm [cite], Pade approximat  [cite]. What about the 
the series expensioms ? NO !!!! Taylor goes to infinite so by fixing a given limit becomes truncated Taylor seires, the precision will be limited to  0,
 and the error increase as soon as $x \rightarrow ln 2$.
 
 $P(y)$ determined, we apply our 231 schemes to evaluate it.
 
\subsubsection{ from x to k and y}
From equation \ref{exponential_formula}  one equation two unknows, how to do ? A bit of math, from previsously 
$\forall x \in  \mathbb{R}, \exists  y \in [0,ln 2] \textrm{  with } k \in \mathbb{N} \nonumber $
\begin{eqnarray}
x & = & y + k ln 2  \nonumber \\
\floor*{\frac{x}{ln2}} & = &  \underbrace{ \floor*{\frac{y}{ln 2}}}_{=0} + \underbrace{\floor*{k}}_{=k} \nonumber \\
			      & = & k 
\end{eqnarray} 
$k$  is determinate and consequently $y$.

\begin{algorithm}[H]
 \KwData{$x$ floating point number}
 \KwResult{$k$ signed integer and $y$ floating point number}
$ k =  \floor*{\frac{x}{ln2}} $ \;
\tcc{For rounding issue $-k \times ln2$ correction: in two steps}
$y = x - k\times6.93145751953125 \times  10^{-1}$\;
$y = x - k\times1.42860682030941723212  \times 10^{-6}$\;
 \textbf{return} $k$ and $y$\;
 \caption{$2^k$ evaluation algorithm. Note the operation  interpret\_as\_double interprets $k$ as a double it is not a cast operation.}
\end{algorithm}

\subsubsection{$2^k$ fast evaluation}
compute $2^k$ first idea in mind left bit shift , working until 63 and need expensive integer floating point conversion with error propagation.
Solution using floating point representation, number can be represented exactly under the following form:

\begin{eqnarray}
x = -1^s \times (1+F) \times 2^{e+bias}
\end{eqnarray}

$s$ sign bit, $e + bias $ is the exponent (11 bits), it is bias (engineering sense of the world). It it biased to facilitate comparison, two complementary method make think harder
bias = 1023 and e belongs [-1022,1023]. $F$ is the fraction (53 bits) $\sum_{n=1}^{p-1} bit_n \times 2^{-n}$. So ... compute $2^k$ set up bit to 0, 

\begin{algorithm}[H]
 \KwData{$k$ signed integer}
 \KwResult{$2^k$ as double}
 $ k = (1023 + k) << 52 $\;
 \textbf{return} interpret\_as\_double($k$)\;
 \caption{$2^k$ evaluation algorithm. Note the operation  interpret\_as\_double interprets $k$ as a double it is not a cast operation.}
\end{algorithm}



\subsubsection{boundary}

\begin{table}[ht]
\caption{Boundary of the exponential }
\begin{center}
\begin{tabular}{l c c c c c c }
$x$                         & $-\infty$  &$+\infty$ & 0 & NaN & $> max$  & $< max$  \\
		          \hline
$e^x$                   &  0 & $+\infty$  & 1 & NaN & $+\infty$   & 0 \\
		             \hline
\end{tabular}
\end{center}
\caption{The number 1 as a specific binary representation in binary number  \label{boundary}}
\end{table}%

compute without branching because, branching may be "slow" and it does not exist for SIMD.
following bit trick algorithm:

\begin{algorithm}[H]
 \KwData{$x$ floating point number}
 \KwResult{$e^x$ with correct boundary condition (table \ref{boundary})}
 \tcc{  inequality operator return false (0) or  true (-1)}
 $mask0 = \neg(|x| > max)$\;   
 $mask1 = x < max$\;
 $mask2 =  +\infty$\;
 \textbf{Compute}: $y = e^x$\;
 $y \,\, \&= mask0$\;
 $mask3  \,\, \&= \neg mask1$\;
 $y \,\, |= mask2$\;
 \textbf{return} $y$\;
 \caption{Boundary condition algorithm}
\end{algorithm}



\begin{figure}[h]
\includegraphics[scale=0.4]{618px-IEEE_754_Double_Floating_Point_Format} 
\caption{double precision floating point number using IEEE-754 norm \label{doubleIEEE}}
\end{figure}

\section{Processor \& methods}
Fid a full review of processors is a hugh work and it is outside the topic of this paper. We focus on the general execution of the processors, but still with enough details
to understand our results. Whatever the architecture of the ship the execution of the binary code follows the same logic. Fetch/decode units read a stream of instructions
from the L1 instructions cache memory and decode them into a series of micro operations that are executed by the processor's parallel executions units.

\subsection{Intel SandyBridge and Broadwell architecture}
The SandyBridge and Broadwell architecture derived from the core2duo architecture (based on a Pentium M).  For both architectures, the instructions fetch unit bandwidth 
is limited to 16 bytes per clock cycle. The instructions decoding execution is complicated, it is split
between a predecoder and a decoder. The predecoder job is to detect where the instructions start, a hard task because the size of the instructions vary between 
1 to 15 bytes. The predecoder also identify the additional field (if available) of the instructions (prefix, ModR/M, SIB, displacement and immediate). Then 4 decoders handle
the incoming instructions into 1 or more $\mu$ops. Only the first decoder can generate more than one $\mu$op. For SandyBridge, per cycle the decoders will generate 3 to 4  
$\mu$ops. On Broadwell the generation is stabilised to 4  $\mu$ops. The 16 bytes per clock cycle is a serious limitation, therefore both architecture have a cache memory
for decoded $\mu$ops after the decoders is presented to avoid limitation of the 16 byte bandwidth. The cache is organised around 32 sets $\times$ 8 ways $\times$ 6 $\mu$ops.
3 caches lines of 6 $\mu$ops. can be allocated simultaneously for a maximum of 32 bytes of code per cycle. Both architecture have the possibilities to perform the fusion of instructions.
At this point, $\mu$ops are still in-order. They are now  stored in the re-order buffer (168 entries for SandyBridge and192 entries for Broadwell) where they will be executed on the execution unit in out-of-order mode.

blabla - focus on VPU and execution pipelines (two port for x86)

\subsection{IBM Power 7/8 architecture}
blabla - focus on VPU and execution pipelines (two port for x86)

\subsection{Latency, throughput }

The latency of an instruction is the delay that the instruction generates in a dependency chain. 
For example, a reciprocal throughput of 2 for FMUL means that a new FMUL instruction can start executing
2 clock cycles after a previous FMUL. A reciprocal throughput of 0.33 for ADD means that the execution units can handle 3 integer additions per clock cycle.


The throughput is the maximum number of instructions of the same kind that can be executed per clock cycle
when the operands of each instruction are independent of the preceding instructions.

\subsection{Precision}

The precision computed by the Unite of Least Precision (ULP).

The original definition: ulp(x) is the gap between the two floating-point number nearest $x$, even is $x$ is one of them.

Precision for the exponential .... depends only polynomial evaluation .... no error on $2^k$ and boundaries.

\subsection{measurement tool}

In order to help HPC specialists wishing to optimise scientific applications on new system close to peak performance, it is useful to
provide a performance summary of latency of the most widely-used instructions.
Such an evaluation is not trivial due to the large number of variables which may impact the measurements. 
Therefore, the approach used within this study consists of performing indirect measurements by mapping the instructions associated to their
mnemonics into a for loop whose unroll factor was parameterizable (between 1 and 5). 
To avoid introducing some operation throughput effect which would corrupt the measurement of latency, a dependency between instructions 
throughout the loop iterations has been artificially enforced. 

Within this study, all benchmarks were compiled using GCC 
and repeated 100 times. The results of this experiment are reported in Figure \ref{fig.LATENCY_OPS} in function of the used unroll factor.  
The experimental data has been fit to a linear distribution ($y=ax+b$ where $a$ corresponds to the slope of the curve which is equal to 100 times the actual 
latency of the instruction). 
Latency measurements have been carried out for both single and double precisions and for both scalar and vectorial implementations of the following operators: +, $\times$, $/$, FMA, $\sqrt{x}$ and $e^x$). For the specific case of the vectorial $e^x$ instruction, calls to IBM MASSV library were used.
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.7]{platency.eps} %\vspace{-1cm}
\caption{Value of latencies using a linear approximation for vectorial double precision instructions on Power8: $+$. 
The slop $a$ represents the latency of the instructions. The residual $b$ of 60 cycle in 0, corresponds to the the additional workload in cycle introduced 
by the program/processor. 
\label{fig.LATENCY_OPS}}
\end{center}
\end{figure}


These results complement the work of \cite{POWER8Thorsten} or Agner [cite]

% by on the one hand making an explicit distinction 
%between scalar and vectorial instructions and, on the other hand providing results of complex mathematical operations 
%such as $\sqrt{x}$ and $e^x$ (Table \ref{LATENCY_OPS}). 





we develop our own tool
latency: explain from power8 paper
throughput: basic loop wi
IACA measure latency/throughput of C++ code, kernel that is pinned by marks into the code.  
return latency/throughput and bottleneck of the mark code.

\subsubsection{validation}
measure the basic instructions $\times, +$ and FMA compare to vendor number or agner number [cite]
 

\subsection{Programming model}
C++ basic recursive meta-programming (like template factorial ), all models are written with fairplay. Assembly check, look good.
Fine control of the code but compiler as the final word, and can do strong optimisation, specially catch FMA pattern.
Vectorial (SIMD) version used our DSL cyme (the DSL does FMA and evaluate the power following the scheme described in previous section),
 normal one code the algo but let the the compiler performed FMA or evaluate the power, following is scheme.

\subsection{ASM quality}
Simple evaluate the number of move compare 
\begin{figure}
\subfloat[]
{
%\resizebox{.1\textwidth}{.9\textwidth}{%
\begin{tikzpicture}[>=latex',line join=bevel,scale=0.3]
%%
\node (11) at (213.0bp,18.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {11. mov};
  \node (10) at (213.0bp,90.0bp) [draw=pink,fill=pink,circle,scale=0.3] {10. FMA};
  \node (1) at (213.0bp,738.0bp) [draw=pink,fill=pink,circle,scale=0.3] {1. FMA};
  \node (0) at (213.0bp,810.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {0. mov};
  \node (3) at (213.0bp,594.0bp) [draw=pink,fill=pink,circle,scale=0.3] {3. FMA};
  \node (2) at (213.0bp,666.0bp) [draw=pink,fill=pink,circle,scale=0.3] {2. FMA};
  \node (5) at (213.0bp,450.0bp) [draw=pink,fill=pink,circle,scale=0.3] {5. FMA};
  \node (4) at (213.0bp,522.0bp) [draw=pink,fill=pink,circle,scale=0.3] {4. FMA};
  \node (7) at (213.0bp,306.0bp) [draw=pink,fill=pink,circle,scale=0.3] {7. FMA};
  \node (6) at (213.0bp,378.0bp) [draw=pink,fill=pink,circle,scale=0.3] {6. FMA};
  \node (9) at (213.0bp,162.0bp) [draw=pink,fill=pink,circle,scale=0.3] {9. FMA};
  \node (8) at (213.0bp,234.0bp) [draw=pink,fill=pink,circle,scale=0.3] {8. FMA};
  \draw [->] (5) ..controls (213.0bp,423.98bp) and (213.0bp,414.71bp)  .. (6);
  \draw [->] (2) ..controls (213.0bp,639.98bp) and (213.0bp,630.71bp)  .. (3);
  \draw [->] (7) ..controls (213.0bp,279.98bp) and (213.0bp,270.71bp)  .. (8);
  \draw [->] (6) ..controls (213.0bp,351.98bp) and (213.0bp,342.71bp)  .. (7);
  \draw [->] (4) ..controls (213.0bp,495.98bp) and (213.0bp,486.71bp)  .. (5);
  \draw [->] (8) ..controls (213.0bp,207.98bp) and (213.0bp,198.71bp)  .. (9);
  \draw [->] (1) ..controls (213.0bp,711.98bp) and (213.0bp,702.71bp)  .. (2);
  \draw [->] (9) ..controls (213.0bp,135.98bp) and (213.0bp,126.71bp)  .. (10);
  \draw [->] (0) ..controls (213.0bp,783.98bp) and (213.0bp,774.71bp)  .. (1);
  \draw [->] (3) ..controls (213.0bp,567.98bp) and (213.0bp,558.71bp)  .. (4);
  \draw [->] (10) ..controls (213.0bp,63.983bp) and (213.0bp,54.712bp)  .. (11);
%
\end{tikzpicture}
%}
}
\subfloat[]
{
\begin{tikzpicture}[>=latex',line join=bevel,,scale=0.3]
%%
\node (11) at (229.0bp,519.0bp) [draw=pink,fill=pink,circle,scale=0.3] {11. $\times$};
  \node (10) at (372.0bp,283.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {10. mov};
  \node (13) at (130.0bp,639.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {13. mov};
  \node (12) at (227.0bp,401.0bp) [draw=pink,fill=pink,circle,scale=0.3] {12. $\times$};
  \node (15) at (131.0bp,401.0bp) [draw=pink,fill=pink,circle,scale=0.3] {15. FMA};
  \node (14) at (130.0bp,519.0bp) [draw=pink,fill=pink,circle,scale=0.3] {14. FMA};
  \node (17) at (280.0bp,161.0bp) [draw=pink,fill=pink,circle,scale=0.3] {17. FMA};
  \node (16) at (158.0bp,283.0bp) [draw=pink,fill=pink,circle,scale=0.3] {16. FMA};
  \node (18) at (280.0bp,41.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {18. mov};
  \node (1) at (35.0bp,639.0bp) [draw=pink,fill=pink,circle,scale=0.3] {1. FMA};
  \node (0) at (35.0bp,752.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {0. mov};
  \node (3) at (38.0bp,401.0bp) [draw=pink,fill=pink,circle,scale=0.3] {3. FMA};
  \node (2) at (35.0bp,519.0bp) [draw=pink,fill=pink,circle,scale=0.3] {2. FMA};
  \node (5) at (280.0bp,283.0bp) [draw=pink,fill=pink,circle,scale=0.3] {5. $\times$};
  \node (4) at (248.0bp,639.0bp) [draw=pink,fill=pink,circle,scale=0.3] {4. $\times$};
  \node (7) at (372.0bp,639.0bp) [draw=pink,fill=pink,circle,scale=0.3] {7. FMA};
  \node (6) at (372.0bp,752.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {6. mov};
  \node (9) at (372.0bp,401.0bp) [draw=pink,fill=pink,circle,scale=0.3] {9. FMA};
  \node (8) at (372.0bp,519.0bp) [draw=pink,fill=pink,circle,scale=0.3] {8. FMA};
  \draw [->,solid] (17) ..controls (280.0bp,109.5bp) and (280.0bp,100.4bp)  .. (18);
  \draw [->,solid] (4) ..controls (267.3bp,594.26bp) and (273.1bp,577.62bp)  .. (276.0bp,562.0bp) .. controls (291.24bp,479.97bp) and (287.72bp,381.97bp)  .. (5);
  \draw [->,solid] (14) ..controls (130.44bp,467.23bp) and (130.52bp,458.16bp)  .. (15);
  \draw [->,solid] (16) ..controls (204.58bp,236.18bp) and (225.05bp,216.05bp)  .. (17);
  \draw [->,solid] (4) ..controls (240.85bp,593.6bp) and (238.66bp,579.97bp)  .. (11);
  \draw [->,solid] (7) ..controls (372.0bp,588.43bp) and (372.0bp,575.18bp)  .. (8);
  \draw [->,solid] (6) ..controls (372.0bp,707.13bp) and (372.0bp,697.21bp)  .. (7);
  \draw [->,solid] (15) ..controls (141.65bp,354.23bp) and (143.95bp,344.36bp)  .. (16);
  \draw [->,solid] (2) ..controls (36.172bp,472.7bp) and (36.54bp,458.45bp)  .. (3);
  \draw [->,solid] (8) ..controls (372.0bp,472.7bp) and (372.0bp,458.45bp)  .. (9);
  \draw [->,solid] (3) ..controls (78.351bp,360.99bp) and (100.96bp,339.14bp)  .. (16);
  \draw [->,solid] (13) ..controls (130.0bp,590.01bp) and (130.0bp,581.05bp)  .. (14);
  \draw [->,solid] (10) ..controls (336.87bp,236.18bp) and (323.75bp,219.07bp)  .. (17);
  \draw [->,solid] (5) ..controls (280.0bp,238.16bp) and (280.0bp,225.98bp)  .. (17);
  \draw [->,solid] (9) ..controls (372.0bp,356.71bp) and (372.0bp,345.03bp)  .. (10);
  \draw [->,solid] (12) ..controls (200.65bp,355.7bp) and (192.43bp,341.89bp)  .. (16);
  \draw [->,solid] (0) ..controls (35.0bp,705.68bp) and (35.0bp,694.02bp)  .. (1);
  \draw [->,solid] (11) ..controls (228.19bp,470.82bp) and (228.0bp,460.08bp)  .. (12);
  \draw [->,solid] (1) ..controls (35.0bp,592.2bp) and (35.0bp,577.16bp)  .. (2);
%
\end{tikzpicture}
}
\subfloat[]
{
\begin{tikzpicture}[>=latex',line join=bevel,scale=0.3]
%%
\node (11) at (44.0bp,446.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {11. mov};
  \node (10) at (445.0bp,580.0bp) [draw=pink,fill=pink,circle,scale=0.3] {10. FMA};
  \node (13) at (248.0bp,446.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {13. mov};
  \node (12) at (44.0bp,307.0bp) [draw=pink,fill=pink,circle,scale=0.3] {12. FMA};
  \node (15) at (146.0bp,307.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {15. mov};
  \node (14) at (248.0bp,307.0bp) [draw=pink,fill=pink,circle,scale=0.3] {14. FMA};
  \node (17) at (322.0bp,44.0bp) [draw=pink,fill=pink,circle,scale=0.3] {17. FMA};
  \node (16) at (228.0bp,173.0bp) [draw=pink,fill=pink,circle,scale=0.3] {16. FMA};
  \node (19) at (580.0bp,307.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {19. mov};
  \node (18) at (555.0bp,446.0bp) [draw=pink,fill=pink,circle,scale=0.3] {18. $\times$};
  \node (1) at (559.0bp,932.0bp) [draw=pink,fill=pink,circle,scale=0.3] {1. FMA};
  \node (0) at (559.0bp,1044.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {0. mov};
  \node (3) at (289.0bp,816.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {3. mov};
  \node (2) at (525.0bp,816.0bp) [draw=pink,fill=pink,circle,scale=0.3] {2. FMA};
  \node (5) at (397.0bp,307.0bp) [draw=pink,fill=pink,circle,scale=0.3] {5. $\times$};
  \node (4) at (466.0bp,700.0bp) [draw=pink,fill=pink,circle,scale=0.3] {4. FMA};
  \node (7) at (716.0bp,446.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {7. mov};
  \node (6) at (339.0bp,173.0bp) [draw=pink,fill=pink,circle,scale=0.3] {6. $\times$};
  \node (9) at (648.0bp,173.0bp) [draw=pink,fill=pink,circle,scale=0.3] {9. FMA};
  \node (8) at (716.0bp,307.0bp) [draw=pink,fill=pink,circle,scale=0.3] {8. FMA};
  \draw [->,solid] (18) ..controls (492.54bp,390.84bp) and (464.25bp,366.31bp)  .. (5);
  \draw [->,solid] (18) ..controls (566.61bp,381.37bp) and (568.9bp,368.82bp)  .. (19);
  \draw [->,solid] (15) ..controls (176.84bp,256.35bp) and (188.88bp,236.98bp)  .. (16);
  \draw [->,solid] (18) ..controls (570.99bp,576.9bp) and (584.84bp,728.08bp)  .. (573.0bp,856.0bp) .. controls (572.18bp,864.82bp) and (570.84bp,874.14bp)  .. (1);
  \draw [->,solid] (10) ..controls (485.62bp,530.25bp) and (500.71bp,512.15bp)  .. (18);
  \draw [->,solid] (6) ..controls (330.26bp,258.85bp) and (328.92bp,311.31bp)  .. (339.0bp,356.0bp) .. controls (353.78bp,421.53bp) and (390.18bp,490.39bp)  .. (10);
  \draw [->,solid] (1) ..controls (545.05bp,884.22bp) and (541.99bp,873.95bp)  .. (2);
  \draw [->,solid] (11) ..controls (44.0bp,391.77bp) and (44.0bp,376.3bp)  .. (12);
  \draw [->,solid] (0) ..controls (559.0bp,999.87bp) and (559.0bp,990.83bp)  .. (1);
  \draw [->,solid] (8) ..controls (689.62bp,254.8bp) and (679.36bp,234.87bp)  .. (9);
  \draw [->,solid] (14) ..controls (239.76bp,251.62bp) and (237.85bp,239.01bp)  .. (16);
  \draw [->,solid] (5) ..controls (410.95bp,221.12bp) and (413.25bp,167.3bp)  .. (396.0bp,124.0bp) .. controls (389.2bp,106.93bp) and (376.79bp,91.366bp)  .. (17);
  \draw [->,solid] (3) ..controls (347.72bp,777.18bp) and (392.34bp,748.44bp)  .. (4);
  \draw [->,solid] (9) ..controls (586.61bp,205.32bp) and (551.84bp,228.33bp)  .. (531.0bp,258.0bp) .. controls (473.52bp,339.85bp) and (454.45bp,458.04bp)  .. (10);
  \draw [->,solid] (18) ..controls (544.94bp,570.41bp) and (534.64bp,696.76bp)  .. (2);
  \draw [->,solid] (2) ..controls (501.25bp,769.1bp) and (494.62bp,756.29bp)  .. (4);d
  \draw [->,solid] (17) ..controls (377.93bp,83.18bp) and (400.95bp,102.52bp)  .. (417.0bp,124.0bp) .. controls (477.55bp,205.06bp) and (517.78bp,316.76bp)  .. (18);
  \draw [->,solid] (18) ..controls (451.15bp,422.98bp) and (368.62bp,405.8bp)  .. (297.0bp,392.0bp) .. controls (208.31bp,374.91bp) and (178.05bp,395.85bp)  .. (97.0bp,356.0bp) .. controls (91.648bp,353.37bp) and (86.437bp,350.03bp)  .. (12);
  \draw [->,solid] (7) ..controls (716.0bp,393.9bp) and (716.0bp,374.42bp)  .. (8);
  \draw [->,solid] (5) ..controls (334.49bp,262.88bp) and (305.62bp,241.97bp)  .. (281.0bp,222.0bp) .. controls (276.49bp,218.34bp) and (271.86bp,214.42bp)  .. (16);
  \draw [->,solid] (13) ..controls (248.0bp,391.77bp) and (248.0bp,376.3bp)  .. (14);
  \draw [->,solid] (18) ..controls (615.96bp,394.95bp) and (640.38bp,374.59bp)  .. (662.0bp,356.0bp) .. controls (667.62bp,351.16bp) and (673.53bp,346.0bp)  .. (8);
  \draw [->,solid] (5) ..controls (372.75bp,250.8bp) and (367.38bp,238.59bp)  .. (6);
  \draw [->,solid] (3) ..controls (244.81bp,711.63bp) and (167.61bp,534.97bp)  .. (93.0bp,392.0bp) .. controls (86.463bp,379.47bp) and (78.964bp,366.16bp)  .. (12);
  \draw [->,solid] (4) ..controls (457.65bp,652.08bp) and (456.0bp,642.81bp)  .. (10);
  \draw [->,solid] (16) ..controls (265.13bp,121.83bp) and (278.34bp,103.98bp)  .. (17);
  \draw [->,solid] (12) ..controls (87.475bp,227.8bp) and (127.44bp,165.72bp)  .. (175.0bp,124.0bp) .. controls (203.6bp,98.917bp) and (241.53bp,78.783bp)  .. (17);
  \draw [->,solid] (18) ..controls (529.27bp,533.04bp) and (513.76bp,581.81bp)  .. (498.0bp,624.0bp) .. controls (494.32bp,633.85bp) and (490.08bp,644.3bp)  .. (4);
  \draw [->,solid] (18) ..controls (461.37bp,408.97bp) and (395.19bp,382.33bp)  .. (339.0bp,356.0bp) .. controls (324.67bp,349.29bp) and (309.36bp,341.47bp)  .. (14);
  \draw [->,solid] (18) ..controls (606.89bp,393.91bp) and (620.81bp,375.44bp)  .. (629.0bp,356.0bp) .. controls (646.87bp,313.59bp) and (650.6bp,260.89bp)  .. (9);
%
\end{tikzpicture}
}
\caption{Haswell mnemonic DAG of the polynomial evaluation for three methods of evaluation (latency/throughput), (a) classical Horner (50.0/3.55 [cycle]), (b) Estrin (22.68/2.90 [cycle]), (c)  Estrin$^6 \times$BruteForce$^4$ (28.94/3.11 [cycle]) }
\end{figure}


\section{Results}


\subsection{polynomials}
build th. model difficult, to make a prediction [cite simulator] outside our knowledge.
Simple model easy for latency and the Horner method order 1...

horner order 1 -> 10 mul/add or FMA  with dependency = 10 * latency of mul/add or FMA... it works all platform see table..
produce of two quadratic horner large degree fix the time 6* latency of mul/add or FMA... + the final mul (dependency) it works.

\subsection{exponentials}

give results compare to vendors intel svml and mass

\begin{table}[ht]
\caption{Latency/Throughput add/mul/fma on the Haswell architecture}
\begin{center}
\begin{tabular}{l c c c c c c }
			& \multicolumn{2}{c}{SandyBridge} & \multicolumn{2}{c}{Haswell}  & \multicolumn{2}{c}{Power 7/8}\\ 
                         & Thr.  & Lat. & Thr. & Lat. & Thr. & Lat.  \\
		          \hline
add                   &  1 & 3  &   0.8 & 3 &  1 & 6     \\
mul                   &  1 & 5  &   0.5 & 5 &  1 & 6   \\
FMA                 &   -  & -  &   0.5 & 5  &  1 & 6   \\
		             \hline
\end{tabular}
\end{center}
\label{default}
\end{table}%


\begin{figure}[h]
\mbox{
\hspace{-2cm}
\includegraphics[scale=0.7]{allht_scalar.eps} 
\hspace{-2.5cm}
\includegraphics[scale=0.7]{allht_vector.eps}
}
\caption{Latency/Throughput for  all architectures, scalar version left and vectorial version right}
\end{figure}

\begin{figure}[h]
\mbox{
\hspace{-2cm}
\includegraphics[scale=0.7]{hw_histo_vector_l.eps} 
\hspace{-2.5cm}
\includegraphics[scale=0.7]{hw_histo_vector_t.eps}
}
\caption{Latency and Throughput frequency of the vectorial version. The analysis is done for the original polynomial or the factorization associated. Sampling interval of 1 for the latency and 0.1 for the throughput.}
\end{figure}


\begin{figure}[h]
\mbox{
\hspace{-2cm}
\includegraphics[scale=0.7]{all_precision_scalar.eps} 
\hspace{-2.5cm}
\includegraphics[scale=0.7]{all_precision_vector.eps}
}
\caption{Histogram of the precision of all polynomial evaluation for scalar version (left) and vector version (right). The bar of 
the histogram is the total number os polynomial evaluations for a given ULP}
\end{figure}



\begin{table}[ht]
\begin{center}
\begin{tabular}{l l c c c  l c c  c }
\hline
		     & & \multicolumn{3}{c}{scalar} & &  \multicolumn{3}{c}{vector} \\
		     &  &   \multicolumn{3}{c}{Throughput Criteria} &   &  \multicolumn{3}{c}{Throughput Criteria} \\
Platform        & Al.                                            & Th.  & La.      & ULP  & Al.                & Th.   & La.     & ULP \\ 	
SandyBridge & $P^{10}_e$                             & 6.73 & 30.20  & 3       & $P^{10}_e$  & 6.89 & 35.81& 3 \\
Haswell         & $P^6_{h^{1}}P^4_{h^{1}}$      & 3.13 & 35.03  & 8           & $P^{10}_e$  & 2.90 & 22.69 & 2 \\
Power7         & $P^{10}_e$                             & 6.60  & 26.02  & 2      & $P^{10}_e$  & 6.70 & 16.94  & 3 \\  
Power8          & $P^{10}_e$                            &  5.31  & 26.11 & 2       &$P^{10}_e$  & 7.58  & 17.03 & 2 \\
		     &  &   \multicolumn{3}{c}{Latency Criteria} &   &  \multicolumn{3}{c}{Latency Criteria} \\
SandyBridge & $P^6_eP^4_e$                      & 7.12 & 28.21  & 8        & $P^6_eP^4_e$  & 7.21 & 29.89 & 8 \\
Haswell         & $P^4_bP^2_bP^2_eP^2_e$  & 3.14 & 26.07  & 9        & $P^{10}_e$  & 2.90 & 22.69 & 3 \\
Power7         & $P^{10}_e$                             & 6.60  & 26.02  & 2      & $P^{10}_e$  & 6.70 & 16.94  & 3 \\  
Power8          & $P^{10}_e$                            &  5.31  & 26.11 & 2       &$P^{10}_e$  & 7.58  & 17.03 & 2 \\
\hline
\end{tabular}
\end{center}
\caption{Best Latency/throughput [cycle] polynomial evaluation on all platforms. The criteria indicates the sort the tuple (throughput, latency, ulp), 
on a specific part \label{LTR_EXP}}
\end{table}%

\section{Conclusions}

factorisation is good

\end{document}
