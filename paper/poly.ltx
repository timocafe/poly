\documentclass[preview]{elsarticle}

\usepackage{amssymb}
\usepackage[table]{xcolor}
\usepackage{subfig}
\usepackage{ytableau}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{pgfplots}
\usepackage{mathtools}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\usetikzlibrary{arrows, calc, positioning, patterns, pgfplots.polar}

\usetikzlibrary{snakes,arrows,shapes}

\title{Polynomial evaluation on super scalar architecture, applied to the elementary function $e^x$}

\author[rvt]{T. Ewart }
\ead{timothee.ewart@epfl.ch}

\author[rvt]{S. Yates}
\ead{sam.yates@epfl.ch}

%\author[rvt]{F.  Sch\"urmann }
%\ead{felix.schuermann@epfl.ch}

\author[rvt]{F. Delalondre}
\ead{fabien.delalondre@epfl.ch}

\author[rvt]{F.  Sch\"urmann }
\ead{felix.schuermann@epfl.ch}


\address[rvt]{Blue Brain Project, Campus Biotech, Ch. des Mines 9. CH-1212 Gen\`eve}


\graphicspath{{plot/figures/}{../res_exp/figures/}{../res_poly/figures/}{graph/}} %do not forget the / at the end

\newcommand{\R}{\mathbb{R}}


\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\begin{document}

\begin{abstract}

%The polynomial evaluation is a main field of computer science, and it has  been massively studied.
%In this paper, we focused on the benchmarking of the polynomial evaluation of the main algorithms
%of literature on the last processor of IBM and Intel.
%
%Modern processors are super scalar and out of order, therefor the performance of the polynomial evaluation 
%will depend of the degree of parallelism of the algorithm, to maximise
%the pipelines and floating point unit usage. The performance will be achieved, 
%if the algorithm has massive degree of parallelism.
%
%
%The art of polynomial evaluation is well know and well documented in the literature. The performance of a polynomial evaluation
%depends on two factor, the efficiency of the algorithm 


The polynomial evaluation of small degree polynomials is critical for the computation of elementary functions. 
It has been massively studied, and is well documented. In this paper, we have evaluated the existing methods for polynomial evaluation
on super scalar architecture.  And, we have completed this work by factorisation evaluation, which is surprisingly neglected in the literature.  
This work has been focused on the most recent processors of  Intel, amongst others, several computational units are available.
Moreover, we have applied our work on the elementary $e^x$ that  requires, in the current implementation,
an evaluation  of a polynomial of degree 10 for a satisfying precision. The results obtained for the  factorization scheme benchmark are faster than the famous
and efficient  Estrin's method.


\end{abstract}


\maketitle

The performance evaluation of the polynomials small degree is the key factor for the good executions and the performances of the elementary functions.
The theoretical research in polynomial evaluation is a very active research field, and it is very well summarised  in \cite{Knuth:1997:ACP:270146, Muller:1997:EFA:261217}.
Complementary, the performance evaluation of these researches on the processors is always active from \cite{DBLP:OverillW94} on the vectorial processor to \cite{Reynolds2010, DBLP:DukhanV13, Shibata2010}  
more recently .

On these examples, the study of \cite{Reynolds2010} evaluates polynomial  but it does not put enough attention in the pure performance (results should be reported in [cycles]), specially for small degree. \cite{DBLP:DukhanV13} only maximises the throughput of elementary functions, using a unique  method of evaluation
(Horner method) and does not provides any information on the boundary limits, and the latency.  \cite{Shibata2010} focus only on the throughput and does not take into account the boundary limits.
 \cite{Costas} provides an excellent work, but boundary conditions is again neglected. On the other hands all these authors limit their polynomial evaluation to the classical Horner method. 
An interesting point will be, the knowledge of the implementation of mathematical vendors libraries 
(IBM - mass or Intel - imf/svml),  obviously vendors do not communicate about theirs implementations.

To summaries an excellent review of method of polynomial and  elementary functions implementation can be found in \cite{Muller:1997:EFA:261217}.
In this paper, we implemented $e^x$ based on the a polynomial evaluation and not on a look at table like \cite{Tang:1989:TIE:63522.214389, 145565} 
because look at table are not SIMD friendly and in the present study, the work focuses on the polynomial factorisation, we necessitate a polynomial
of degree close to 10. Look at table algorithms need polynomial of degree 6 or less. One more time, the goal of this paper is not to evaluate 
the $e^x$ implementation, but evaluate the factorisation pattern.

In this paper we are considering the performance can be only reached if the algorithm is specifically design to the hardware which is  target.
Nowdays, in HPC the superscalar architecture is standard, consequently the algorithm must be parallel to maximise the pipelines utilisation.

This paper focuses on the evaluation on polynomial of degree 10 on super-scalar processor, and in the end it's application on the elementary function
$e^x$. We remember the state of the art of polynomial  and the elementary function $e^x$. Then, more technically, we describe the super scalar architecture and 
 core unit of the last processor X86 of Intel (SandyBridge and BroadWell), and the different method we will use for coding and measure the veracity of the results.
We present results for the polynomial evaluation and exponential for scalar and vectorial version, and then we compare to vendor exponential library.

\section{Polynomial evaluation, state of the art}
In evaluating the polynomial of degree $n$,
 \begin{eqnarray}
   P(x) = a_0 + a_1 x  + \dots + a_{n} x^{n}, \,\, (a_i  \in \R) \label{P0}
 \end{eqnarray}
 a large number of methods can be applied. In this study, we selected the most significant, extracted from the work of Knuth \cite{Knuth:1997:ACP:270146} who collects and summarise
 all the state of the art. %the art of programming [cite vol 2].
 The followings methods have been selected: the Brute Force method, the Horner method from order 1 to $n-1$, the Estrin method. Moreover, factorisation techniques have been applied to all the  methods previously enumerated. To improve performance, the main challenge consists in maximising the parallelism and minimising the pipelining data hazards, in order to avoid bubbles that have the effect of deteriorating performance.
%\section{Algorithms}
\subsection{Evaluation of Powers \label{powern}}
The basic step in performing a polynomial evaluation is the computation of the power function.
To compute $x^n$, the naive approach would be to multiply $x$ with itself $n$ times in sequence, which has complexity of o($n$).
Following the advice of Knuth, the Right-to-Left binary method for exponentiation is an excellent alternative for our small degree polynomial \cite{Knuth:1997:ACP:270146}. 
The method has a complexity o($\lfloor log(n) \rfloor + \upsilon(n)$) multiplications where  $\upsilon(n)$ is the number of ones in the binary representation of $n$.
\subsection{Brute force}
The brute force method simply consists in evaluating the polynomial~\eqref{P0}, term after term, computing the power function as described above.
The  algorithm needs $n$ additions and  $2n-1$ multiplications.
\subsection{Horner method: classical - $1^{st}$-order}
The homonym Horner's method at order one is the most classical. Given an input value $x_0$, it evaluates recursively the polynomial~\eqref{P0} as
 \begin{eqnarray}
   P(x_0) = a_0 +  x_0 ( a_1  +  x_0 ( a_2 + x_0(  \dots a_{n}  ) )).
  \end{eqnarray}
\textit{``Horner's method''} needs $n$ multiplications and $n$ additions, or $n$ Fused Multiply-Add (FMA). The method is efficient,
but inefficient from the point of view of the processor, because it introduces pipelining data hazards due to the dependency chain in the recursive process.

\subsection{Horner method: $k^{th}$-order}
\textit{``Horner's method''} can be generalised to an arbitrary order by building new polynomials over $x^k$ instead of $x$ \cite{Dorn:1962:GHR:1661979.1661987}.
The main advantage is to introduce parallelism that removes the data hazards, especially for high orders of $k$, \textit{e.g.} for $k=2$ it corresponds to the "Even-Odd" pattern.
Given a degree of parallelism $k$, Horner's $k^{th}$-order method reads:
 \begin{eqnarray}
   P(x_0) &=&  Q_0(x_0^k) + Q_1(x_0^k) + \dots + Q_{k-1}(x_0^k) \\
   Q_{k-1}(x_0^k) &=&  x_0^{k-1}(a_{k-1} + x_0^{k}(a_{2k-1} + x_0^{k}(a_{3k-1} + x_0^{k}(\dots a_{m \le n}))))
     \end{eqnarray}
This method has  $n+k-1$ multiplications and $n$ additions. The classical Horner method corresponds to $k=1$.

\subsection{Horner method: Estrin}
An alternative to the generalization of Horner's rule is the Estrin method \cite{10.1109/AFIPS.1960.28}, described as follows:
\begin{equation}\label{estrin}
\begin{split}
c_i^{(0)} &= a_i + x_0  a_{i+1}, \\
c_i^n     &= c_i^{(n-1)} + x_0^{2^n} c_{i+2^n}^{n-1}, \\
P(x_0)    &= c_0^n.
\end{split}
\end{equation}
Developing equation~\eqref{estrin} leads to: 
\begin{eqnarray}
P(x_0) &=&  a_0 + a_1 x_0   \nonumber \\
               &+&  x_0^2(a_2  + a_3 x_0) \nonumber \\
               &+&  x_0^4(a_4  + a_5 x_0 + x_0^2 (a_6 + a_7x_0)) \nonumber \\
               &+&  x_0^8(a_8  + a_9 x_0 + x_0^2 (a_{10} + a_{11}x_0) + \nonumber \\ 
               & &  x_0^4 ((a_{12}+a_{13}x_0 +x_0^2(a_{14}+a_{15}x_0)))))  \nonumber \\
               & &\dots
\end{eqnarray}
This method has $n$ multiplications and $n$ additions, just like Horner's
method, but it has a remarkable pattern design for the following reason: every
$c_i^j$ can be associated to an independent FMA, and by virtue of this
independence we expect a good efficiency on super scalar machines (\textit{i.e.} no
bubbles in the pipeline).

\subsection{Factorization}

Finally we consider the factorization method. This method is based on the fundamental algebraic property of polynomials with real coefficients , which states that any polynomial $P(x)$ of degree $2n$
 can be decomposed into a product of $n$ quadratics factors with real coefficients (factorization).
\begin{equation}\label{real-factor}
P(x)= \prod\limits_{j=1}^{n} (x^2 + \alpha_j x + \beta_j),
\end{equation}
Instead of only evaluating all the simple factors appearing in~\eqref{real-factor} one-by-one, we consider all their possible combinations into polynomials of higher degree, to test whether a particular sequence of higher degree polynomials yields better results.
Each higher degree polynomial can be evaluated by one
of the previously described methods.
The factorization technique can introduce a large degree of parallelism and may outperform others for superscalar architectures.

\subsection{Number of operations}
To conclude the section on the algorithms, the total number of operations $(\times,+)$ are provided for all methods is summarised in Table \ref{complexity}. In this table
the factorisation is combination of the different method $m_i$ indicates the number of addition/subtraction of the evaluation according the chosen method.
\begin{table}[h!]
\begin{center}
\begin{tabular}{l c c  }
	\hline
		             & addition & multiplication \\
Brute force            &  $n$       & $2n-1$     \\
Horner $k^{th}$-order  & $n+k-1$ & $n$   \\
Estrin                    & $n$        & $n$             \\
Factorization        &    $\sum\limits_{i=1}^n m_i$     &    $\sum\limits_{i=1}^n m_i$ + n   \\    
	\hline
\end{tabular}
\end{center}
\caption{Number of operations for different algorithms used for the evaluation of a polynomial of degree $n$. \label{complexity}}
\end{table}%

\section{Elementary function: $e^x$}
Small polynomials are extensively used in the approximation of elementary functions.
The present study focuses on $e^x$, a possible approach to compute $e^x$ is:

\begin{equation}\label{begin_exp}
\begin{split}
\forall x \in  \mathbb{R}, \exists  y \in [0,ln 2[ & \textrm{  with } k \in \mathbb{N} \textrm{ such that:}\\
    x  & =  y + k ln 2   \\
e^x  &=  2^k e^y
%       &=&  2^k P(y) \label{exponential_formula} 
\end{split}
\end{equation}

The restriction on the range $[0,ln 2]$ simplifies the evaluation of $e^y$. A truncated Taylor series could be a good idea to evaluate $e^y$.
Nevertheless, it can be a source of large error due to its non uniform error estimate, as we are going to demonstrate.

\subsection{Truncated series}
 If we consider the Taylor series:

\begin{eqnarray}
e^y = \underbrace{\overbrace{1 + y + \frac{y^2}{2!} + \dots + \frac{y^n}{n!}}^{\text{Approximation}} + \overbrace{\frac{y^{n+1}}{n+1!} + \dots}^{\text{Truncation Error}}}_{\text{Exact mathematical formulation}}
\end{eqnarray}

Here the "Truncation Error" corresponds to the remainder $R_n$ from Taylor's Theorem. It can be expressed under an integral or the Lagrange form by:
\begin{eqnarray}
R_n & = & \int_{a}^{y} \frac{(y-t)^n}{n!}f^{(n+1)}(t)dt \,\,\, \text{(the integral form)} \\
	& = & \frac{f^{(n+1)}(c)}{(n+1)!}(y-1)^{n+1}, \,\,\, c \in [a,y] \,\,\, \text{(the Lagrange form)}
\end{eqnarray}
For $f(y)=e^y$ and $a=0$, we have $f^{(n+1)}(y)=e^y$, thus
\begin{eqnarray}
R_n &=& \frac{e^c}{(n+1)!}y^{n+1}, \,\,\, c \in [0,y] \\
       &\leq& \left| \frac{e^y}{(n+1)!} y^{n+1} \right|
\end{eqnarray}

When $y=ln2$ and $n=10$, the remainder term has magnitude $R_{10} \leq \left| \frac{e^{ln2}}{11!}ln(2)^{11}\right| \sim 8.89 \times 10^{-10}$, which can be considered a large error compared to floating point accuracy. According to this formula, to guarantee an error lower than the double precision of the machine, $n=16$ is needed. An alternative would be to develop the Taylor 
series around $ln(2)/2$ but once again the error would be local.  Clever author \cite{Shibata2010} adds second range reduction using property of the $e^x$, shifting the reduce value to 0
and then improving the precision, unfortunately it necessitate an final additional transposition.

\subsection{Polynomial approximation}

As demonstrated truncated Taylor series can not be selected due to locality issues. An excellent alternative is to approximate $e^x$ with a polynomial using a minimax method.
The Remez algorithm is an excellent solution. The method certifies that $f(x)$  ($e^x$ in our case) can be approximate by a unique minimax solution $R(x)$ minimising 
an  error function \textit{e.g.} $E(x)=f(x)-R(x)$) in the concerning interval. Moreover, the minimax solution verifies (Chebyshev proof).

\begin{itemize}
\item if $R(x)$ is a polynomial of degree $n$ with $(n+1)$ $c_i$ coefficients, there are $n+2$ unknowns: the $n+1$ coefficients of the polynomial, and the maximal value of the error function ($E(x) = f(x) - R(x)$)
\item The error function has $n+1$ roots and $n+2$ extrema 
\item The extrema alternate in sign, and all have the same magnitude 
\end{itemize}

That means that if the location of the extrema of the error function are known, then  $n+2$ simultaneous equations can be written:

\begin{eqnarray}
R(x_i) + (-1)^iE = f(x_i) \label{system}
\end{eqnarray}

where $E$ is the maximal error term, and $x_i$ are the abscissa values (or control point) and should be the $n+2$ extrema of the error function. The resolution of this system of equation is easy 
nevertheless it does not  provide the position of the extrema. In practice the  Remez algorithm necessitates an initialisation and  two iterative steps. First the polynomial approximation comes from a 
Chebyshev approximation, the extrema are determinate, and they become the control points $x_i$ of the error functions. Then the two iterative steps are performed:

\begin{enumerate}
\item Compute the $n+1$ coefficients $c_i$ and the error $E$ by solving the  equation (\ref{system}). A solution is obtained with the same error on the control point but it is not necessarily the minimax 
solution, because the extrema are not located on the control points.
\item Find the extrema of the new solution using the properties that the extrema is between the $n+1$ root of the polynomial and the control points. This step is called exchange step. 
And iterate until the error function is lower than the wanted precision. 
\end{enumerate}

The Remez method is powerfull, a full detail and rigorous analysis can be found in \cite{Fraser:1965:SMC:321281.321282}. The only default of the Remez method 
is the usage of irrational number that maybe not represented by a finite number of bit (53 bits precision for double precision IEEE), the Remez algorithm may introduce
penalisable rounding effect,  \cite{BrisebarreChevillard2007} proposed alternative to reach better approximation. Consequently, the Remez approximation has been performed 
with a customise extended form of the floating point (150 bits digit precision) with a polynomial of degree 10. It gives a satisfactory trade-off between accuracy and performance (demonstrate in section \ref{section_ulp}). This polynomial will be the starting point for the method previously describe. 


\subsection{Combinatory}

The factorization method and the different polynomial evaluation techniques described above give rise to a large number of possible combinations of polynomial factors.
We now present a method for computing exactly how many different combinations we need to consider.

In this study, we focus on the evaluation of polynomials of degree 10, where the coefficients are known in advance. We concentrate specifically on polynomials of degree 10 
because they are very useful in the approximation of the exponential function $e^x$ with $x \in [0, ln2]$.
The polynomials we consider are strictly positive in the range $[0, ln2]$, and therefore they do not have any real roots, but five pairs of complex conjugate
roots. From the five pairs, five quadratic polynomials can be built following the factorization process described above. 
The product of these five quadratic factors is the factorisation scheme. We note that there exist other alternative schemes.

But just before doing the combinatory analysis, let introduce the following notation
Any polynomial $P$ is characterised by two parameters, the degree and the method of evaluation. 
The degree is represented by the superscript $n$, whereas the subscript is the method of evaluation. The different methods of evaluation are
the brute force method ($b$), the Horner method of order $k$ ($h^k$) and the Estrin method ($e$). For example, $P^{6}_{e}$ means a polynomial of degree 6
evaluated with the method of Estrin, whereas $P^8_{h^2}$ means a polynomial of degree 8 evaluated with the method of Horner with order 2.

To find the total number of combinations of polynomial factors, we start by considering all possible partitions of the number 10 as sum of even numbers.
We only consider even numbers because, as stated above, all the polynomial factors we consider are of degree two, arising from the product of two factors associated to complex conjugate roots.
Partitioning the number ten in even summands s equivalent to partitioning the number five into arbitrary summands, then multiplying by two.
There are seven partitions of the number five, which are, using conventional notation: $(5),(4,1),(3,2),(3,1^2),(2^2,1),(2,1^3),(1^5)$.
Consequently the partitions of ten into even summands are

\begin{eqnarray}
 (10),\,(8,2),\,(6,4),\,(6,2^2),\,(4^2,2),\,(4,2^3) \textrm{ and } (2^5) \label{evaluation}
\end{eqnarray}

To evaluate the number of combinations, three considerations are made:

\begin{enumerate}
\item each polynomial $P^m$ has $m+1$ methods of evaluation: $P^m_b$, $P^m_e$ and $P^m_{h^{k}}$ where $k \in [1,m-1]$ because $P^m_{h^1} = P^m_{h^n}$;
\item for each polynomial product $P^l P^m$ with $l\neq m$, there are $(l+1) \times (m+1)$ possible combinations (we do not consider the alternative $P^m P^l$  because multiplication is commutative);
\item for each product of $k$ polynomials having the same degree $m$ there are ${m+k \choose k} $  and not ${m+1 \choose k}$ possible combinations because repetitions are not counted.
\end{enumerate}

Applying this scheme to~\eqref{evaluation} determines the total number of combinations $S$:

\begin{eqnarray}
S  &=&  \textrm{card}((10) + (8,2) + (6,4) + (6,2^2) + (4^2,2) + (4,2^3) + (2^5)) \nonumber \\
    &=& 11 + 9\times3 + 7\times5 + 7\times{4 \choose 2} + 3 \times {6 \choose 2} + 5 \times {5 \choose 3} + {7 \choose 5}  \nonumber \\
    &=& 231
\end{eqnarray}

The total number of combinations to evaluate $P^{10}$ is therefore 231.

\subsection{ From x to k and y}
The polynomial evaluation issue solved, going back to the $e^x$ computation. The equation~\eqref{begin_exp} can be solved  although it has two unknowns.\\
$\forall x \in  \mathbb{R}, \exists  y \in [0,ln 2[, k \in \mathbb{N} \nonumber $ such that:
\begin{eqnarray}\label{xyk}
x & = & y + k ln 2.
\end{eqnarray}
To determine $k$, we apply the floor function to both sides of~\eqref{xyk}
\begin{eqnarray}\label{floork}
\floor*{\frac{x}{ln2}} & = &  \underbrace{ \floor*{\frac{y}{ln 2}}}_{=0} + \underbrace{\floor*{k}}_{=k} \nonumber \\
			      & = & k.
\end{eqnarray} 
Now that $k$ has been determined, we can compute $y$ by solving~\eqref{xyk}.

The previous equations can be integrated in the next algorithm. However, the algorithm may introduce rounding error \cite{Goldberg:1991:CSK:103162.103163}
due to the multiplication between the large gap between $k$ and $ln(2)$. Consequently to extent the precision,
the distributed property precision of the multiplication is used to double the precision (potentially).

\begin{eqnarray}
x \otimes (c_h \oplus c_l) = x \otimes c_h \oplus x \otimes c_l
\end{eqnarray}

In our case $x$ is $k$ and $ln(2)$ is $ (c_h \oplus c_l)$. The coefficient have been extracted from the cephes library \cite{MOS00a}: $c_l=6.93145751953125\times  10^{-1}$
and $c_h=1.42860682030941723212  \times 10^{-6}$.

% To determine $c_l$ we use the following formula
%
%\begin{eqnarray}
%c_h = \floor*{\frac{2^b ln(2)_{104}}{2^b}} \,\,\, \textrm{and} \,\,\, c_l = ln(2) - c_h
%\end{eqnarray}
%For a maximum precision $b$ is setup to $52$ (precision of double floating point precision), $ln(2)_{104}$ must at least equal to 104 bits precision.
%The computation of $c_h$ and $c_l$ give $c_h = \texttt{0x3FE62E42FEFA39F0}$ and $c_l = \texttt{0xBC9950D86FA0778D}$. These two number are surprising
%compare too the literature. In fact a large number of mathematical library are based on $c_h = \texttt{0x401BB9D000000000}$ and $c_l = \texttt{0x3EB7F7D1CF79ABCA}$
%.... surprising ....

\begin{algorithm}[H]
 \KwData{$x$ floating point number}
 \KwResult{$k$ signed integer and $y$ floating point number}
$ k \leftarrow  \floor*{\frac{x}{ln(2)}} $ \;
\tcc{For rounding issue $-k \times ln(2)$ correction: in two steps}
$y \leftarrow x - k\times6.93145751953125 \times  10^{-1}$\;
$y \leftarrow x - k\times1.42860682030941723212  \times 10^{-6}$\;
 \textbf{return} $k$ and $y$\;
 \caption{$2^k$ evaluation algorithm.}
\end{algorithm}

\subsection{$2^k$ fast evaluation} \label{TwokSection}
Computing $2^k$ may be done with a left bit shift. But is is not suitable because of the limited domain with 64-bit integer.
An alternative solution utilises the floating point representation. Real numbers can be represented approximately under the following form:
Le
\begin{eqnarray}
x = -1^s \times (1+F) \times 2^{e+bias}
\end{eqnarray}


%\begin{figure}[h]
%\begin{center}
%\includegraphics[scale=0.4]{618px-IEEE_754_Double_Floating_Point_Format} 
%\caption{double precision floating point number using IEEE-754 norm \label{doubleIEEE}}
%\end{center}
%\end{figure}

For IEEE-754 double precision number: $s$ sign bit, $e + bias $ is the exponent (11 bits), it is bias (engineering sense of the word).
The bias is equal to 1023 and $e$ belongs [-1022,1023]. $F$ is the mantissa (53 bits) $\sum_{n=1}^{p-1} bit_n \times 
2^{-n}$. The method consists in taking the wanted $k$, adding the bias and moving it into the exponent part of the floating point representation, and letting the machine interpret it like a double.
The method is efficient, fast and does not introduce any errors. This method can be described by the following algorithm:

\begin{algorithm}[H]
 \KwData{$k$ signed integer}
 \KwResult{$2^k$ as double}
 $ k  \leftarrow (1023 + k) << 52 $\;
 \textbf{return} interpret\_as\_double($k$)\;
 \caption{$2^k$ evaluation algorithm. Note the operation  interpret\_as\_double interprets $k$ as a double it is not a cast operation.}
\end{algorithm}

\subsection{Boundary limits} 
 \label{BoundarySection}
The last step consists in fixing the boundary limits. This step is mandatory and it introduces additional workflows, although it is sometimes neglected 
in previous exponential study \cite{Costas}. The  boundary limits to respect are summarised in the Table~\ref{boundary}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{l c c c c c c }
$x$                         & $-\infty$  &$+\infty$ & 0 & NaN & $> 707$  & $< -707$  \\
		          \hline
$e^x$                   &  0 & $+\infty$  & 1 & NaN & $+\infty$   & 0 \\
		             \hline
\end{tabular}
\caption{Boundary of the exponential \label{boundary}}
\end{center}
\end{table}%

Branching is impossible on SIMD architectures, nevertheless we describe an alternative using bitwise manipulation, as described in the Algorithm~\ref{manipulation}.

\begin{algorithm}[H]
 \KwData{$x$ floating point number}
 \KwResult{$e^x$ with correct boundary condition (table \label{boundary})}

 $\texttt{mask0} \leftarrow \neg(|x| > max)$\;   
 $\texttt{mask1} \leftarrow x < max$\;
 $\texttt{mask2} \leftarrow  +\infty$\;
  \tcc{here $e^x$ does not compute extrema}
 \textbf{Compute}: $y \leftarrow e^x$\;
 $y  \leftarrow y \, \& \, \texttt{mask0}$\;
 $\texttt{mask3} \leftarrow  \texttt{mask3} \, \& \, \neg \texttt{mask1}$\;
 $y \leftarrow  y | \texttt{mask2}$\;
 \textbf{return} $y$\;
 \caption{Boundary condition algorithm. On X86 SIMD architecture true is represented by all bits true.\label{manipulation}}
\end{algorithm}

This algorithm has been applied for the scalar and vectorial versions.

\section{Superscalar Processors \& methods}


\subsection{Latency, throughput definitions}

This section provides the definitions of the main concepts manipulate in the next sections.

\texttt{Dependency chain}: a dependency chain is a series of operations. Each operation need the computation of the previous one to be computed.

\texttt{Pipeline and pipelining}: a pipeline is a technical implementation (in hardware) in which multiple instructions are overlapped in execution.

\texttt{Latency}: The latency of an instruction is the delay that the instruction generates in a dependency chain.

\texttt{Throughput}: The throughput is the maximum number of instructions of the same kind that can be executed per clock cycle when the operands of each instruction are independent of the preceding instructions.

The latency and the throughput of the main instructions for all platforms are reported in Table~\ref{latencyvendor}.

\subsection{Architecture detils}

It exists several definition of superscalar processor, but two seem more interesting:  "Dispatch multiple instructions every cycle" from
\cite{thomas1987high} or "Execute two or more scalar operations in parallel" from \cite{Lam1990}.
Whatever the definitions, they are two main ideas: execute instructions concurrently and independently in separate execution pipelines. Moreover, 
the throughput of the execution into the concurrent pipelines may be really improved by allowing out-of-order execution.
Nevertheless, if the dispatcher is ineffective at keeping all of these units fed with instructions, the performance of the system will be no better than that of a simpler, cheaper in-order design.

Modern processors have more than one pipelines. On the last release of Intel this number reaches 8 pipelines for the BroadWell processor, and more interesting processors can have several Floating Point Unit (FPU - \textit{e.g.} two on the BroadWell) and Arithmetic Logic Unit (ALU - \textit{e.g.} four on the BroadWell). Complementary to the execution pipeline, the execution of a single instructions is also pipeline in several step \cite{Power7FPU}. It allows to restart the same operation (if no dependency chain) without waiting the completeness of the previous one.

Performing a full review of processors is a huge work and it is outside the scope of this paper. We focus on the general execution of the processors, but still consider enough details
to explain our results. Whatever the architecture of the chip, the execution of the binary code follows the same logic. Fetch/decode units read a stream of instructions
from the L1 instructions cache memory and decode them into a series of micro operations that are executed by the processor's parallel executions units.
The next section is a summarise of the great work of \cite{Agner}.

\subsection{Intel SandyBridge and BroadWell architecture}
The SandyBridge and BroadWell architecture derived from the core2duo architecture (based on a Pentium M). Both architectures support the new set of 256 bit vectorial instructions 
named AVX and AVX2 (BroadWell only). The AVX2 provides FMA support and better integer and bitwise manipulation.  For both architectures, the instructions fetch unit bandwidth 
is limited to 16 bytes per clock cycle. The instructions decoding execution is complicated, it is split
between a predecoder and a decoder. The predecoder job is to detect where the instructions start, a hard task because the size of an instruction can vary between 
1 to 15 bytes. The predecoder also identifies the additional field (if available) of the instructions (prefix, ModR/M, SIB, displacement and immediate). Then 4 decoders decompose
the incoming instructions into 1 or more $\mu$ops. Only the first decoder can generate more than one $\mu$op. For SandyBridge, decoders will generate 3 to 4  
$\mu$ops per cycle. On BroadWell, the generation rate is fixed to 4 $\mu$ops per cycle. The 16 bytes per clock cycle is a serious limitation, therefore both architectures have a cache memory
to avoid the limitations arising from the 16 byte per cycle bandwidth. The effect of the cache memory can double the total size of  $\mu$ops generated: 32 bytes per cycle. 
%The cache is organised around 32 sets $\times$ 8 ways $\times$ 6 $\mu$ops.
%$3 caches lines of 6 $\mu$ops. can be allocated simultaneously for a maximum of 32 bytes of code per cycle. 
Both architecture have the possibility to perform a fusion of instructions.
At this point, $\mu$ops are still in-order. They are now  stored in the re-order buffer (168 entries for SandyBridge and  192 entries for BroadWell) for the renaming of the register,  
the re-ordering $e.g.$ following a Tomasulo Algorithm, and \emph{in fine} the reservation station of 54 entries on Sandy Bridge and 60 on BroadWell respectively. At this last point the $\mu$ops will be executed on the execution units in out-of-order mode.
%The out of order execution is now performed on 6 ports on the SandyBridge and 8 port on the BroadWell, however the number of port dedicated for the vectorial addition, multiplications and FMA (BroadWell only) are
%equal to two, the port 0 and 1 respectively. 

The Sandy Bridge has six execution ports (or terminology execution pipelines). There are tree integer ALUs so that the most common integer operations can execute
 with a throughput of four instructions per clock cycle (port 0, 1 and 5). Two ports can handle memory read operations (port 2 and 3)
Port 4 is for memory write. Read/Write can be performed by port 2, 3 and 4 (write only) but the port 4 can not make address computation.
The maximum throughput is one unfused $\mu$op on each port per clock cycle.
Port 0, 1 and 5 support usual 256 bit vector operations. There are separate ports for multiplication of general purpose registers and vector registers. 
The integer and floating point vector multiplier are on port 0.  These two multipliers can run simultaneously and both are fully pipelined with a throughput of 1 vector operation per clock cycle.
%Integer division uses the floating point division unit on port 0. This is the only unit that is not pipelined.

The BroadWell architecture has 8 execution ports (or terminology execution pipelines).  There are four integer ALUs so that the most common integer operations can execute
 with a throughput of four instructions per clock cycle. There are three ports that can handle integer vector operations (port 0, 1 and 5).
 Two ports can handle branches (port 0 and 6).  Two ports can handle memory read operations (port 2 and 3), and one port can handle memory writes (port 4).
Two ports can handle floating point vector operations (port 0 and 1). The direct consequences  of this larger number of port for floating point operations 
is a better throughput for vectorial addition (0.8) and vectorial multiplication (0.5) compared to SandyBridge.

%\subsection{IBM Power7 and Power8 architecture}
%The Power8 is the last release of the IBM processor derived from the Power7.
%Similarly to the x86 architecture, the Power7/8 chips support vectorial instructions but are limited to 128 bit width. 
%For both architectures the Instruction Fetch Unit bandwidth is equal to 32 bytes per clock. Contrary to the X86 architecture the instructions are enconded in the POWER 32-bit
%fixed-width RISC ISA. Therefore 8 inscriptions are pre-decoded into $\mu$ops by the IFU, and store in I-cache of 32 KB (4-way set associative  L1 cache for the Power7 and
%8-way set associative  L1 cache). This operation is performed in 2 cycles. Similarly to x86, both architectures can perform fusion of instructions, with higher efficiency for the Power8 architecture.
%If the requesting thread is ready, the  $\mu$ops are bypassed directly from the I-cache to the instructions buffer (still in order). The Power7 has a rate of completed instructions per cycle of 6, compared to 8 of the Power8.
%
%The  $\mu$ops are be managed by the Instructions Sequencing Unit (ISU), responsible for doing the renaming in out-of order mode and the dispatching in the various issue queues to
%the executions pipelines. After renaming and the dispatching the $\mu$ops (at least the compute ones) are transferred to the UniQueue (as the reservation station for the x86 architecture). 
%the UniQueue is implemented as 48-entry queue  (power7) or 64-entry queue (power8) and split into two halves.
%
%The Power7 has 12 executions pipeline, but XXX are dedicated to half UniQueue, so  only  3 instructions per cycle can be executed, 
%one fixed-point instruction on the Fixed-Point Unit, one load, store or simple  fixed-point instruction (Addition and logical instructions) on the Load/Store unit (LSU). One load or simple-fixed instruction on the Load unit (LU) and one scalar/vector instruction on the Vector Scalar Unit. 
%
%The Power8 has 16 execution pipelines, but  6 are dedicated to half UniQueue, so  only  4 instructions per cycle can be executed, one fixed-point instructions on the Fixed-Point Unit,
%one load, store or simple  fixed-point instruction (Addition and logical instructions) on the Load/Store unit (LSU)  and one scalar/vector instructions on the Vector Scalar Unit. The 4 other executions pipelines are dedicated to the cryptography, the branch execution, the condition logical and the decimal floating point operations. 




\subsection{Precision}

We compute the precision using the Unit of Least Precision (ULP) \cite{muller:inria-00070503}.
The standard definition is: \textit{ulp(x) is the gap between the two floating-point numbers closest to $x$, even if $x$ is one of them.}
The precision for the implementation of the exponential function will depend only on the error arising from the polynomial evaluation, because the evaluation of $2^k$ uses the exact hardware representation of the floating point numbers, and the boundaries conditions 
only introduce bit  manipulation tricks. In this work, we compute the ULP using the boost library, more specially the \texttt{boost::math}
package.

\subsection{Measurement tools}

Measuring the throughput/latency is not a trivial task, especially for the latency. A direct measurement is not trivial due 
to the large number of variables which may impact the measurements. 
Therefore, the approach used within this study consists of performing indirect measurements by mapping the instructions associated to their
mnemonics into a for loop whose unroll factor was parameterizable (between 1 and 5). 
To avoid introducing some operation throughput effect which would corrupt the measurement of latency, a dependency between instructions 
throughout the loop iterations has been artificially enforced.  This method of measurement has been tested with success on the Power8 machine \cite{Ewart:2015:PEI:2832087.2832088}
This measurement was not easy, and necessitate a total control of the generated code and a good understanding of the CPU states. In fact the compiler may introduce unwanted effect like reorganising inline assembly block, \textit{etc} ..

Within this study, all benchmarks were compiled using GCC and repeated 100 times. 
%The results of this experiment are reported in Figure~\ref{fig.LATENCY_OPS} in function of the used unroll factor.  
The experimental data has been fitted to a linear distribution ($y=ax+b$ where $a$ corresponds to the slope of the curve which is equal to 100 times the actual 
latency of the instruction).  Latency measurements have been carried out for all basic instructions +, $\times$ and FMA and compared to the 
vendor results to validate our method with success. Latency of the previous instructions are reported in Table~\ref{latencyvendor}.

%\begin{figure}[h]
%\begin{center}
%\includegraphics[scale=0.7]{platency.eps} %\vspace{-1cm}
%\caption{Value of latencies using a linear approximation for vectorial double precision instructions on Power8: $+$. 
%The slop $a$ represents the latency of the instructions. The residual $b$ of 60 cycle in 0, corresponds to the the additional workload in cycle introduced 
%by the program/processor. 
%\label{fig.LATENCY_OPS}}
%\end{center}
%\end{figure}

\begin{table}[ht]
\begin{center}
%\begin{tabular}{l c c c c c c }
%			& \multicolumn{2}{c}{SandyBridge} & \multicolumn{2}{c}{BroadWell}  & \multicolumn{2}{c}{Power 7/8}\\ 
%                         & Thr.  & Lat. & Thr. & Lat. & Thr. & Lat.  \\
%		          \hline
%add                   &  1 & 3  &   0.8 & 3 &  1 & 6     \\
%mul                   &  1 & 5  &   0.5 & 5 &  1 & 6   \\
%FMA                 &   -  & -  &   0.5 & 5  &  1 & 6   \\
%		             \hline
%\end{tabular}
\begin{tabular}{l c c c c  }
			& \multicolumn{2}{c}{SandyBridge} & \multicolumn{2}{c}{BroadWell}  \\ 
                         & Thr.  & Lat. & Thr. & Lat. \\
		          \hline
add                   &  1 & 3  &   0.8 & 3    \\
mul                   &  1 & 5  &   0.5 & 5    \\
FMA                 &   -  & -  &   0.5 & 5    \\
		             \hline
\end{tabular}
\end{center}
\caption{Latency/Throughput add/mul/fma on the BroadWell architecture. \label{latencyvendor}}
\end{table}%

A full review  of all x86 instructions can be found in \cite{Agner}. To conclude, Intel has released in the past the IACA tool \cite{iaca}.
 This tool was nice to perform latency and throughput analysis, and obtain predictions on several Intel processor target. Unfortunately the tool has not been maintained since 2012, date of its last release. 
However, we utilize the tool to generate the Direct Acyclic Graph of the polynomial evaluation on the BroadWell platform, Figure~\ref{DAG}.

\subsection{Programming model}

Programming polynomial evaluation and $e^x$ for high performance computing is not trivial because the code can not be polluted by additional workload, and useless
instructions. A solution would be to write directly in assembly language: the solution is conceivable although it is very time consuming. 
We consider the alternative of using C++ meta-programming associated to recursive algorithms, inspired from the famous pattern of the meta-factorial \cite{citeulike:2443933}.
 It simplifies the generation of all polynomial evaluations
as soon as all possible methods have been programmed. Moreover, as the algorithms are generic, they can be plugged into any DSL to generate 
a SIMD code. The algorithms are successively instantiated with the basic type double and a generic vector class coming from any SIMD DSL library  \cite{Pohl:2016:ECS:2870650.2870653},
here, we selected a library we developed inside the BBP \cite{CymeISC2014}.
To control the quality of our ASM at least on x86 platform we extract the DAG (Figure~\ref{DAG}) using the IACA tool, for example for the evaluation
of the first order Horner method on a polynomial of degree 10. The match is perfect between the theory and the generated assembly.

\begin{figure}[t]
\subfloat[]
{
%\resizebox{.1\textwidth}{.9\textwidth}{%
\begin{tikzpicture}[>=latex',line join=bevel,scale=0.27]
%%
\node (11) at (213.0bp,18.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {11. mov};
  \node (10) at (213.0bp,90.0bp) [draw=pink,fill=pink,circle,scale=0.3] {10. FMA};
  \node (1) at (213.0bp,738.0bp) [draw=pink,fill=pink,circle,scale=0.3] {1. FMA};
  \node (0) at (213.0bp,810.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {0. mov};
  \node (3) at (213.0bp,594.0bp) [draw=pink,fill=pink,circle,scale=0.3] {3. FMA};
  \node (2) at (213.0bp,666.0bp) [draw=pink,fill=pink,circle,scale=0.3] {2. FMA};
  \node (5) at (213.0bp,450.0bp) [draw=pink,fill=pink,circle,scale=0.3] {5. FMA};
  \node (4) at (213.0bp,522.0bp) [draw=pink,fill=pink,circle,scale=0.3] {4. FMA};
  \node (7) at (213.0bp,306.0bp) [draw=pink,fill=pink,circle,scale=0.3] {7. FMA};
  \node (6) at (213.0bp,378.0bp) [draw=pink,fill=pink,circle,scale=0.3] {6. FMA};
  \node (9) at (213.0bp,162.0bp) [draw=pink,fill=pink,circle,scale=0.3] {9. FMA};
  \node (8) at (213.0bp,234.0bp) [draw=pink,fill=pink,circle,scale=0.3] {8. FMA};
  \draw [->] (5) ..controls (213.0bp,423.98bp) and (213.0bp,414.71bp)  .. (6);
  \draw [->] (2) ..controls (213.0bp,639.98bp) and (213.0bp,630.71bp)  .. (3);
  \draw [->] (7) ..controls (213.0bp,279.98bp) and (213.0bp,270.71bp)  .. (8);
  \draw [->] (6) ..controls (213.0bp,351.98bp) and (213.0bp,342.71bp)  .. (7);
  \draw [->] (4) ..controls (213.0bp,495.98bp) and (213.0bp,486.71bp)  .. (5);
  \draw [->] (8) ..controls (213.0bp,207.98bp) and (213.0bp,198.71bp)  .. (9);
  \draw [->] (1) ..controls (213.0bp,711.98bp) and (213.0bp,702.71bp)  .. (2);
  \draw [->] (9) ..controls (213.0bp,135.98bp) and (213.0bp,126.71bp)  .. (10);
  \draw [->] (0) ..controls (213.0bp,783.98bp) and (213.0bp,774.71bp)  .. (1);
  \draw [->] (3) ..controls (213.0bp,567.98bp) and (213.0bp,558.71bp)  .. (4);
  \draw [->] (10) ..controls (213.0bp,63.983bp) and (213.0bp,54.712bp)  .. (11);
%
\end{tikzpicture}
%}
}
\subfloat[]
{
\begin{tikzpicture}[>=latex',line join=bevel,,scale=0.27]
%%
\node (11) at (229.0bp,519.0bp) [draw=pink,fill=pink,circle,scale=0.3] {11. $\times$};
  \node (10) at (372.0bp,283.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {10. mov};
  \node (13) at (130.0bp,639.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {13. mov};
  \node (12) at (227.0bp,401.0bp) [draw=pink,fill=pink,circle,scale=0.3] {12. $\times$};
  \node (15) at (131.0bp,401.0bp) [draw=pink,fill=pink,circle,scale=0.3] {15. FMA};
  \node (14) at (130.0bp,519.0bp) [draw=pink,fill=pink,circle,scale=0.3] {14. FMA};
  \node (17) at (280.0bp,161.0bp) [draw=pink,fill=pink,circle,scale=0.3] {17. FMA};
  \node (16) at (158.0bp,283.0bp) [draw=pink,fill=pink,circle,scale=0.3] {16. FMA};
  \node (18) at (280.0bp,41.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {18. mov};
  \node (1) at (35.0bp,639.0bp) [draw=pink,fill=pink,circle,scale=0.3] {1. FMA};
  \node (0) at (35.0bp,752.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {0. mov};
  \node (3) at (38.0bp,401.0bp) [draw=pink,fill=pink,circle,scale=0.3] {3. FMA};
  \node (2) at (35.0bp,519.0bp) [draw=pink,fill=pink,circle,scale=0.3] {2. FMA};
  \node (5) at (280.0bp,283.0bp) [draw=pink,fill=pink,circle,scale=0.3] {5. $\times$};
  \node (4) at (248.0bp,639.0bp) [draw=pink,fill=pink,circle,scale=0.3] {4. $\times$};
  \node (7) at (372.0bp,639.0bp) [draw=pink,fill=pink,circle,scale=0.3] {7. FMA};
  \node (6) at (372.0bp,752.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {6. mov};
  \node (9) at (372.0bp,401.0bp) [draw=pink,fill=pink,circle,scale=0.3] {9. FMA};
  \node (8) at (372.0bp,519.0bp) [draw=pink,fill=pink,circle,scale=0.3] {8. FMA};
  \draw [->,solid] (17) ..controls (280.0bp,109.5bp) and (280.0bp,100.4bp)  .. (18);
  \draw [->,solid] (4) ..controls (267.3bp,594.26bp) and (273.1bp,577.62bp)  .. (276.0bp,562.0bp) .. controls (291.24bp,479.97bp) and (287.72bp,381.97bp)  .. (5);
  \draw [->,solid] (14) ..controls (130.44bp,467.23bp) and (130.52bp,458.16bp)  .. (15);
  \draw [->,solid] (16) ..controls (204.58bp,236.18bp) and (225.05bp,216.05bp)  .. (17);
  \draw [->,solid] (4) ..controls (240.85bp,593.6bp) and (238.66bp,579.97bp)  .. (11);
  \draw [->,solid] (7) ..controls (372.0bp,588.43bp) and (372.0bp,575.18bp)  .. (8);
  \draw [->,solid] (6) ..controls (372.0bp,707.13bp) and (372.0bp,697.21bp)  .. (7);
  \draw [->,solid] (15) ..controls (141.65bp,354.23bp) and (143.95bp,344.36bp)  .. (16);
  \draw [->,solid] (2) ..controls (36.172bp,472.7bp) and (36.54bp,458.45bp)  .. (3);
  \draw [->,solid] (8) ..controls (372.0bp,472.7bp) and (372.0bp,458.45bp)  .. (9);
  \draw [->,solid] (3) ..controls (78.351bp,360.99bp) and (100.96bp,339.14bp)  .. (16);
  \draw [->,solid] (13) ..controls (130.0bp,590.01bp) and (130.0bp,581.05bp)  .. (14);
  \draw [->,solid] (10) ..controls (336.87bp,236.18bp) and (323.75bp,219.07bp)  .. (17);
  \draw [->,solid] (5) ..controls (280.0bp,238.16bp) and (280.0bp,225.98bp)  .. (17);
  \draw [->,solid] (9) ..controls (372.0bp,356.71bp) and (372.0bp,345.03bp)  .. (10);
  \draw [->,solid] (12) ..controls (200.65bp,355.7bp) and (192.43bp,341.89bp)  .. (16);
  \draw [->,solid] (0) ..controls (35.0bp,705.68bp) and (35.0bp,694.02bp)  .. (1);
  \draw [->,solid] (11) ..controls (228.19bp,470.82bp) and (228.0bp,460.08bp)  .. (12);
  \draw [->,solid] (1) ..controls (35.0bp,592.2bp) and (35.0bp,577.16bp)  .. (2);
%
\end{tikzpicture}
}
\subfloat[]
{
\begin{tikzpicture}[>=latex',line join=bevel,scale=0.27]
%%
\node (11) at (44.0bp,446.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {11. mov};
  \node (10) at (445.0bp,580.0bp) [draw=pink,fill=pink,circle,scale=0.3] {10. FMA};
  \node (13) at (248.0bp,446.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {13. mov};
  \node (12) at (44.0bp,307.0bp) [draw=pink,fill=pink,circle,scale=0.3] {12. FMA};
  \node (15) at (146.0bp,307.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {15. mov};
  \node (14) at (248.0bp,307.0bp) [draw=pink,fill=pink,circle,scale=0.3] {14. FMA};
  \node (17) at (322.0bp,44.0bp) [draw=pink,fill=pink,circle,scale=0.3] {17. FMA};
  \node (16) at (228.0bp,173.0bp) [draw=pink,fill=pink,circle,scale=0.3] {16. FMA};
  \node (19) at (580.0bp,307.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {19. mov};
  \node (18) at (555.0bp,446.0bp) [draw=pink,fill=pink,circle,scale=0.3] {18. $\times$};
  \node (1) at (559.0bp,932.0bp) [draw=pink,fill=pink,circle,scale=0.3] {1. FMA};
  \node (0) at (559.0bp,1044.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {0. mov};
  \node (3) at (289.0bp,816.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {3. mov};
  \node (2) at (525.0bp,816.0bp) [draw=pink,fill=pink,circle,scale=0.3] {2. FMA};
  \node (5) at (397.0bp,307.0bp) [draw=pink,fill=pink,circle,scale=0.3] {5. $\times$};
  \node (4) at (466.0bp,700.0bp) [draw=pink,fill=pink,circle,scale=0.3] {4. FMA};
  \node (7) at (716.0bp,446.0bp) [draw=cyan,fill=cyan,circle,scale=0.3] {7. mov};
  \node (6) at (339.0bp,173.0bp) [draw=pink,fill=pink,circle,scale=0.3] {6. $\times$};
  \node (9) at (648.0bp,173.0bp) [draw=pink,fill=pink,circle,scale=0.3] {9. FMA};
  \node (8) at (716.0bp,307.0bp) [draw=pink,fill=pink,circle,scale=0.3] {8. FMA};
  \draw [->,solid] (18) ..controls (492.54bp,390.84bp) and (464.25bp,366.31bp)  .. (5);
  \draw [->,solid] (18) ..controls (566.61bp,381.37bp) and (568.9bp,368.82bp)  .. (19);
  \draw [->,solid] (15) ..controls (176.84bp,256.35bp) and (188.88bp,236.98bp)  .. (16);
  \draw [->,solid] (18) ..controls (570.99bp,576.9bp) and (584.84bp,728.08bp)  .. (573.0bp,856.0bp) .. controls (572.18bp,864.82bp) and (570.84bp,874.14bp)  .. (1);
  \draw [->,solid] (10) ..controls (485.62bp,530.25bp) and (500.71bp,512.15bp)  .. (18);
  \draw [->,solid] (6) ..controls (330.26bp,258.85bp) and (328.92bp,311.31bp)  .. (339.0bp,356.0bp) .. controls (353.78bp,421.53bp) and (390.18bp,490.39bp)  .. (10);
  \draw [->,solid] (1) ..controls (545.05bp,884.22bp) and (541.99bp,873.95bp)  .. (2);
  \draw [->,solid] (11) ..controls (44.0bp,391.77bp) and (44.0bp,376.3bp)  .. (12);
  \draw [->,solid] (0) ..controls (559.0bp,999.87bp) and (559.0bp,990.83bp)  .. (1);
  \draw [->,solid] (8) ..controls (689.62bp,254.8bp) and (679.36bp,234.87bp)  .. (9);
  \draw [->,solid] (14) ..controls (239.76bp,251.62bp) and (237.85bp,239.01bp)  .. (16);
  \draw [->,solid] (5) ..controls (410.95bp,221.12bp) and (413.25bp,167.3bp)  .. (396.0bp,124.0bp) .. controls (389.2bp,106.93bp) and (376.79bp,91.366bp)  .. (17);
  \draw [->,solid] (3) ..controls (347.72bp,777.18bp) and (392.34bp,748.44bp)  .. (4);
  \draw [->,solid] (9) ..controls (586.61bp,205.32bp) and (551.84bp,228.33bp)  .. (531.0bp,258.0bp) .. controls (473.52bp,339.85bp) and (454.45bp,458.04bp)  .. (10);
  \draw [->,solid] (18) ..controls (544.94bp,570.41bp) and (534.64bp,696.76bp)  .. (2);
  \draw [->,solid] (2) ..controls (501.25bp,769.1bp) and (494.62bp,756.29bp)  .. (4);d
  \draw [->,solid] (17) ..controls (377.93bp,83.18bp) and (400.95bp,102.52bp)  .. (417.0bp,124.0bp) .. controls (477.55bp,205.06bp) and (517.78bp,316.76bp)  .. (18);
  \draw [->,solid] (18) ..controls (451.15bp,422.98bp) and (368.62bp,405.8bp)  .. (297.0bp,392.0bp) .. controls (208.31bp,374.91bp) and (178.05bp,395.85bp)  .. (97.0bp,356.0bp) .. controls (91.648bp,353.37bp) and (86.437bp,350.03bp)  .. (12);
  \draw [->,solid] (7) ..controls (716.0bp,393.9bp) and (716.0bp,374.42bp)  .. (8);
  \draw [->,solid] (5) ..controls (334.49bp,262.88bp) and (305.62bp,241.97bp)  .. (281.0bp,222.0bp) .. controls (276.49bp,218.34bp) and (271.86bp,214.42bp)  .. (16);
  \draw [->,solid] (13) ..controls (248.0bp,391.77bp) and (248.0bp,376.3bp)  .. (14);
  \draw [->,solid] (18) ..controls (615.96bp,394.95bp) and (640.38bp,374.59bp)  .. (662.0bp,356.0bp) .. controls (667.62bp,351.16bp) and (673.53bp,346.0bp)  .. (8);
  \draw [->,solid] (5) ..controls (372.75bp,250.8bp) and (367.38bp,238.59bp)  .. (6);
  \draw [->,solid] (3) ..controls (244.81bp,711.63bp) and (167.61bp,534.97bp)  .. (93.0bp,392.0bp) .. controls (86.463bp,379.47bp) and (78.964bp,366.16bp)  .. (12);
  \draw [->,solid] (4) ..controls (457.65bp,652.08bp) and (456.0bp,642.81bp)  .. (10);
  \draw [->,solid] (16) ..controls (265.13bp,121.83bp) and (278.34bp,103.98bp)  .. (17);
  \draw [->,solid] (12) ..controls (87.475bp,227.8bp) and (127.44bp,165.72bp)  .. (175.0bp,124.0bp) .. controls (203.6bp,98.917bp) and (241.53bp,78.783bp)  .. (17);
  \draw [->,solid] (18) ..controls (529.27bp,533.04bp) and (513.76bp,581.81bp)  .. (498.0bp,624.0bp) .. controls (494.32bp,633.85bp) and (490.08bp,644.3bp)  .. (4);
  \draw [->,solid] (18) ..controls (461.37bp,408.97bp) and (395.19bp,382.33bp)  .. (339.0bp,356.0bp) .. controls (324.67bp,349.29bp) and (309.36bp,341.47bp)  .. (14);
  \draw [->,solid] (18) ..controls (606.89bp,393.91bp) and (620.81bp,375.44bp)  .. (629.0bp,356.0bp) .. controls (646.87bp,313.59bp) and (650.6bp,260.89bp)  .. (9);
%
\end{tikzpicture}
}
\caption{BroadWell mnemonic DAG of the polynomial evaluation for three methods of evaluation (latency/throughput), (a) classical Horner (50.0/3.55 [cycle]), (b) Estrin (22.68/2.90 [cycle]), (c)  Estrin$^6 \times$BruteForce$^4$ (28.94/3.11 [cycle])  \label{DAG}}
\end{figure}
\begin{table}[ht]
\begin{center}
\begin{tabular}{ l c c c c }
\hline
Architecture   & Model & FMA & Compiler & Options Compilation \\
SandyBridge & E5-2670 & no   & GCC 4.9  & \texttt{-O3 -march=native } \\
                      &                &        &                 & \texttt{-fast-math -fabi-version=6}  \\
BroadWell     & E5-2630 & yes & GCC 4.9.3  &  \texttt{-O3 -march=native -fma} \\
                      &                &        &                 & \texttt{-fast-math -fabi-version=6}  \\
\hline
\end{tabular}
\caption{Description of the processors, the compilers and the compilation options. \label{env_compil}} 
\end{center}
\end{table}

\subsection{Processors and Compiler}

This work focuses on the last release of Intel Xeon processor (BroadWell) and an oldest generation (SandyBridge). We privileged the intel architecture 
because it is well know and  well documented. The compiler is GCC. All informations are summarised in the table \ref{env_compil}.

The options \texttt{-fast-math} reorders the algebraical associative expressions following the IEEE-754 standard for a faster evaluation. Note this optimization
only work for the scalar version. Compilers does not reorganise SIMD code, every intrinsics are considered like it is. The \texttt{-fma} is mandatory to catch FMA pattern.
And finally the option \texttt{-fabi-version=6} for a usage of GCC vector Instructions through library signature.

 
\section{Results}
\subsection{Polynomials, preliminary analysis and validations}

\begin{figure}[ht!]
\mbox{
\hspace{-2cm}
\includegraphics[scale=0.7]{allht_scalar.eps} 
\hspace{-2.5cm}
\includegraphics[scale=0.7]{allht_vector.eps}
}
\mbox{
\hspace{-2cm}
\includegraphics[scale=0.7]{all_precision_scalar.eps} 
\hspace{-2.5cm}
\includegraphics[scale=0.7]{all_precision_vector.eps}
}
\caption{Latency/Throughput for  all architectures, scalar version left and vectorial version right \label{PLOT_LT0}
Histogram of the precision of all polynomial evaluation for scalar version (left) and vector version (right). The bar of 
the histogram is the total number os polynomial evaluations for a given ULP \label{PRECISION_ULP}}
\end{figure}




Build an efficient theoretical method is not trivial, and vendor performance simulator  are not released on the market. 
Nevertheless a few experimental data for latency measurement with the simplest polynomial evaluation can be verified easily. 
The classical Horner's method  provides a dependency chain between all the operations (As shown by the AMS DAG, Figure \ref{DAG}), therefore any instructions
can be only executed if the previous one is finished. The classical Horner's method gives 10 successives mull/add or 
FMA with dependency,  Therefore the theoretical total latency of the polynomial is: $ 10 [\textrm{mul+add}]_{latency}$ or $ 10 [\textrm{FMA}]_{latency}$
if the machine supports natively the FMA operations. The measurement gives 50.03 [cycle] on the BroadWell architecture and 80.01 [cycle] on the SandyBridge 
architecture which correspond perfectly on the tiny model proposes  previously.

Last but not least the simple factorizations combining the classical horner can be also modeled. Considering   $P^6_{h^1}P^4_{h^1}$, as the operations
mul, add and FMA are fully pipelined, the computation of the polynomial of lower degree should be fully hidden by the polynomial of larger degree.
In such situation the total latency of the produce of the polynomial is $ 6 [\textrm{mul+add}]_{latency} + [\textrm{mul}]_{latency}$ or $ 6 [\textrm{FMA}]_{latency}+ [\textrm{mul}]_{latency}$.
The last multiplication corresponds to the final product between the two polynomials, it is a blocking operations. The measurement gives  36.00 [cycle] on the BroadWell 
architecture 53.04 [cycle] on the SandyBridge, one more time the results are perfects.  This first analysis shows already two informations, the factorization and the programming model is efficient.

\subsection{Polynomials, latency analysis} 

The totality of the results latency/throughput for the polynomial evaluation on all platforms are presented on the figures \ref{PLOT_LT0}. 
The results are not distributed uniformly. For the scalar version they are distributed around 30 [cycle] (BroadWell) and 40 [cycle] (SandyBriddge). For the vectorial version, the distribution is more regular. In details, some packets of points appear (confirm by the table \ref{LTR_EXP_0} and \ref{LTR_EXP_1}. It is due to the \texttt{-fast-math} option. The \texttt{-fast-math} reorganises the computation  of the power of $n$ think impossible for the vector version, as all the version are just slightly different (confirm by a fast quick look on the disassemble ,code), considering the out of order execution and the renaming of the register, it is clear that a few version give the same results. Oppositely for the vectorial version these packets are less visible because SIMD intrinsics programming prevent any optimisation of the \texttt{-fast-math},  every algorithm is unique.  

For the Latency measurement, as excepted for all platform the longest latency concerns the classical Horner method due to the chain dependency. Considering the Horner method like the reference performance point the spectra of the results are large. The best results can out-perform by a factor lightly larger than 2 for the SandyBridge. In details if we have a look on the 10 best  results in the Table \ref{LTR_EXP_0} and \ref{LTR_EXP_1}, as excepted the factorisation is a good pattern for improve the performance. 

\subsection{Polynomials, throughput analysis}

The throughput analysis is similar to the latency one, but with one more additional information. Factorization pattern increase slightly the throughput in cycle, it can be
easily understand. The  factorisation pattern  improves the parallelism and consequently a better fill up of the pipeline, therefore it takes more cycle to restart a new operations
as the pipelines are full. Oppositely, the classical Horner scheme with its chain-dependency introduces a lot of bubbles in the pipelines and improve the throughput. The programmers 
are confronted to a dilemma, intensive users of polynomials should privilege the throughput and so Horner method  \cite{DBLP:DukhanV13} whereas occasional user should prefer latency
and so factorisation scheme.

\subsection{Polynomials, precision analysis \label{section_ulp}}
\begin{table}[t!]
\begin{center}
\begin{tabular}{ l c c c  l c c  c }
\hline
		       & \multicolumn{3}{c}{scalar} & &  \multicolumn{3}{c}{vector} \\
		       &   \multicolumn{3}{c}{Throughput Criteria} &   &  \multicolumn{3}{c}{Throughput Criteria} \\
 Algorithm        & Th.  & La.      & ULP  & Algorithm                & Th.   & La.     & ULP \\ 	
$P_{h^4}^{6}P_e^{2}P_{h^1}^{2}$ & 6.08 & 35.10 & 8  & $P_b^{2}P_{h^2}^{4}P_{h^2}^{4}$ & 5.95 & 34.77 & 9 \\
$P_{h^4}^{6}P_{h^1}^{2}P_{h^1}^{2}$ & 6.20 & 36.03 & 8  & $P_b^{2}P_{h^1}^{4}P_{h^1}^{4}$ & 6.13 & 43.70 & 9 \\
$P_{h^4}^{6}P_{h^1}^{4}$ & 6.23 & 37.82 & 8  & $P_b^{2}P_{h^1}^{4}P_{h^2}^{4}$ & 6.24 & 43.06 & 10 \\
$P_{h^4}^{6}P_e^{4}$ & 6.26 & 34.12 & 8  & $P_b^{2}P_e^{4}P_{h^2}^{4}$ & 6.37 & 31.56 & 11 \\
$P_{h^3}^{6}P_e^{2}P_e^{2}$ & 6.27 & 39.17 & 8  & $P_b^{2}P_e^{2}P_e^{2}P_e^{2}P_{h^1}^{2}$ & 6.37 & 40.05 & 9 \\
$P_{h^4}^{6}P_{h^2}^{4}$ & 6.28 & 34.18 & 8  & $P_b^{2}P_e^{4}P_{h^1}^{4}$ & 6.39 & 37.27 & 10 \\
$P_{h^3}^{6}P_b^{2}P_b^{2}$ & 6.30 & 39.18 & 9  & $P_b^{2}P_e^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 6.41 & 39.73 & 9 \\
$P_{h^3}^{6}P_{h^1}^{2}P_{h^1}^{2}$ & 6.31 & 38.93 & 9  & $P_b^{2}P_e^{2}P_e^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 6.44 & 40.05 & 9 \\
$P_{h^5}^{6}P_b^{2}P_{h^1}^{2}$ & 6.32 & 38.81 & 8  & $P_b^{2}P_b^{2}P_b^{2}P_b^{2}P_b^{2}$ & 6.46 & 42.46 & 8 \\
$P_{h^5}^{6}P_b^{2}P_b^{2}$ & 6.33 & 38.87 & 8  & $P_b^{2}P_b^{2}P_e^{2}P_e^{2}P_e^{2}$ & 6.49 & 41.38 & 9 \\
		       & \multicolumn{3}{c}{Latency Criteria} &           &  \multicolumn{3}{c}{Latency Criteria}\\
$P_e^{6}P_b^{4}$ & 7.00 & 29.03 & 9  & $P_e^{10}$ & 7.09 & 29.07 & 4 \\
$P_e^{6}P_e^{4}$ & 6.99 & 29.72 & 8  & $P_e^{6}P_e^{4}$ & 8.22 & 29.15 & 10 \\
$P_{h^1}^{2}P_b^{4}P_e^{4}$ & 7.46 & 30.16 & 9  & $P_e^{6}P_{h^2}^{4}$ & 7.32 & 29.15 & 9 \\
$P_{h^1}^{2}P_e^{4}P_{h^2}^{4}$ & 7.12 & 31.20 & 9  & $P_e^{2}P_e^{4}P_{h^2}^{4}$ & 7.74 & 30.05 & 9 \\
$P_e^{2}P_b^{4}P_e^{4}$ & 7.47 & 31.67 & 9  & $P_{h^1}^{2}P_e^{4}P_{h^2}^{4}$ & 7.68 & 30.07 & 10 \\
$P_b^{4}P_b^{2}P_e^{2}P_{h^1}^{2}$ & 7.33 & 31.70 & 10  & $P_e^{6}P_b^{4}$ & 7.58 & 30.65 & 8 \\
$P_b^{4}P_b^{2}P_b^{2}P_{h^1}^{2}$ & 7.34 & 31.70 & 10  & $P_b^{2}P_e^{4}P_{h^2}^{4}$ & 6.37 & 31.56 & 11 \\
$P_e^{4}P_e^{2}P_e^{2}P_{h^1}^{2}$ & 7.36 & 31.70 & 10  & $P_e^{8}P_e^{2}$ & 7.53 & 32.27 & 8 \\
$P_e^{4}P_b^{2}P_b^{2}P_{h^1}^{2}$ & 7.20 & 31.71 & 9  & $P_e^{8}P_{h^1}^{2}$ & 7.49 & 33.05 & 8 \\
$P_b^{4}P_e^{2}P_e^{2}P_{h^1}^{2}$ & 7.36 & 31.71 & 12  & $P_e^{6}P_e^{2}P_{h^1}^{2}$ & 7.48 & 33.05 & 9 \\
\hline
\end{tabular}
\end{center}
\caption{Best Latency/throughput [cycle] polynomial evaluations on SandyBridge platform. The criteria indicates the sort of the tuple (throughput, latency, ulp), 
on a specific part \label{LTR_EXP_0}}
\end{table}%
The ULP error estimator is presented on the Figure  \ref{PRECISION_ULP}. Errors is dispatched in two group. First group with a small ULP 2/3 or a larger one around 8/9.
This behaviour is explained easily. The first group concerns all the polynomial of type $P^{10}_m$, no factorisation has been applied on it. Contrary to the $2^{nd}$ group 
where the factorisation has been applied. This processus was performed with Matlab software, and it necessitates resolution of quadratic equation  it introduces numerical error.
of the determinant due to the round-off error. Kahan the father of IEEE floating point investigated the phenomena on simple quadratic \cite{Kahan2002}, 
consequently it should be possible to improve the ULP of all factorisation pattern. 




More surprisingly, the vector presents a shift of 1 ULP compare to the scalar version. Although we do not have a clear idea of this shift, for the scalar
version the compiler has more freedom than the vector version where  the SIMD intrinsics associated to the algorithm fix the operations executions.
In the scalar version, the compiler reorganise the operations following is setting in policy, we think the generated is done to maximise the performance and
minimise the error.

%\begin{figure}[h]
%\mbox{
%\vspace{-2.5cm}
%\includegraphics[scale=0.7]{hw_histo_vector_l_sb.eps} 
%\hspace{-2.5cm}
%\includegraphics[scale=0.7]{hw_histo_vector_t_sb.eps} 
%}
%\mbox{
%\vspace{-2.5cm}
%\includegraphics[scale=0.7]{hw_histo_vector_l_hw.eps} 
%\hspace{-2.5cm}
%\includegraphics[scale=0.7]{hw_histo_vector_t_hw.eps} 
%}
%\caption{Latency and Throughput frequency of the vectorial version. The analysis is done for the original polynomial or the factorization associated. Sampling interval of 1 for the latency and 0.1 for the throughput.}
%\end{figure}



\begin{table}[t!]
\begin{center}
\begin{tabular}{ l c c c  l c c  c }
\hline
		       & \multicolumn{3}{c}{scalar}              &           &  \multicolumn{3}{c}{vector}\\
		       & \multicolumn{3}{c}{Throughput Criteria} &           &  \multicolumn{3}{c}{Throughput Criteria}\\
 Algorithm             & Th.  & La.     & ULP                    & Algorithm & Th.   & La.     & ULP\\
$P_{h^1}^{2}P_e^{4}P_{h^1}^{4}$ & 3.54 & 25.06 & 9  & $P_e^{6}P_e^{4}$ & 3.30 & 20.99 & 8 \\
$P_{h^1}^{2}P_b^{4}P_{h^1}^{4}$ & 3.54 & 25.03 & 8  & $P_e^{6}P_{h^2}^{4}$ & 3.40 & 21.03 & 9 \\
$P_e^{2}P_e^{4}P_{h^1}^{4}$ & 3.54 & 25.03 & 8  & $P_e^{6}P_{h^1}^{4}$ & 3.40 & 23.67 & 8 \\
$P_e^{6}P_e^{2}P_e^{2}$ & 3.54 & 25.59 & 9  & $P_e^{10}$ & 3.41 & 22.65 & 4 \\
$P_e^{4}P_b^{2}P_b^{2}P_b^{2}$ & 3.54 & 23.17 & 9  & $P_{h^2}^{6}P_{h^1}^{4}$ & 3.45 & 26.28 & 8 \\
$P_e^{4}P_b^{2}P_e^{2}P_e^{2}$ & 3.54 & 23.17 & 10  & $P_{h^2}^{6}P_{h^2}^{4}$ & 3.46 & 27.27 & 8 \\
$P_e^{4}P_e^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 3.54 & 23.50 & 9  & $P_e^{6}P_e^{2}P_e^{2}$ & 3.46 & 23.80 & 9 \\
$P_e^{4}P_b^{2}P_b^{2}P_e^{2}$ & 3.54 & 23.19 & 10  & $P_e^{2}P_e^{4}P_e^{4}$ & 3.47 & 23.65 & 10 \\
$P_e^{4}P_b^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 3.54 & 23.57 & 9  & $P_e^{8}P_e^{2}$ & 3.47 & 26.01 & 9 \\
$P_e^{6}P_b^{2}P_e^{2}$ & 3.55 & 25.58 & 8  & $P_e^{2}P_{h^1}^{4}P_{h^2}^{4}$ & 3.49 & 26.45 & 9 \\
		       & \multicolumn{3}{c}{Latency Criteria} &           &  \multicolumn{3}{c}{Latency Criteria}\\
$P_b^{2}P_b^{2}P_b^{2}P_b^{2}P_{h^1}^{2}$ & 3.64 & 21.46 & 9  & $P_e^{6}P_e^{4}$ & 3.30 & 20.99 & 8 \\
$P_b^{2}P_e^{2}P_e^{2}P_e^{2}P_{h^1}^{2}$ & 5.03 & 21.46 & 9  & $P_e^{6}P_{h^2}^{4}$ & 3.40 & 21.03 & 9 \\
$P_b^{2}P_b^{2}P_b^{2}P_e^{2}P_{h^1}^{2}$ & 3.63 & 21.46 & 10  & $P_{h^1}^{2}P_e^{4}P_{h^2}^{4}$ & 3.57 & 22.43 & 9 \\
$P_b^{2}P_b^{2}P_e^{2}P_e^{2}P_{h^1}^{2}$ & 3.63 & 21.47 & 8  & $P_e^{2}P_e^{4}P_{h^2}^{4}$ & 3.56 & 22.62 & 10 \\
$P_b^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 3.58 & 21.48 & 7  & $P_e^{10}$ & 3.41 & 22.65 & 4 \\
$P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 3.62 & 21.48 & 8  & $P_{h^1}^{2}P_e^{4}P_e^{4}$ & 3.49 & 23.29 & 9 \\
$P_e^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 3.58 & 21.48 & 8  & $P_b^{2}P_e^{4}P_{h^2}^{4}$ & 3.85 & 23.33 & 10 \\
$P_e^{2}P_e^{2}P_e^{2}P_e^{2}P_{h^1}^{2}$ & 3.63 & 21.51 & 9  & $P_e^{2}P_e^{4}P_e^{4}$ & 3.47 & 23.65 & 10 \\
$P_b^{2}P_b^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 3.60 & 21.64 & 8  & $P_e^{6}P_{h^1}^{4}$ & 3.40 & 23.67 & 8 \\
$P_b^{2}P_e^{2}P_e^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 5.05 & 21.64 & 8  & $P_e^{6}P_e^{2}P_{h^1}^{2}$ & 3.53 & 23.74 & 9 \\

\hline
\end{tabular}
\end{center}
\caption{Best Latency/throughput [cycle] polynomial evaluations on BroadWell platform. The criteria indicates the sort the tuple (throughput, latency, ulp),
on a specific part \label{LTR_EXP_1}}
\end{table}%

\subsection{Exponentials}
Evaluate the exponential necessitates the addition of the computation of $2^k$ and the boundary conditions that have been described in the section \ref{TwokSection} and \ref{BoundarySection}.
The throughput/latency benchmarks are reported in the table  \ref{twokboundary}. The results are similar whatever  the platforms and the backend (vectorial/scalar), with a little improvement 
on the BroadWell version, and specially for the vector version because it supports  AVX2  which allows more vectorial SIMD integers instructions.
However the latency of the boundary and the $2^k$ are quiet expensive in term of cycle. It is easily explained. Both operations introduced a chain
of dependency that does not allow parallelism although the large number of ALU inside the processors.



\begin{table}[t]
\begin{center}
\begin{tabular}{ l c c  c c c c c c  }
\hline
Architecture & \multicolumn{4}{c}{$2^k$} & \multicolumn{4}{c}{boundary} \\
	             & \multicolumn{2}{c}{scalar} & \multicolumn{2}{c}{vector} & \multicolumn{2}{c}{scalar} & \multicolumn{2}{c}{vector} \\
			& Thr. & Lat. 		& Thr. & Lat. 		& Thr. & Lat.  		& Thr. & Lat.\\		    
SandyBridge & 4.71  & 15.00  &3.49 &18.99   & 4.71 &9.95 & 4.04 &  8.01 \\
BroadWell     & 3.63  &16.00  & 2.70 &   18.00 &  4.94&  10.65& 2.64 &  8.01 \\
\hline
\end{tabular}
\end{center}
\caption{Benchmark of the $2^k$ and the boundary conditions for the evaluation of $e^x$. \label{twokboundary}}
\end{table}%
\begin{figure}[ht]
\mbox{
\hspace{-2cm}
\includegraphics[scale=0.7]{allht_scalar_exp.eps} 
\hspace{-2.5cm}
\includegraphics[scale=0.7]{allht_vector_exp.eps}
}
\caption{Latency/Throughput for  all architectures, scalar version left and vectorial version right \label{PLOT_LT00} }
\end{figure}
\begin{table}[t]
\begin{center}
\begin{tabular}{ l c c c  l c c  c }
\hline
		       & \multicolumn{3}{c}{scalar}              &           &  \multicolumn{3}{c}{vector}\\
		       & \multicolumn{3}{c}{Throughput Criteria} &           &  \multicolumn{3}{c}{Throughput Criteria}\\
 Algorithm             & Th.  & La.     & ULP                    & Algorithm & Th.   & La.     & ULP\\
$P_e^{2}P_b^{4}P_{h^2}^{4}$ & 12.44 & 64.73 & 9  & $P_e^{6}P_b^{4}$ & 13.69 & 66.92 & 9 \\
$P_{h^2}^{6}P_e^{2}P_{h^1}^{2}$ & 12.48 & 72.88 & 7  & $P_b^{2}P_e^{4}P_e^{4}$ & 13.69 & 66.92 & 11 \\
$P_b^{2}P_e^{4}P_{h^2}^{4}$ & 12.62 & 64.75 & 9  & $P_e^{2}P_e^{4}P_{h^2}^{4}$ & 13.71 & 67.79 & 9 \\
$P_b^{2}P_{h^2}^{4}P_{h^2}^{4}$ & 12.72 & 67.79 & 9  & $P_e^{2}P_{h^1}^{4}P_{h^1}^{4}$ & 13.76 & 78.90 & 9 \\
$P_{h^3}^{6}P_{h^1}^{2}P_{h^1}^{2}$ & 12.76 & 70.01 & 9  & $P_{h^1}^{2}P_e^{4}P_{h^2}^{4}$ & 13.82 & 66.08 & 10 \\
$P_{h^1}^{2}P_b^{4}P_{h^3}^{4}$ & 12.79 & 64.76 & 10  & $P_{h^2}^{6}P_{h^1}^{4}$ & 13.90 & 74.14 & 8 \\
$P_e^{2}P_b^{4}P_e^{4}$ & 12.80 & 63.88 & 9  & $P_e^{8}P_e^{2}$ & 13.93 & 70.01 & 9 \\
$P_{h^4}^{10}$ & 12.81 & 76.13 & 2  & $P_{h^1}^{2}P_{h^2}^{4}P_{h^2}^{4}$ & 13.99 & 72.01 & 9 \\
$P_{h^2}^{6}P_e^{2}P_e^{2}$ & 12.82 & 72.75 & 8  & $P_b^{2}P_{h^1}^{4}P_{h^2}^{4}$ & 14.03 & 78.11 & 10 \\
$P_b^{2}P_{h^3}^{4}P_{h^3}^{4}$ & 12.82 & 69.78 & 9  & $P_b^{2}P_{h^1}^{4}P_{h^1}^{4}$ & 14.09 & 80.04 & 10 \\
		       & \multicolumn{3}{c}{Latency Criteria} &           &  \multicolumn{3}{c}{Latency Criteria}\\
$P_{h^1}^{2}P_b^{4}P_e^{4}$ & 15.40 & 62.88 & 9  & $P_e^{6}P_{h^2}^{4}$ & 14.78 & 65.08 & 9 \\
$P_e^{2}P_e^{4}P_e^{4}$ & 16.17 & 63.19 & 10  & $P_e^{10}$ & 16.07 & 65.12 & 3 \\
$P_b^{2}P_e^{4}P_e^{4}$ & 15.95 & 63.21 & 10  & $P_e^{6}P_e^{4}$ & 15.88 & 65.72 & 9 \\
$P_{h^1}^{2}P_e^{4}P_e^{4}$ & 13.18 & 63.84 & 9  & $P_{h^1}^{2}P_e^{4}P_{h^2}^{4}$ & 13.82 & 66.08 & 10 \\
$P_e^{2}P_b^{4}P_e^{4}$ & 12.80 & 63.88 & 9  & $P_b^{2}P_e^{4}P_e^{4}$ & 13.69 & 66.92 & 11 \\
$P_e^{6}P_b^{4}$ & 12.84 & 63.90 & 8  & $P_e^{6}P_b^{4}$ & 13.69 & 66.92 & 9 \\
$P_b^{2}P_b^{4}P_e^{4}$ & 15.18 & 63.90 & 10  & $P_e^{6}P_b^{2}P_b^{2}$ & 15.72 & 67.73 & 9 \\
$P_{h^1}^{2}P_e^{4}P_{h^3}^{4}$ & 15.22 & 63.98 & 9  & $P_e^{2}P_e^{4}P_{h^2}^{4}$ & 13.71 & 67.79 & 9 \\
$P_e^{2}P_e^{4}P_{h^2}^{4}$ & 15.18 & 64.73 & 9  & $P_b^{2}P_e^{4}P_{h^2}^{4}$ & 16.70 & 67.80 & 9 \\
$P_e^{2}P_b^{4}P_{h^2}^{4}$ & 12.44 & 64.73 & 9  & $P_e^{2}P_e^{4}P_e^{4}$ & 16.25 & 68.02 & 10 \\
\hline
\end{tabular}
\end{center}
\caption{Best Latency/throughput [cycle] $e^x$  evaluations SandBridge platform. The criteria indicates the sort the tuple (throughput, latency, ulp),
on a specific part \label{LTR_EXP_00}}
\end{table}%

\begin{table}[ht]
\begin{center}
\begin{tabular}{ l c c c  l c c  c }
\hline
		       & \multicolumn{3}{c}{scalar}              &           &  \multicolumn{3}{c}{vector}\\
		       & \multicolumn{3}{c}{Throughput Criteria} &           &  \multicolumn{3}{c}{Throughput Criteria}\\
 Algorithm             & Th.  & La.     & ULP                    & Algorithm & Th.   & La.     & ULP\\
$P_e^{6}P_{h^1}^{4}$ & 8.94 & 57.77 & 8  & $P_{h^2}^{6}P_b^{4}$ & 8.17 & 64.44 & 9 \\
$P_{h^1}^{2}P_{h^1}^{4}P_{h^1}^{4}$ & 9.01 & 58.05 & 9  & $P_{h^2}^{10}$ & 8.18 & 71.52 & 3 \\
$P_b^{2}P_{h^1}^{4}P_{h^1}^{4}$ & 9.01 & 58.05 & 9  & $P_{h^2}^{8}P_b^{2}$ & 8.26 & 69.10 & 8 \\
$P_e^{2}P_{h^1}^{4}P_{h^1}^{4}$ & 9.01 & 58.05 & 8  & $P_b^{2}P_b^{4}P_{h^2}^{4}$ & 8.68 & 59.60 & 10 \\
$P_{h^1}^{2}P_e^{4}P_{h^1}^{4}$ & 9.03 & 57.55 & 8  & $P_e^{6}P_{h^2}^{4}$ & 8.68 & 59.57 & 9 \\
$P_e^{6}P_b^{2}P_b^{2}$ & 9.05 & 58.04 & 8  & $P_e^{6}P_e^{4}$ & 8.80 & 60.86 & 9 \\
$P_{h^1}^{2}P_b^{4}P_{h^1}^{4}$ & 9.05 & 57.53 & 10  & $P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 8.84 & 55.89 & 9 \\
$P_e^{6}P_b^{2}P_e^{2}$ & 9.08 & 58.04 & 8  & $P_{h^2}^{8}P_{h^1}^{2}$ & 8.87 & 69.29 & 8 \\
$P_b^{2}P_e^{4}P_{h^1}^{4}$ & 9.08 & 57.61 & 9  & $P_e^{2}P_b^{4}P_{h^1}^{4}$ & 8.87 & 61.34 & 10 \\
$P_e^{2}P_b^{4}P_{h^1}^{4}$ & 9.09 & 57.62 & 10  & $P_e^{2}P_e^{4}P_{h^1}^{4}$ & 8.90 & 60.49 & 9 \\
		       & \multicolumn{3}{c}{Latency Criteria} &           &  \multicolumn{3}{c}{Latency Criteria}\\
$P_b^{4}P_e^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 9.40 & 52.07 & 10  & $P_b^{2}P_b^{2}P_b^{2}P_b^{2}P_e^{2}$ & 13.35 & 55.65 & 10 \\
$P_e^{4}P_b^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 9.42 & 52.07 & 9  & $P_b^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 9.12 & 55.67 & 9 \\
$P_b^{4}P_b^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 9.42 & 52.07 & 9  & $P_b^{2}P_b^{2}P_b^{2}P_b^{2}P_{h^1}^{2}$ & 13.48 & 55.76 & 9 \\
$P_e^{4}P_e^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 9.42 & 52.08 & 9  & $P_b^{2}P_e^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 9.60 & 55.80 & 10 \\
$P_b^{2}P_b^{2}P_e^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 9.53 & 52.09 & 8  & $P_b^{2}P_b^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 9.90 & 55.86 & 9 \\
$P_e^{2}P_e^{2}P_e^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 9.54 & 52.10 & 8  & $P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 8.84 & 55.89 & 9 \\
$P_b^{2}P_b^{2}P_b^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 9.54 & 52.10 & 9  & $P_b^{2}P_b^{2}P_e^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 13.27 & 55.95 & 10 \\
$P_b^{2}P_e^{2}P_e^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 9.54 & 52.10 & 9  & $P_b^{2}P_b^{2}P_b^{2}P_b^{2}P_b^{2}$ & 12.70 & 55.96 & 10 \\
$P_b^{4}P_b^{2}P_b^{2}P_{h^1}^{2}$ & 9.31 & 52.16 & 10  & $P_b^{2}P_e^{2}P_e^{2}P_{h^1}^{2}P_{h^1}^{2}$ & 12.50 & 55.99 & 9 \\
$P_e^{4}P_b^{2}P_b^{2}P_{h^1}^{2}$ & 9.31 & 52.16 & 10  & $P_b^{2}P_e^{2}P_e^{2}P_e^{2}P_{h^1}^{2}$ & 12.73 & 56.02 & 9 \\
\hline
\end{tabular}
\end{center}
\caption{Best Latency/throughput [cycle]  $e^x$  evaluations BroadWell platform. The criteria indicates the sort the tuple (throughput, latency, ulp),
on a specific part \label{LTR_EXP_11}}
\end{table}%

Evaluate  $e^x$  (Figures \ref{PLOT_LT00} and Tables \ref{LTR_EXP_00} and \ref{LTR_EXP_11}) confirm the results obtained by the polynomials evaluation.
The factorisation scheme provides the best results. Globally the throughput has been multiplied by three and the latency by two.  However thinks are a bit more complex.
In details, "large" vectorisations provide better latency due to a better fill up of the pipelines, the consequences the port 0,1 (floating point) are saturated, it slows down 
the integer and bitwise operations  for the boundary conditions and the evaluation of $2^k$ which are also evaluated on similar port (0,1 and 5), consequently
throughput increases drastically for this scheme. The larger number of port and FPU/APU limit this effect on BroadWell compare to SandyBridge.
Moreover, this effect is much presented on the vectorial version because the code the SIMD code is generated like it is, the intrinsics prevents any optimisation 
of the compiler due to the \texttt{-fast-math} option
compare to the scalar version.

The final comparison concerns the Intel Mathematical Function (IMF - scalar version) and Intel Short Vector Mathematical Library (SVML - vectorial version), 
both libraries are delivered with Intel Compiler (15.0.0). The benchmarks throughtput/latency/ulp provide the following results for the scalar [20.61/52.69/2] 
and vectorial [6.38/39.20/2] on BroadWell and scalar [12.11/67.39/2], vectorial [12.25/55.37/2] on SandyBridge.
%Note we were not able to measure the properly  the ulp of the scalar version because the intel does not provide a specific API contrary to the SVML, 
%which is problematic in the ULP benchmark. 
Our results that are summurized on the table \ref{LTR_EXP_00} and \ref{LTR_EXP_11} can be equivalent 
or better specially for the scalar version on SandyBridge platform. However the vectorial on BroadWell is not reachable. The SandyBridge platform is quiet old
the first product where released in 2011, Intel may have not updated their library for this target. Our results have a larger error, we know it comes from the factorisation
method. For the latency/throughput difference of performance, we do not have any informations about the algorithm that Intel implements. 
Disassemble the code lets appear  a look at table and a polynomial of lower degree, which explain easily the difference of performance.
About the poor performance on the IMF library, we do not have good explanation, make a good vectorial version like the SVML should automatically
gives good results for the scalar version as the registers are the same (XMM is nested in YMM).

\section{Conclusions}

We evaluate the most famous pattern of polynomial on 10 degree polynomial, moreover we explored the factorisation scheme with systematic combination
of all patterns. We did a systematic study of the 3 keys parameters throughput/latency/ulp.  We show the factorization is an interesting pattern on superscalar
architecture and it provides better results for the throughput and the latency. We show the dependency effect between the throughput and the latency. 
We do not reach better results than vendors library although our best score, but we do not have a clear idea of the algorithm of the vendors. However factorization
is definitively a good pattern on superscalar processor and could be applied to other fields of computer sciences.

\section{Acknowledgments} The authors of the present paper would like to gratefully thank the members of the Blue Brain Project HPC team, for the many discussions 
and feedback they provided. In addition, theSwiss National Supercomputing Center (CSCS) and their collaborators (Hussein Nasser El- Harake) 
are here thanked for providing access and support to a BroadWell system.  This work has been funded by both the EPFL Blue Brain Project (funded by the Swiss ETH board) 
and the Supercomputing and Modeling for the Human Brain (SMHB) project supported by the German Helmholtz Association. 
\section{References}
\bibliographystyle{elsarticle-num}
\bibliography{bib/modeling}

\end{document}
